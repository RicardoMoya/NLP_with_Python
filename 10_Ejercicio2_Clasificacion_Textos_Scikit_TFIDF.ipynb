{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Ejercicio 2: Clasificación de textos con Scikit-Learn\n",
    "\n",
    "\n",
    "* Este ejercicio es exactamente igual que el notebook \"*09_Scikit_Clasificacion_Textos*\" que consistia en clasificar una serie de Tweets en Ingles sobre críticas a los productos de Apple.\n",
    "\n",
    "\n",
    "* Estos tweets estan clasificados como: *positivos*, *neutros* o *negativos*\n",
    "\n",
    "\n",
    "* En esta caso vamos a realizar un cambio para ver si los resultados de clasificación mejoran o no respecto al notebook \"*09_Scikit_Clasificacion_Textos*\" y este cambio va a consistir en ***cambiar la bolsa de palabras de frecuencias a TF-IDF***\n",
    "\n",
    "\n",
    "* ***El objetivo es ver si con este cambio los resultados obtenidos por los modelos generados son mejores, peores o iguales que los obtenidos anteriormente***\n",
    "\n",
    "\n",
    "* Al igual que en el notebook \"*09_Scikit_Clasificacion_Textos*\" realizaremos los siguientes pasos:\n",
    "    \n",
    "    1. Carga de los datos (tweets)\n",
    "    2. Normalización (en ingles) de los tweets\n",
    "    3. ***Creacción de la Bolsa de Palabras*** - TODO -\n",
    "    4. Particionado de Datos\n",
    "    5. Creacción de modelos\n",
    "        - Multinomial Naive Bayes\n",
    "        - Bernoulli Naive Bayes\n",
    "        - Regresion Logistica\n",
    "        - Support Vector Machine\n",
    "        - Random Forest\n",
    "    6. Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Carga de Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tweets_file = './data/Apple_Tweets.csv'\n",
    "df = pd.read_csv(tweets_file, header=None)\n",
    "tweets = [tuple(x) for x in df.values]\n",
    "print('Número de Tweets Cargados: {num}'.format(num=len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Normalización\n",
    "\n",
    "* Para ***normalizar*** los tweets realizaremos las siguientes acciones:\n",
    "    1. Pasamos las frases a minúsculas.\n",
    "    2. Eliminamos los signos de puntuación.\n",
    "    3. Eliminamos las palabras con menos de 3 caracteres.\n",
    "    4. Eliminamos las Stop-Words.\n",
    "    5. Eliminamos las palabras que empiecen por '@' o 'http'.\n",
    "    6. Pasamos la palabra a su lema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Divido los datos en dos listas \n",
    "#     X: los tweets\n",
    "#     y: target (polaridad)\n",
    "\n",
    "X = [doc[0] for doc in tweets]\n",
    "y = [doc[1] for doc in tweets]\n",
    "\n",
    "def normalize(sentenses):\n",
    "    \"\"\"normalizamos la lista de frases y devolvemos la misma lista de frases normalizada\"\"\"\n",
    "    for index, sentense in enumerate(tqdm(sentenses)):\n",
    "        sentense = nlp(sentense.lower()) # Paso la frase a minúsculas y a un objeto de la clase Doc de Spacy\n",
    "        sentenses[index] = \" \".join([word.lemma_ for word in sentense if (not word.is_punct)\n",
    "                                     and (len(word.text) > 2) and (not word.is_stop) \n",
    "                                     and (not word.text.startswith('@')) and (not word.text.startswith('http'))])\n",
    "    return sentenses\n",
    "\n",
    "# Normalizamos las frases\n",
    "X = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Bolsa de Palabras - EJERCICIO -\n",
    "\n",
    "\n",
    "* En este punto hay que construir la bolsa de palabras con el TF-IDF (Ver notebook: \"*05_Bag_of_Words_BoW*\")\n",
    "\n",
    "\n",
    "* Al igual que la implementación de la clase \"*CountVectorizer*\", la clase \"*TfidfVectorizer*\" también permite quedarnos con las palabras más relevantes, utilizando los dos parámetros que son:\n",
    "    - **max_features**: Con este parámetro le indicamos que nos seleccione la '*X*' palabras más frecuentes del corpus. En este ejemplo **seleccionaremos las 1000 más frecuentes**.\n",
    "    - **min_df**: Con este parámetro le indicamos el número mínimo de documentos en la que tiene que aparecer la palabra para que se incluya en la bolsa de palabras. En este ejemplo **seleccionaremos 3 documentos** (tweets).\n",
    "    \n",
    "\n",
    "* ***NOTA***: para más información podéis mirar la documentación de la clase \"*TfidfVectorizer*\" en: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Particionado de Datos (Train y Test)\n",
    "\n",
    "* Particionamos de la siguiente manera:\n",
    "\n",
    "    - 80% de datos de entrenamiento\n",
    "    - 20% de datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Número de Tweets para el entrenamiento: {num}'.format(num=X_train.shape[0]))\n",
    "print('Número de Tweets para el test: {num}'.format(num=X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Creacción de los Modelos\n",
    "\n",
    "\n",
    "* Para este ejercicio vamos a usar los siguientes algoritmos de aprendizaje:\n",
    "\n",
    "    - Multinomial Naive Bayes\n",
    "    - Bernoulli Naive Bayes\n",
    "    - Regresion Logistica\n",
    "    - Support Vector Machine\n",
    "    - Random Forest (ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "svm_lin = SVC(kernel='linear')\n",
    "svm_poly = SVC(kernel='poly')\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "svm_sig = SVC(kernel='sigmoid')\n",
    "rf_20 = RandomForestClassifier(n_estimators=500, bootstrap=True, criterion='gini', max_depth=20, random_state=0)\n",
    "rf_50 = RandomForestClassifier(n_estimators=500, bootstrap=True, criterion='gini', max_depth=50, random_state=0)\n",
    "\n",
    "clasificadores = {'Multinomial NB': mnb,\n",
    "                  'Bernoulli NB': bnb,\n",
    "                  'Regresion Logistica': lr,\n",
    "                  'SVM lineal': svm_lin,\n",
    "                  'SVM polinomico': svm_poly,\n",
    "                  'SVM Kernel rbf': svm_rbf,\n",
    "                  'SVM Kernel Sigmoid': svm_sig,\n",
    "                  'Random Forest d_20': rf_20,\n",
    "                  'Random Forest d_50': rf_50}\n",
    "\n",
    "\n",
    "# Ajustamos los modelos y calculamos el accuracy para los datos de entrenamiento\n",
    "for k, v in clasificadores.items():\n",
    "    print ('CREANDO MODELO: {clas}'.format(clas=k))\n",
    "    v.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Evaluación del Modelo\n",
    "\n",
    "\n",
    "* Para cada uno de los modelos vamos a calcular las siguientes métricas de evaluación:\n",
    "\n",
    "    1. **Accuracy**\n",
    "    2. **Precision**\n",
    "    3. **Recall**\n",
    "    4. **F1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluation(model, name, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Función de devuelve en un diccionario las métricas de evaluación de \n",
    "    Accuracy, Precision, Recall y F1 para los conjuntos de datos de entrenamiento y test\n",
    "        model: modelo a evaluar\n",
    "        name: nombre del modelo\n",
    "        X_train: Variables de entrada del conjunto de datos de entrenamiento\n",
    "        y_train: Variable de salida del conjunto de datos de entrenamiento\n",
    "        X_test: Variables de entrada del conjunto de datos de test\n",
    "        y_test: Variable de salida del conjunto de datos de test\n",
    "        return: diccionario con el nombre del modelo y el valor de las métricas\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "    model_dict['name'] = name\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    model_dict['accuracy_train'] = accuracy_score(y_true=y_train, y_pred=y_pred_train)\n",
    "    model_dict['accuracy_tests'] = accuracy_score(y_true=y_test, y_pred=y_pred_test)\n",
    "    model_dict['precision_train'] = precision_score(y_true=y_train, y_pred=y_pred_train, average='weighted')\n",
    "    model_dict['precision_tests'] = precision_score(y_true=y_test, y_pred=y_pred_test, average='weighted')\n",
    "    model_dict['recall_train'] = recall_score(y_true=y_train, y_pred=y_pred_train, average='weighted')\n",
    "    model_dict['recall_tests'] = recall_score(y_true=y_test, y_pred=y_pred_test, average='weighted')\n",
    "    model_dict['f1_train'] = f1_score(y_true=y_train, y_pred=y_pred_train, average='weighted')\n",
    "    model_dict['f1_tests'] = f1_score(y_true=y_test, y_pred=y_pred_test, average='weighted')\n",
    "    \n",
    "    return model_dict\n",
    "\n",
    "\n",
    "# Calculamos las métricas de los modelos por separado\n",
    "evaluacion = list()\n",
    "for key, model in clasificadores.items():\n",
    "    evaluacion.append(evaluation(model=model, name=key, \n",
    "                                 X_train=X_train, y_train=y_train,\n",
    "                                 X_test=X_test, y_test=y_test))\n",
    "\n",
    "# Pasamos los resultados a un DataFrame para visualizarlos mejor\n",
    "df = pd.DataFrame.from_dict(evaluacion)\n",
    "df.set_index(\"name\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Representamos las métricas para los diferentes modelos en un gráfico de barras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Métricas a pintar\n",
    "METRICS = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "\n",
    "# Transformamos el dataframe para pintar las gráficas con seaborn\n",
    "df_plot = df.reset_index().melt(id_vars='name').rename(columns=str.title)\n",
    "\n",
    "plt.figure(figsize=(25, 12))\n",
    "pos = 1\n",
    "for metric in METRICS:\n",
    "    # Filtramos la métrica a pintar\n",
    "    df_aux = df_plot[df_plot['Variable'].str.contains(metric)]\n",
    "    \n",
    "    # Pintamos la gráfica en su posición 2x2\n",
    "    plt.subplot(2, 2, pos)\n",
    "    sns.barplot(x='Name', y='Value', hue='Variable', data=df_aux)\n",
    "    plt.title(metric.upper())\n",
    "    plt.grid()\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xticks(rotation=20)\n",
    "    pos += 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
