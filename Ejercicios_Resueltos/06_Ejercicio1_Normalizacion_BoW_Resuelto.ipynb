{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n",
    "\n",
    "* En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n",
    "\n",
    "\n",
    "* Estos artículos se encuentran en un fichero csv dentro de la carpeta \"data\" del proyecto (***./data/corpus_mundo_today.csv***).\n",
    "\n",
    "\n",
    "* Este CSV esta formado por 3 campos que son:\n",
    "    - Tema\n",
    "    - Título\n",
    "    - Texto\n",
    "    \n",
    "    \n",
    "* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n",
    "\n",
    "\n",
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
    "<span></span><br><br>\n",
    "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "<span></span><br><br>        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>        \n",
    "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "* Aprovechando la normalización realizada anteriormente se pide:\n",
    "<span></span><br><br>\n",
    "    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "<span></span><br><br>       \n",
    "    7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n",
    "<span></span><br><br>   \n",
    "    8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_file = '../data/corpus_mundo_today.csv'\n",
    "docs_list = list()\n",
    "file_txt = open(docs_file, encoding=\"utf8\").read()\n",
    "for line in file_txt.split('\\n'):\n",
    "    line = line.split('||')\n",
    "    docs_list.append(line[1] + ' ' + line[2])\n",
    "docs_list = docs_list[1:] # Elimino la cabecera del fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def tokenization(docs_list):\n",
    "    for index, doc in enumerate(docs_list):\n",
    "        docs_list[index] = [word.text.strip().lower() for word in nlp(doc) if word.text.strip() != \"\"]\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = nlp(\" \".join(doc))\n",
    "        docs[index] = [word.text for word in d if not word.is_punct and not word.is_stop]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = nlp(\" \".join(doc))\n",
    "        docs[index] = [word.lemma_ for word in d]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = nlp(\" \".join(doc))\n",
    "        docs[index] = [word.text for word in d if word.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADV', 'ADJ']]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gobierno', 'español', 'sumar', 'junquera', 'condena', 'cumplir', 'puigdemont', 'revés', 'recibido', 'gobierno', 'españo', 'puesto', 'libertad', 'carl', 'puigdemont', 'justicia', 'alemán', 'juez', 'pablo', 'llarén', 'decidido', 'semana', 'instancia', 'ejecutivo', 'sumar', 'oriol', 'junquera', 'condena', 'cumplir', 'líder', 'pdecat', 'exvicepresidente', 'cataluña', 'permanecer', 'prisión', 'madrileño', 'estremero', 'noviembre', 'asumir', 'delito', 'atribuido', 'carl', 'puigdemont', 'tribunal', 'supremo', 'asegurar', 'acto', 'expresidente', 'catalán', 'legislatura', 'quedar', 'impún', 'junquera', 'pagar', 'maniobra', 'ideado', 'burlar', 'justicia', 'alemán', 'líder', 'esquerro', 'republicano', 'enfrentar', 'año', 'prisión', 'seguir', 'junquera', 'caer', 'año', 'parar', 'carl', 'puigdemont', 'alemania', 'junquera', 'sacrificar', 'asumir', 'resignación', 'determinación', 'prometido', 'seguim', 'tuitear', 'trascender', 'decisión', 'llarén', 'fuente', 'anónimo', 'judicial', 'barajar', 'posibilidad', 'añadir', 'pena', 'oriol', 'junquera', 'condena', 'imponer', 'futuro', 'iñaki', 'urdangarin', 'rodrigo', 'rato', 'esperanza', 'aguirrir', 'delito', 'robo', 'fuerza', 'ocurrido', 'semana', 'huesco', 'policía', 'incapaz', 'encontrar', 'culpable']\n"
     ]
    }
   ],
   "source": [
    "def normalization(docs_list):\n",
    "    corpus = tokenization(docs_list)\n",
    "    corpus = remove_words(corpus)\n",
    "    corpus = lematization(corpus)\n",
    "    corpus = filter_words(corpus)\n",
    "    return corpus\n",
    "\n",
    "corpus = normalization(docs_list)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gobierno', 'español', 'puigdemont', 'gobierno', 'españo', 'puesto', 'puigdemont', 'semana', 'líder', 'cataluña', 'puigdemont', 'asegurar', 'catalán', 'quedar', 'líder', 'año', 'seguir', 'año', 'puigdemont', 'semana']\n"
     ]
    }
   ],
   "source": [
    "def bow_freq (corpus):\n",
    "    \"\"\"Función que cuenta el número de veces que aparece una palabra \n",
    "        en el corpus y lo almacena en un diccionario\n",
    "    \"\"\"\n",
    "    bow = dict()\n",
    "    for doc in corpus:\n",
    "        for word in doc:\n",
    "            if word in bow:\n",
    "                bow[word] += 1\n",
    "            else:\n",
    "                bow[word] = 1\n",
    "    return bow\n",
    "\n",
    "def drop_less_frecuency_words(corpus, n):\n",
    "    bow = bow_freq(corpus)\n",
    "    for index, doc in enumerate(corpus):\n",
    "        corpus[index] = [word for word in doc if bow[word] >= n]\n",
    "    return corpus\n",
    "\n",
    "corpus = drop_less_frecuency_words(corpus, 10)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de palabras -> palabra: id\n",
      "\n",
      "{'asegurar': 0, 'año': 1, 'cataluña': 2, 'catalán': 3, 'españo': 4, 'español': 5, 'gobierno': 6, 'líder': 7, 'puesto': 8, 'puigdemont': 9, 'quedar': 10, 'seguir': 11, 'semana': 12, 'cifuent': 13, 'ciudadano': 14, 'demostrar': 15, 'elección': 16, 'hacer': 17, 'madrid': 18, 'partido': 19, 'pasar': 20, 'presidenta': 21, 'venir': 22, 'decir': 23, 'equipo': 24, 'importante': 25, 'jugar': 26, 'liga': 27, 'ligo': 28, 'mariano': 29, 'mañana': 30, 'nombre': 31, 'país': 32, 'presidencia': 33, 'público': 34, 'rajoy': 35, 'recordar': 36, 'empezar': 37, 'españa': 38, 'hora': 39, 'pedir': 40, 'psoe': 41, 'tipo': 42, 'millón': 43, 'ministro': 44, 'presupuesto': 45, 'salir': 46, 'casa': 47, 'marcar': 48, 'presidente': 49, 'real': 50, 'volver': 51, 'arrimada': 52, 'llevar': 53, 'minuto': 54, 'central': 55, 'diario': 56, 'gracias': 57, 'punto': 58, 'resultado': 59, 'dejar': 60, 'ganar': 61, 'pedro': 62, 'sánchez': 63, 'díaz': 64, 'llegar': 65, 'barcelón': 66, 'cabeza': 67, 'hablar': 68, 'ver': 69, 'jornada': 70, 'rojo': 71, 'sociedad': 72, 'victoria': 73, 'gallego': 74, 'iglesia': 75, 'banco': 76, 'bce': 77, 'crisis': 78, 'draghi': 79, 'economía': 80, 'europa': 81, 'europeo': 82, 'guindo': 83, 'conseguir': 84, 'fmi': 85, 'gráfico': 86, 'alavés': 87, 'athletic': 88, 'atlético': 89, 'barça': 90, 'campo': 91, 'celta': 92, 'champions': 93, 'deportivo': 94, 'eibar': 95, 'espanyol': 96, 'futbol': 97, 'getafir': 98, 'girón': 99, 'jugador': 100, 'leganés': 101, 'levante': 102, 'palma': 103, 'sevillo': 104, 'valencia': 105, 'villareal': 106, 'árbitro': 107, 'aficionado': 108, 'betis': 109, 'gol': 110, 'ronaldo': 111, 'temporada': 112, 'estadio': 113, 'valencio': 114, 'local': 115, 'pitar': 116}\n",
      "\n",
      "Apariciones de las palabras en los documentos (id, 1):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "vectors = [[(token[0], 1) for token in dictionary.doc2bow(doc)] for doc in corpus]\n",
    "\n",
    "# Resultados\n",
    "print('Diccionario de palabras -> palabra: id\\n')\n",
    "print(dictionary.token2id)\n",
    "print('\\nApariciones de las palabras en los documentos (id, 1):')\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aficionado', 'alavés', 'arrimada', 'asegurar', 'athletic', 'atlético', 'año', 'banco', 'barcelón', 'barça', 'bce', 'betis', 'cabeza', 'campo', 'casa', 'cataluña', 'catalán', 'celta', 'central', 'champions', 'cifuent', 'ciudadano', 'conseguir', 'crisis', 'decir', 'dejar', 'demostrar', 'deportivo', 'diario', 'draghi', 'díaz', 'economía', 'eibar', 'elección', 'empezar', 'equipo', 'espanyol', 'españa', 'españo', 'español', 'estadio', 'europa', 'europeo', 'fmi', 'futbol', 'gallego', 'ganar', 'getafir', 'girón', 'gobierno', 'gol', 'gracias', 'gráfico', 'guindo', 'hablar', 'hacer', 'hora', 'iglesia', 'importante', 'jornada', 'jugador', 'jugar', 'leganés', 'levante', 'liga', 'ligo', 'llegar', 'llevar', 'local', 'líder', 'madrid', 'marcar', 'mariano', 'mañana', 'millón', 'ministro', 'minuto', 'nombre', 'palma', 'partido', 'pasar', 'país', 'pedir', 'pedro', 'pitar', 'presidencia', 'presidenta', 'presidente', 'presupuesto', 'psoe', 'puesto', 'puigdemont', 'punto', 'público', 'quedar', 'rajoy', 'real', 'recordar', 'resultado', 'rojo', 'ronaldo', 'salir', 'seguir', 'semana', 'sevillo', 'sociedad', 'sánchez', 'temporada', 'tipo', 'valencia', 'valencio', 'venir', 'ver', 'victoria', 'villareal', 'volver', 'árbitro']\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.23388801 0.14192601 0.         ... 0.         0.19573158 0.17397362]\n",
      " [0.         0.05077815 0.         ... 0.11157363 0.         0.        ]\n",
      " [0.11959074 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Transformamos los documentos tokenizados a documentos sin tokenizar\n",
    "for index, doc in enumerate(corpus):\n",
    "    corpus[index] = \" \".join(doc)\n",
    "    \n",
    "tfidf = TfidfVectorizer()\n",
    "corpus_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Resultados\n",
    "print(tfidf.get_feature_names())\n",
    "print(corpus_tfidf.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
