{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n",
    "\n",
    "* En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n",
    "\n",
    "\n",
    "* Estos artículos se encuentran en un fichero csv dentro de la carpeta \"data\" del proyecto (***./data/corpus_mundo_today.csv***).\n",
    "\n",
    "\n",
    "* Este CSV esta formado por 3 campos que son:\n",
    "    - Tema\n",
    "    - Título\n",
    "    - Texto\n",
    "    \n",
    "    \n",
    "* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n",
    "\n",
    "\n",
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
    "<span></span><br><br>\n",
    "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "<span></span><br><br>        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>        \n",
    "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "* Aprovechando la normalización realizada anteriormente se pide:\n",
    "<span></span><br><br>\n",
    "    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "<span></span><br><br>    \n",
    "    7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras por frecuencia de aparición con NLTK**\n",
    "<span></span><br><br>    \n",
    "    8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n",
    "<span></span><br><br>   \n",
    "    9. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_file = '../data/corpus_mundo_today.csv'\n",
    "docs_list = list()\n",
    "file_txt = open(docs_file, encoding=\"utf8\").read()\n",
    "for line in file_txt.split('\\n'):\n",
    "    line = line.split('||')\n",
    "    docs_list.append(line[1] + ' ' + line[2])\n",
    "docs_list = docs_list[1:] # Elimino la cabecera del fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def tokenization(docs_list):\n",
    "    for index, doc in enumerate(docs_list):\n",
    "        docs_list[index] = [word.text.strip().lower() for word in nlp(doc) if word.text.strip() != \"\"]\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = spacy.tokens.doc.Doc(nlp.vocab, words=doc)\n",
    "        docs[index] = [word.text for word in d if not word.is_punct and not word.is_stop]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = spacy.tokens.doc.Doc(nlp.vocab, words=doc)\n",
    "        docs[index] = [word.lemma_ for word in d]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = nlp(\" \".join(doc))\n",
    "        docs[index] = [word.text for word in d if word.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADV', 'ADJ']]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gobernar', 'español', 'sumar', 'junquera', 'cumplir', 'puigdemont', 'rever', 'recibir', 'gobernar', 'españa', 'poner', 'libertar', 'carles', 'puigdemont', 'justicia', 'alemán', 'juez', 'pablar', 'llarena', 'decidir', 'semana', 'instancia', 'ejecutivo', 'sumar', 'oriol', 'cumplir', 'líder', 'pdecat', 'exvicepresidente', 'cataluña', 'permanecer', 'prisión', 'madrileño', 'estremera', 'noviembre', 'asumir', 'delito', 'atribuir', 'carles', 'puigdemont', 'tribunal', 'supremo', 'asegurar', 'acto', 'expresidente', 'catalán', 'legislatura', 'quedar', 'impune', 'pagar', 'maniobrar', 'idear', 'burlar', 'justicia', 'alemán', 'líder', 'esquerra', 'republicano', 'enfrentar', 'año', 'prisión', 'seguir', 'junquera', 'caer', 'año', 'parar', 'carles', 'puigdemont', 'alemania', 'hacer', 'junquera', 'sacrificar', 'asumir', 'resignación', 'determinación', 'prometer', 'seguim', 'tuiteaba', 'trascender', 'decisión', 'llarena', 'fuente', 'anónimo', 'judicial', 'barajar', 'posibilidad', 'penar', 'oriol', 'condenar', 'poder', 'imponerse', 'futuro', 'iñaki', 'urdangarin', 'rodrigar', 'rato', 'esperanzar', 'aguirre', 'delito', 'forzar', 'ocurrir', 'semana', 'huesca', 'policía', 'incapaz', 'encontrar', 'culpable']\n"
     ]
    }
   ],
   "source": [
    "def normalization(docs_list):\n",
    "    corpus = tokenization(docs_list)\n",
    "    corpus = remove_words(corpus)\n",
    "    corpus = lematization(corpus)\n",
    "    corpus = filter_words(corpus)\n",
    "    return corpus\n",
    "\n",
    "corpus = normalization(docs_list)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gobernar', 'puigdemont', 'gobernar', 'españa', 'poner', 'puigdemont', 'semana', 'líder', 'cataluña', 'puigdemont', 'asegurar', 'catalán', 'líder', 'año', 'año', 'puigdemont', 'semana']\n"
     ]
    }
   ],
   "source": [
    "def bow_freq (corpus):\n",
    "    \"\"\"Función que cuenta el número de veces que aparece una palabra \n",
    "        en el corpus y lo almacena en un diccionario\n",
    "    \"\"\"\n",
    "    bow = dict()\n",
    "    for doc in corpus:\n",
    "        for word in doc:\n",
    "            if word in bow:\n",
    "                bow[word] += 1\n",
    "            else:\n",
    "                bow[word] = 1\n",
    "    return bow\n",
    "\n",
    "def drop_less_frecuency_words(corpus, n):\n",
    "    bow = bow_freq(corpus)\n",
    "    for index, doc in enumerate(corpus):\n",
    "        corpus[index] = [word for word in doc if bow[word] >= n]\n",
    "    return corpus\n",
    "\n",
    "corpus = drop_less_frecuency_words(corpus, 10)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras por frecuencia de aparición con NLTK**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'gobernar': 2, 'puigdemont': 4, 'españa': 1, 'poner': 1, 'semana': 2, 'líder': 2, 'cataluña': 1, 'asegurar': 1, 'catalán': 1, 'año': 2})\n",
      "defaultdict(<class 'int'>, {'ciudadano': 4, 'cifuentes': 5, 'año': 3, 'falto': 3, 'elección': 3, 'mostrar': 2, 'gobernar': 2, 'partir': 2, 'semana': 1, 'cristino': 2, 'presidente': 2, 'madrid': 3, 'líder': 1, 'demostrar': 1, 'votar': 1, 'poner': 2, 'declarar': 1, 'prensar': 1})\n",
      "defaultdict(<class 'int'>, {'mariano': 3, 'rajoy': 5, 'presidencia': 5, 'españa': 2, 'líder': 1, 'partir': 1, 'cristino': 1, 'cifuentes': 1, 'casar': 1, 'año': 2, 'insistir': 1, 'país': 1, 'equipar': 1, 'mirar': 1, 'importante': 1, 'jugar': 1, 'ligar': 3, 'mañana': 1, 'demostrar': 1, 'gobernar': 1, 'público': 1, 'asegurar': 1, 'poner': 1, 'ciudadano': 1})\n",
      "defaultdict(<class 'int'>, {'cristino': 3, 'cifuentes': 6, 'presidente': 1, 'madrid': 1, 'mañana': 1, 'hora': 1, 'presidencia': 1, 'gobernar': 1, 'demostrar': 1, 'líder': 1, 'españa': 1})\n",
      "defaultdict(<class 'int'>, {'puigdemont': 2, 'pedir': 2, 'cifuentes': 3, 'rajoy': 3, 'españa': 1, 'cataluña': 1, 'presidente': 1, 'madrid': 3, 'cristino': 2, 'insistir': 1, 'españolar': 1, 'catalán': 1, 'asegurar': 1, 'tipo': 1, 'psoe': 1, 'presentar': 1, 'presidencia': 1, 'hora': 1, 'gobernar': 1, 'partir': 1, 'declarar': 1})\n",
      "defaultdict(<class 'int'>, {'mostrar': 2, 'cristino': 2, 'cifuentes': 5, 'presentar': 1, 'presidente': 2, 'madrid': 1, 'gobernar': 1, 'psoe': 2, 'insistir': 1, 'ciudadano': 2})\n",
      "defaultdict(<class 'int'>, {'presupuesto': 8, 'ministro': 3, 'salir': 2, 'mañana': 1, 'prensar': 1, 'falto': 2, 'casar': 1, 'presentar': 1, 'mirar': 1, 'millón': 1, 'gobernar': 1, 'insistir': 1})\n",
      "defaultdict(<class 'int'>, {'presupuesto': 2, 'ministro': 1, 'presentar': 2, 'importante': 1, 'año': 3, 'partir': 4, 'gobernar': 3, 'mañana': 1, 'casar': 1, 'millón': 1})\n",
      "defaultdict(<class 'int'>, {'mariano': 3, 'rajoy': 4, 'españa': 7, 'asegurar': 1, 'prensar': 3, 'vestir': 1, 'salir': 1, 'país': 1, 'catalán': 1, 'informar': 1, 'real': 1, 'volver': 3, 'presidente': 1, 'tipo': 1})\n",
      "defaultdict(<class 'int'>, {'presidente': 3, 'casar': 1, 'informar': 2, 'vestir': 2, 'gobernar': 1, 'catalán': 2, 'votar': 2, 'volver': 1, 'cataluña': 1, 'minuto': 1, 'llevar': 1, 'hora': 1})\n",
      "defaultdict(<class 'int'>, {'ciudadano': 3, 'ganar': 2, 'elección': 4, 'diario': 1, 'país': 2, 'punto': 1, 'psoe': 1, 'poner': 1, 'presidente': 3, 'partir': 2, 'mariano': 2, 'rajoy': 3, 'gracia': 1, 'presidencia': 1, 'cataluña': 1, 'casar': 1, 'líder': 1, 'central': 1, 'madrid': 1, 'españolar': 1, 'resultar': 1})\n",
      "defaultdict(<class 'int'>, {'rajoy': 4, 'puigdemont': 2, 'presidente': 1, 'gobernar': 1, 'españa': 1, 'mariano': 2, 'asegurar': 1, 'insistir': 1, 'catalán': 1, 'madrid': 1})\n",
      "defaultdict(<class 'int'>, {'puigdemont': 5, 'gobernar': 4, 'españa': 3, 'presidente': 1, 'cataluña': 3, 'españolar': 2, 'catalán': 3, 'mariano': 3, 'rajoy': 5, 'minuto': 1, 'insistir': 1, 'hora': 1, 'elección': 1, 'central': 1, 'asegurar': 1, 'equipar': 1, 'vivir': 1})\n",
      "defaultdict(<class 'int'>, {'rajoy': 3, 'ganar': 1, 'elección': 1, 'resultar': 3, 'mariano': 2, 'informar': 1, 'minuto': 2, 'gobernar': 3, 'españa': 3, 'presidente': 2, 'partir': 3, 'demostrar': 1, 'declarar': 1, 'psoe': 1, 'líder': 1, 'ciudadano': 1, 'pedro': 1, 'sánchez': 1})\n",
      "defaultdict(<class 'int'>, {'gobernar': 3, 'cataluña': 2, 'país': 7, 'ciudadano': 2, 'presidencia': 2, 'catalán': 2, 'diario': 4, 'votar': 2, 'elección': 1, 'mañana': 1, 'presidente': 3, 'llegar': 1, 'mostrar': 1, 'informar': 1, 'insistir': 1, 'equipar': 1, 'poner': 1, 'partir': 1, 'díaz': 1})\n",
      "defaultdict(<class 'int'>, {'cabeza': 4, 'votar': 6, 'ciudadano': 7, 'importante': 1, 'país': 1, 'año': 1, 'barcelona': 1, 'llegar': 1, 'insistir': 1, 'resultar': 1, 'hablar': 1})\n",
      "defaultdict(<class 'int'>, {'pedro': 3, 'sánchez': 6, 'rajoy': 4, 'díaz': 3, 'líder': 2, 'partir': 1, 'informar': 1, 'presidencia': 1, 'catalán': 1, 'presidente': 2, 'gobernar': 1, 'mariano': 1, 'hablar': 1, 'pedir': 1, 'psoe': 2})\n",
      "defaultdict(<class 'int'>, {'pedro': 6, 'sánchez': 5, 'líder': 1, 'mañana': 2, 'llevar': 1, 'psoe': 1, 'mirar': 3, 'informar': 2, 'pasar': 1, 'año': 2, 'presidente': 1, 'hora': 1, 'palmar': 1})\n",
      "defaultdict(<class 'int'>, {'díaz': 7, 'ganar': 1, 'rojo': 1, 'victoria': 2, 'psoe': 2, 'informar': 1, 'ciudadano': 1, 'sevilla': 1, 'sánchez': 2, 'casar': 1, 'pedir': 1, 'sociedad': 1, 'presidente': 2, 'jornada': 1, 'gracia': 1})\n",
      "defaultdict(<class 'int'>, {'diario': 5, 'país': 5, 'resultar': 1, 'psoe': 1, 'victoria': 1, 'pedro': 2, 'sánchez': 2, 'asegurar': 1, 'díaz': 1, 'hora': 1})\n",
      "defaultdict(<class 'int'>, {'presentar': 1, 'rajoy': 3, 'asegurar': 1, 'mariano': 2, 'presidente': 1, 'presupuesto': 1, 'psoe': 1, 'llevar': 1, 'partir': 1, 'elección': 1, 'gallego': 1})\n",
      "defaultdict(<class 'int'>, {'iglesia': 6, 'partir': 2, 'asegurar': 1, 'españa': 1, 'elección': 1, 'líder': 2, 'hora': 1, 'prensar': 1})\n",
      "defaultdict(<class 'int'>, {'elección': 2, 'mañana': 1, 'psoe': 3, 'españa': 2, 'partir': 1, 'iglesia': 2, 'votar': 2, 'ciudadano': 1, 'tipo': 1, 'declarar': 1, 'hora': 1, 'resultar': 1})\n",
      "defaultdict(<class 'int'>, {'banco': 2, 'central': 2, 'europeo': 5, 'bce': 2, 'millón': 1, 'españolar': 2, 'guindo': 4, 'europa': 2, 'tipo': 1, 'ministro': 2, 'economía': 1, 'año': 1, 'españa': 3, 'insistir': 1, 'salir': 1, 'crisis': 1, 'llegar': 1, 'presidente': 1, 'draghi': 1})\n",
      "defaultdict(<class 'int'>, {'europeo': 2, 'año': 2, 'público': 2, 'economía': 2, 'jornada': 1, 'casar': 3, 'prensar': 1, 'mañana': 2, 'bce': 1, 'crisis': 1})\n",
      "defaultdict(<class 'int'>, {'presidente': 3, 'banco': 3, 'central': 2, 'europeo': 3, 'hablar': 2, 'draghi': 4, 'bce': 2, 'público': 1, 'declarar': 1, 'economía': 1, 'mostrar': 1, 'europa': 1, 'crisis': 1, 'pedir': 1})\n",
      "defaultdict(<class 'int'>, {'bce': 7, 'crisis': 5, 'gráfico': 13, 'curvo': 12, 'draghi': 3, 'presidente': 2, 'banco': 2, 'central': 1, 'europeo': 3, 'fmi': 1, 'economía': 2, 'rojo': 7, 'tipo': 2, 'gobernar': 1, 'llevar': 1, 'hablar': 1, 'casar': 1, 'presentar': 1, 'país': 1, 'vestir': 1, 'marcar': 1})\n",
      "defaultdict(<class 'int'>, {'bce': 5, 'tipo': 5, 'economía': 1, 'europeo': 2, 'banco': 1, 'central': 1, 'punto': 1, 'historia': 1, 'presidente': 2, 'draghi': 3, 'minuto': 1, 'público': 1, 'vestir': 2, 'llevar': 1, 'jugar': 1, 'mañana': 1, 'llegar': 1, 'mirar': 2, 'asegurar': 1, 'europa': 1})\n",
      "defaultdict(<class 'int'>, {'guindo': 4, 'banco': 3, 'central': 3, 'europeo': 2, 'llevar': 1, 'gobernar': 2, 'españa': 2, 'presentar': 1, 'ministro': 2, 'poner': 3, 'economía': 2, 'bce': 1, 'país': 1, 'draghi': 2, 'españolar': 2, 'declarar': 1, 'importante': 1, 'rajoy': 1, 'año': 2, 'demostrar': 1, 'llegar': 1})\n",
      "defaultdict(<class 'int'>, {'gobernar': 2, 'crisis': 3, 'vivir': 2, 'ministro': 1, 'economía': 5, 'guindo': 2, 'mañana': 1, 'españolar': 2, 'sociedad': 1, 'año': 1, 'real': 1})\n",
      "defaultdict(<class 'int'>, {'país': 4, 'mañana': 1, 'fmi': 3, 'iglesia': 3, 'millón': 2, 'presidente': 1, 'insistir': 1, 'españa': 1, 'informar': 1, 'falto': 1, 'partir': 1})\n",
      "defaultdict(<class 'int'>, {'fmi': 5, 'pedir': 1, 'españa': 4, 'gobernar': 1, 'mariano': 1, 'rajoy': 1, 'españolar': 1, 'llevar': 1, 'mañana': 1})\n",
      "defaultdict(<class 'int'>, {'diario': 1, 'economía': 1, 'ministro': 1, 'público': 1, 'gobernar': 2, 'salir': 1, 'europeo': 3, 'crisis': 1, 'llegar': 1, 'vivir': 1, 'asegurar': 1, 'presidente': 2, 'presentar': 1, 'año': 1, 'país': 1, 'rojo': 2, 'fmi': 1, 'mostrar': 1})\n",
      "defaultdict(<class 'int'>, {'fmi': 4, 'año': 4, 'españa': 2, 'mañana': 1, 'españolar': 3, 'poner': 1, 'resultar': 1, 'importante': 1, 'llegar': 3, 'banco': 1, 'semana': 1, 'pasar': 2, 'gobernar': 1, 'ciudadano': 1})\n",
      "defaultdict(<class 'int'>, {'barça': 1, 'minuto': 2, 'ligar': 2, 'girona': 1, 'betis': 2, 'jugador': 2, 'ganar': 4, 'partir': 5, 'europa': 1, 'año': 3, 'sevilla': 1, 'villareal': 1, 'semana': 1, 'barcelona': 1, 'valencia': 2, 'catalán': 1, 'llegar': 1, 'gracia': 1, 'leganés': 2, 'celta': 2, 'punto': 1, 'futbol': 4, 'palmar': 1, 'real': 3, 'sociedad': 1, 'demostrar': 1, 'asegurar': 1, 'hora': 1, 'athletic': 1, 'deportivo': 2, 'eibar': 1, 'alavés': 1, 'atlético': 2, 'levantar': 1, 'champions': 1, 'getafe': 1, 'espanyol': 1, 'málaga': 2, 'madrid': 2, 'árbitro': 1})\n",
      "defaultdict(<class 'int'>, {'real': 5, 'madrid': 3, 'ronaldo': 2, 'levantar': 3, 'eibar': 1, 'jugador': 3, 'año': 2, 'futbol': 2, 'deportivo': 1, 'palmar': 1, 'gallego': 1, 'ganar': 2, 'pasar': 1, 'valencia': 1, 'alavés': 1, 'volver': 1, 'pedir': 1, 'cabeza': 2, 'presidente': 1, 'sociedad': 1, 'getafe': 1, 'betis': 2, 'espanyol': 1, 'jugar': 1, 'victoria': 2, 'equipar': 1, 'historia': 1, 'leganés': 1, 'sevilla': 2, 'barcelona': 1, 'athletic': 1, 'barça': 1, 'catalán': 1, 'temporada': 2, 'villareal': 1, 'atlético': 2, 'ligar': 2, 'importante': 1, 'celta': 1, 'málaga': 1, 'partir': 1, 'girona': 1, 'aficionar': 1, 'gol': 1})\n",
      "defaultdict(<class 'int'>, {'españolar': 1, 'barça': 4, 'ganar': 3, 'ligar': 3, 'villareal': 2, 'girona': 2, 'llevar': 1, 'punto': 4, 'demostrar': 1, 'sevilla': 2, 'athletic': 1, 'vivir': 1, 'aficionar': 1, 'equipar': 2, 'año': 1, 'deportivo': 2, 'eibar': 2, 'jugador': 3, 'poner': 1, 'marcar': 1, 'gol': 1, 'leganés': 1, 'málaga': 2, 'jornada': 1, 'partir': 2, 'estadio': 1, 'salir': 3, 'real': 3, 'madrid': 2, 'getafe': 2, 'victoria': 2, 'psg': 1, 'levantar': 1, 'espanyol': 3, 'valencia': 3, 'sánchez': 1, 'temporada': 2, 'árbitro': 1, 'futbol': 1, 'resultar': 1, 'jugar': 2, 'barcelona': 1, 'atlético': 2, 'gracia': 2, 'catalán': 1, 'sociedad': 1, 'alavés': 1, 'volver': 1, 'europa': 1, 'betis': 1, 'pedir': 2, 'celta': 1, 'palmar': 1, 'champions': 1, 'semana': 1})\n",
      "defaultdict(<class 'int'>, {'pitar': 3, 'barça': 2, 'girona': 1, 'leganés': 1, 'catalán': 1, 'partir': 2, 'llevar': 1, 'ronaldo': 5, 'gol': 3, 'psg': 8, 'palmar': 1, 'sevilla': 2, 'punto': 2, 'importante': 2, 'victoria': 1, 'real': 9, 'madrid': 10, 'poner': 1, 'champions': 5, 'eibar': 1, 'barcelona': 1, 'local': 1, 'millón': 1, 'año': 1, 'gracia': 1, 'alavés': 2, 'deportivo': 1, 'hablar': 1, 'europa': 1, 'ganar': 4, 'málaga': 2, 'valencia': 2, 'resultar': 2, 'sociedad': 2, 'levantar': 1, 'casar': 1, 'atlético': 2, 'athletic': 2, 'vivir': 1, 'ligar': 1, 'estadio': 1, 'espanyol': 1, 'equipo': 1, 'betis': 1, 'semana': 1, 'marcar': 1, 'getafe': 1, 'celta': 1, 'jugar': 1, 'jugador': 1})\n",
      "defaultdict(<class 'int'>, {'psg': 1, 'partir': 5, 'champions': 1, 'gol': 5, 'ronaldo': 2, 'athletic': 2, 'palmar': 1, 'millón': 2, 'ganar': 2, 'alavés': 2, 'llevar': 2, 'punto': 1, 'villareal': 1, 'local': 1, 'jugador': 2, 'salir': 1, 'málaga': 1, 'atlético': 1, 'marcar': 2, 'pasar': 2, 'mirar': 2, 'equipo': 1, 'jugar': 1, 'leganés': 2, 'europa': 1, 'madrid': 2, 'sociedad': 1, 'equipar': 1, 'aficionar': 2, 'sevilla': 2, 'girona': 1, 'vestir': 1, 'minuto': 3, 'barcelona': 1, 'getafe': 1, 'barça': 1, 'catalán': 1, 'celta': 2, 'espanyol': 2, 'valencia': 1, 'levantar': 1, 'deportivo': 2, 'betis': 1, 'gallego': 1, 'casar': 1})\n",
      "defaultdict(<class 'int'>, {'pitar': 4, 'barça': 2, 'athletic': 1, 'equipo': 1, 'llevar': 1, 'punto': 2, 'árbitro': 2, 'deportivo': 1, 'levantar': 1, 'gallego': 1, 'salir': 2, 'gol': 3, 'minuto': 1, 'valencia': 1, 'madrid': 3, 'real': 2, 'europa': 1, 'demostrar': 1, 'málaga': 1, 'girona': 1, 'jugador': 4, 'partir': 2, 'marcar': 2, 'público': 1, 'estadio': 2, 'sociedad': 1, 'villareal': 1, 'ganar': 1, 'casar': 1, 'leganés': 1, 'espanyol': 1, 'mirar': 1, 'atlético': 1, 'palmar': 1, 'victoria': 1, 'local': 1, 'aficionar': 1, 'sevilla': 2, 'getafe': 2, 'falto': 1, 'barcelona': 2, 'alavés': 1, 'país': 1, 'catalán': 1, 'año': 1, 'celta': 1, 'betis': 2, 'jornada': 1})\n",
      "defaultdict(<class 'int'>, {'real': 4, 'madrid': 3, 'jugar': 2, 'partir': 6, 'deportivo': 3, 'getafe': 1, 'athletic': 1, 'público': 1, 'espanyol': 2, 'sevilla': 1, 'jugador': 4, 'ganar': 2, 'barça': 2, 'mañana': 1, 'atlético': 2, 'girona': 1, 'casar': 1, 'estadio': 1, 'aficionar': 1, 'mostrar': 1, 'levantar': 2, 'villareal': 1, 'tipo': 1, 'palmar': 1, 'valencia': 2, 'punto': 2, 'ligar': 1, 'alavés': 2, 'leganés': 2, 'local': 1, 'marcar': 1, 'gol': 2, 'jornada': 1, 'poner': 2, 'sociedad': 1, 'celta': 1, 'vivir': 1, 'pasar': 1, 'llevar': 1, 'gallego': 1, 'betis': 1, 'barcelona': 1, 'demostrar': 1, 'historia': 1, 'gracia': 1, 'eibar': 2, 'málaga': 2, 'año': 1})\n",
      "defaultdict(<class 'int'>, {'real': 5, 'madrid': 5, 'gracia': 2, 'punto': 3, 'getafe': 1, 'málaga': 1, 'jugador': 5, 'local': 4, 'cabeza': 1, 'volver': 2, 'girona': 2, 'llevar': 1, 'gol': 2, 'casar': 2, 'villareal': 2, 'ganar': 3, 'historia': 1, 'equipar': 4, 'falto': 1, 'eibar': 1, 'atlético': 1, 'aficionar': 1, 'partir': 1, 'deportivo': 1, 'valencia': 1, 'demostrar': 2, 'gallego': 2, 'insistir': 1, 'marcar': 1, 'levantar': 1, 'celta': 3, 'asegurar': 1, 'prensar': 1, 'alavés': 1, 'sevilla': 1, 'espanyol': 2, 'athletic': 1, 'sánchez': 1, 'mostrar': 1, 'presidente': 1, 'semana': 1, 'sociedad': 1, 'barcelona': 1, 'barça': 1, 'año': 1, 'catalán': 1, 'betis': 1, 'leganés': 1, 'jugar': 1})\n",
      "defaultdict(<class 'int'>, {'real': 4, 'madrid': 3, 'punto': 6, 'jugador': 5, 'atlético': 2, 'getafe': 1, 'árbitro': 1, 'público': 1, 'demostrar': 1, 'falto': 1, 'casar': 1, 'mostrar': 1, 'valencia': 2, 'girona': 1, 'volver': 2, 'gracia': 2, 'palmar': 1, 'eibar': 1, 'europa': 1, 'victoria': 1, 'sevilla': 1, 'betis': 1, 'pasar': 1, 'historia': 1, 'gol': 1, 'leganés': 2, 'sociedad': 1, 'temporada': 1, 'champions': 1, 'barcelona': 2, 'levantar': 1, 'catalán': 1, 'villareal': 1, 'deportivo': 1, 'gallego': 1, 'athletic': 1, 'alavés': 1, 'local': 1, 'celta': 1, 'ligar': 1, 'málaga': 2, 'espanyol': 2, 'millón': 1, 'país': 1})\n",
      "defaultdict(<class 'int'>, {'barça': 1, 'temporada': 3, 'sevilla': 1, 'levantar': 1, 'equipo': 2, 'gol': 2, 'año': 2, 'athletic': 1, 'sociedad': 1, 'jugar': 5, 'españolar': 1, 'resultar': 1, 'eibar': 2, 'valencia': 2, 'punto': 2, 'jugador': 3, 'local': 1, 'atlético': 2, 'alavés': 1, 'victoria': 1, 'madrid': 4, 'salir': 1, 'real': 3, 'girona': 2, 'getafe': 1, 'ganar': 2, 'partir': 5, 'celta': 1, 'estadio': 1, 'palmar': 2, 'espanyol': 1, 'historia': 1, 'barcelona': 2, 'deportivo': 1, 'málaga': 1, 'betis': 1, 'hablar': 1, 'leganés': 1, 'jornada': 1, 'equipar': 1, 'importante': 1})\n",
      "defaultdict(<class 'int'>, {'gol': 5, 'jugador': 3, 'futbol': 1, 'barcelona': 1, 'llegar': 2, 'estadio': 1, 'presentar': 1, 'mañana': 1, 'barça': 2, 'equipar': 4, 'catalán': 1, 'presidente': 1, 'mostrar': 1, 'prensar': 1, 'getafe': 1, 'temporada': 1, 'sociedad': 1, 'deportivo': 1, 'champions': 1, 'aficionar': 1, 'marcar': 1, 'declarar': 1, 'españa': 1})\n",
      "defaultdict(<class 'int'>, {'semana': 2, 'alavés': 2, 'palmar': 1, 'jugador': 4, 'partir': 7, 'hora': 1, 'gol': 2, 'asegurar': 1, 'victoria': 1, 'equipo': 2, 'pedir': 1, 'jugar': 3, 'tipo': 1, 'getafe': 1, 'público': 2, 'estadio': 2, 'pasar': 1, 'real': 3, 'madrid': 3, 'sevilla': 1, 'ronaldo': 1, 'vivir': 2, 'deportivo': 1, 'leganés': 1, 'equipar': 4, 'gallego': 1, 'ganar': 2, 'árbitro': 2, 'pitar': 1, 'local': 4, 'valencia': 1, 'celta': 1, 'demostrar': 2, 'sociedad': 1, 'málaga': 2, 'betis': 2, 'atlético': 1, 'poner': 1, 'punto': 1, 'levantar': 1, 'athletic': 1, 'llegar': 1, 'jornada': 1, 'villareal': 1, 'barcelona': 1, 'marcar': 1, 'barça': 1, 'catalán': 2, 'espanyol': 1, 'girona': 1, 'resultar': 1, 'españolar': 1})\n",
      "defaultdict(<class 'int'>, {'real': 4, 'madrid': 3, 'equipar': 3, 'punto': 2, 'málaga': 1, 'levantar': 1, 'partir': 5, 'gol': 4, 'barcelona': 1, 'celta': 2, 'árbitro': 1, 'victoria': 3, 'barça': 1, 'demostrar': 2, 'atlético': 1, 'sociedad': 1, 'local': 3, 'sevilla': 1, 'deportivo': 2, 'jugador': 4, 'poner': 1, 'athletic': 1, 'importante': 1, 'semana': 1, 'volver': 1, 'ligar': 2, 'leganés': 2, 'villareal': 1, 'marcar': 1, 'futbol': 1, 'getafe': 1, 'valencia': 2, 'temporada': 1, 'público': 1, 'pitar': 1, 'jornada': 1, 'eibar': 2, 'espanyol': 1, 'minuto': 1, 'estadio': 1, 'palmar': 1, 'betis': 1, 'llevar': 1, 'jugar': 1, 'girona': 2, 'alavés': 1})\n",
      "defaultdict(<class 'int'>, {'prensar': 1, 'partir': 1, 'semana': 1, 'mañana': 1, 'diario': 1})\n",
      "defaultdict(<class 'int'>, {'levantar': 2, 'cabeza': 4, 'real': 7, 'madrid': 5, 'punto': 5, 'barça': 2, 'girona': 1, 'sociedad': 1, 'catalán': 1, 'hablar': 1, 'árbitro': 3, 'getafe': 2, 'alavés': 1, 'historia': 1, 'ligar': 2, 'partir': 4, 'gol': 4, 'aficionar': 1, 'victoria': 1, 'equipar': 5, 'ganar': 2, 'leganés': 1, 'barcelona': 2, 'jugador': 2, 'salir': 2, 'sevilla': 2, 'celta': 2, 'llevar': 1, 'gallego': 1, 'betis': 2, 'volver': 1, 'sánchez': 1, 'declarar': 1, 'atlético': 1, 'local': 1, 'málaga': 2, 'deportivo': 1, 'falto': 1, 'minuto': 1, 'pedir': 1, 'espanyol': 1, 'valencia': 2, 'jugar': 1, 'vestir': 1, 'athletic': 1, 'futbol': 1, 'marcar': 1, 'eibar': 1, 'vivir': 1, 'equipo': 1})\n",
      "defaultdict(<class 'int'>, {'madrid': 3, 'ligar': 3, 'victoria': 5, 'palmar': 3, 'betis': 1, 'getafe': 1, 'millón': 1, 'historia': 2, 'árbitro': 2, 'partir': 3, 'valencia': 2, 'leganés': 2, 'deportivo': 3, 'local': 1, 'hablar': 2, 'jornada': 1, 'atlético': 2, 'jugador': 2, 'vestir': 2, 'rojo': 1, 'presidente': 1, 'alavés': 2, 'espanyol': 1, 'barcelona': 1, 'sevilla': 2, 'barça': 1, 'ganar': 1, 'catalán': 1, 'punto': 3, 'levantar': 1, 'girona': 1, 'cataluña': 1, 'equipo': 1, 'españolar': 1, 'celta': 1, 'athletic': 1, 'gracia': 2, 'aficionar': 2, 'gallego': 1, 'pitar': 1, 'sociedad': 1, 'eibar': 1, 'tipo': 1, 'llegar': 1, 'año': 1, 'málaga': 2, 'jugar': 1, 'equipar': 2, 'real': 1, 'demostrar': 1, 'importante': 1, 'volver': 1, 'prensar': 1})\n",
      "defaultdict(<class 'int'>, {'jornada': 4, 'ligar': 8, 'gol': 1, 'gobernar': 1, 'españa': 2, 'equipo': 5, 'catalán': 11, 'real': 5, 'madrid': 4, 'jugar': 4, 'temporada': 3, 'barcelona': 2, 'málaga': 1, 'ganar': 1, 'partir': 5, 'historia': 2, 'españolar': 1, 'jugador': 2, 'punto': 5, 'levantar': 1, 'getafe': 1, 'valencia': 4, 'semana': 2, 'pasar': 1, 'eibar': 1, 'victoria': 1, 'líder': 1, 'europeo': 1, 'equipar': 1, 'betis': 2, 'alavés': 1, 'futbol': 1, 'año': 1, 'sevilla': 1, 'poner': 1, 'barça': 1, 'celta': 1, 'atlético': 1, 'gallego': 1, 'leganés': 2, 'athletic': 1, 'villareal': 2, 'palmar': 1, 'minuto': 1, 'sociedad': 1, 'espanyol': 2, 'deportivo': 1, 'girona': 1, 'hora': 1, 'mañana': 1})\n",
      "defaultdict(<class 'int'>, {'real': 5, 'madrid': 6, 'cataluña': 2, 'presidente': 1, 'semana': 1, 'estadio': 1, 'barcelona': 2, 'catalán': 5, 'aficionar': 1, 'declarar': 2, 'insistir': 1, 'españa': 2, 'líder': 1, 'psoe': 1, 'pedro': 1, 'sánchez': 1, 'mariano': 1, 'rajoy': 1})\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "def vectorize(corpus):\n",
    "    features = defaultdict(int)\n",
    "    for token in corpus:\n",
    "        features[token] += 1\n",
    "    return features\n",
    "\n",
    "vectors = map(vectorize, corpus)\n",
    "\n",
    "# Resultados\n",
    "for v in vectors:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de palabras -> palabra: id\n",
      "\n",
      "{'asegurar': 0, 'año': 1, 'cataluña': 2, 'catalán': 3, 'españa': 4, 'gobernar': 5, 'líder': 6, 'poner': 7, 'puigdemont': 8, 'semana': 9, 'cifuentes': 10, 'ciudadano': 11, 'cristino': 12, 'declarar': 13, 'demostrar': 14, 'elección': 15, 'falto': 16, 'madrid': 17, 'mostrar': 18, 'partir': 19, 'prensar': 20, 'presidente': 21, 'votar': 22, 'casar': 23, 'equipar': 24, 'importante': 25, 'insistir': 26, 'jugar': 27, 'ligar': 28, 'mariano': 29, 'mañana': 30, 'mirar': 31, 'país': 32, 'presidencia': 33, 'público': 34, 'rajoy': 35, 'hora': 36, 'españolar': 37, 'pedir': 38, 'presentar': 39, 'psoe': 40, 'tipo': 41, 'millón': 42, 'ministro': 43, 'presupuesto': 44, 'salir': 45, 'informar': 46, 'real': 47, 'vestir': 48, 'volver': 49, 'llevar': 50, 'minuto': 51, 'central': 52, 'diario': 53, 'ganar': 54, 'gracia': 55, 'punto': 56, 'resultar': 57, 'vivir': 58, 'pedro': 59, 'sánchez': 60, 'díaz': 61, 'llegar': 62, 'barcelona': 63, 'cabeza': 64, 'hablar': 65, 'palmar': 66, 'pasar': 67, 'jornada': 68, 'rojo': 69, 'sevilla': 70, 'sociedad': 71, 'victoria': 72, 'gallego': 73, 'iglesia': 74, 'banco': 75, 'bce': 76, 'crisis': 77, 'draghi': 78, 'economía': 79, 'europa': 80, 'europeo': 81, 'guindo': 82, 'curvo': 83, 'fmi': 84, 'gráfico': 85, 'marcar': 86, 'historia': 87, 'alavés': 88, 'athletic': 89, 'atlético': 90, 'barça': 91, 'betis': 92, 'celta': 93, 'champions': 94, 'deportivo': 95, 'eibar': 96, 'espanyol': 97, 'futbol': 98, 'getafe': 99, 'girona': 100, 'jugador': 101, 'leganés': 102, 'levantar': 103, 'málaga': 104, 'valencia': 105, 'villareal': 106, 'árbitro': 107, 'aficionar': 108, 'gol': 109, 'ronaldo': 110, 'temporada': 111, 'estadio': 112, 'psg': 113, 'equipo': 114, 'local': 115, 'pitar': 116}\n",
      "\n",
      "Apariciones de las palabras en los documentos (id, 1):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "vectors = [[(token[0], 1) for token in dictionary.doc2bow(doc)] for doc in corpus]\n",
    "\n",
    "# Resultados\n",
    "print('Diccionario de palabras -> palabra: id\\n')\n",
    "print(dictionary.token2id)\n",
    "print('\\nApariciones de las palabras en los documentos (id, 1):')\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aficionar', 'alavés', 'asegurar', 'athletic', 'atlético', 'año', 'banco', 'barcelona', 'barça', 'bce', 'betis', 'cabeza', 'casar', 'cataluña', 'catalán', 'celta', 'central', 'champions', 'cifuentes', 'ciudadano', 'crisis', 'cristino', 'curvo', 'declarar', 'demostrar', 'deportivo', 'diario', 'draghi', 'díaz', 'economía', 'eibar', 'elección', 'equipar', 'equipo', 'espanyol', 'españa', 'españolar', 'estadio', 'europa', 'europeo', 'falto', 'fmi', 'futbol', 'gallego', 'ganar', 'getafe', 'girona', 'gobernar', 'gol', 'gracia', 'gráfico', 'guindo', 'hablar', 'historia', 'hora', 'iglesia', 'importante', 'informar', 'insistir', 'jornada', 'jugador', 'jugar', 'leganés', 'levantar', 'ligar', 'llegar', 'llevar', 'local', 'líder', 'madrid', 'marcar', 'mariano', 'mañana', 'millón', 'ministro', 'minuto', 'mirar', 'mostrar', 'málaga', 'palmar', 'partir', 'pasar', 'país', 'pedir', 'pedro', 'pitar', 'poner', 'prensar', 'presentar', 'presidencia', 'presidente', 'presupuesto', 'psg', 'psoe', 'puigdemont', 'punto', 'público', 'rajoy', 'real', 'resultar', 'rojo', 'ronaldo', 'salir', 'semana', 'sevilla', 'sociedad', 'sánchez', 'temporada', 'tipo', 'valencia', 'vestir', 'victoria', 'villareal', 'vivir', 'volver', 'votar', 'árbitro']\n",
      "[[0.         0.         0.13474278 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.11751561 0.        ]\n",
      " [0.         0.         0.09035511 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.17243585 0.14731897 0.         ... 0.08941239 0.         0.18588744]\n",
      " [0.         0.04886926 0.         ... 0.         0.         0.        ]\n",
      " [0.11794874 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Transformamos los documentos tokenizados a documentos sin tokenizar\n",
    "for index, doc in enumerate(corpus):\n",
    "    corpus[index] = \" \".join(doc)\n",
    "    \n",
    "tfidf = TfidfVectorizer()\n",
    "corpus_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Resultados\n",
    "print(tfidf.get_feature_names())\n",
    "print(corpus_tfidf.toarray(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
