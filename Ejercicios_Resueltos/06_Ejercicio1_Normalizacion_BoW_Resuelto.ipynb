{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n",
    "\n",
    "* En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n",
    "\n",
    "\n",
    "* Estos artículos se encuentran en un fichero csv dentro de la carpeta \"data\" del proyecto (***./data/corpus_mundo_today.csv***).\n",
    "\n",
    "\n",
    "* Este CSV esta formado por 3 campos que son:\n",
    "    - Tema\n",
    "    - Título\n",
    "    - Texto\n",
    "    \n",
    "    \n",
    "* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n",
    "\n",
    "\n",
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
    "<span></span><br><br>\n",
    "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "<span></span><br><br>        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>        \n",
    "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "* Aprovechando la normalización realizada anteriormente se pide:\n",
    "<span></span><br><br>\n",
    "    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "<span></span><br><br>    \n",
    "    7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras por frecuencia de aparición con NLTK**\n",
    "<span></span><br><br>    \n",
    "    8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n",
    "<span></span><br><br>   \n",
    "    9. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_file = '../data/corpus_mundo_today.csv'\n",
    "docs_list = list()\n",
    "file_txt = open(docs_file, encoding=\"utf8\").read()\n",
    "for line in file_txt.split('\\n'):\n",
    "    line = line.split('||')\n",
    "    docs_list.append(line[1] + ' ' + line[2])\n",
    "docs_list = docs_list[1:] # Elimino la cabecera del fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def tokenization(docs_list):\n",
    "    for index, doc in enumerate(docs_list):\n",
    "        docs_list[index] = [word.text.strip().lower() for word in nlp(doc) if word.text.strip() != \"\"]\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = nlp(\" \".join(doc))\n",
    "        docs[index] = [word.text for word in d if not word.is_punct and not word.is_stop]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = nlp(\" \".join(doc))\n",
    "        docs[index] = [word.lemma_ for word in d]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(docs):\n",
    "    for index, doc in enumerate(docs):\n",
    "        d = nlp(\" \".join(doc))\n",
    "        docs[index] = [word.text for word in d if word.pos_ in ['NOUN', 'PROPN', 'VERB', 'ADV', 'ADJ']]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gobierno', 'español', 'sumar', 'junquera', 'condena', 'cumplir', 'puigdemont', 'revés', 'recibido', 'gobierno', 'españo', 'puesto', 'libertad', 'carl', 'puigdemont', 'justicia', 'alemán', 'juez', 'pablo', 'llarén', 'decidido', 'semana', 'instancia', 'ejecutivo', 'sumar', 'oriol', 'junquera', 'condena', 'cumplir', 'líder', 'pdecat', 'exvicepresidente', 'cataluña', 'permanecer', 'prisión', 'madrileño', 'estremero', 'noviembre', 'asumir', 'delito', 'atribuido', 'carl', 'puigdemont', 'tribunal', 'supremo', 'asegurar', 'acto', 'expresidente', 'catalán', 'legislatura', 'quedar', 'impún', 'junquera', 'pagar', 'maniobra', 'ideado', 'burlar', 'justicia', 'alemán', 'líder', 'esquerro', 'republicano', 'enfrentar', 'año', 'prisión', 'seguir', 'junquera', 'caer', 'año', 'parar', 'carl', 'puigdemont', 'alemania', 'junquera', 'sacrificar', 'asumir', 'resignación', 'determinación', 'prometido', 'seguim', 'tuitear', 'trascender', 'decisión', 'llarén', 'fuente', 'anónimo', 'judicial', 'barajar', 'posibilidad', 'añadir', 'pena', 'oriol', 'junquera', 'condena', 'imponer', 'futuro', 'iñaki', 'urdangarin', 'rodrigo', 'rato', 'esperanza', 'aguirrir', 'delito', 'robo', 'fuerza', 'ocurrido', 'semana', 'huesco', 'policía', 'incapaz', 'encontrar', 'culpable']\n"
     ]
    }
   ],
   "source": [
    "def normalization(docs_list):\n",
    "    corpus = tokenization(docs_list)\n",
    "    corpus = remove_words(corpus)\n",
    "    corpus = lematization(corpus)\n",
    "    corpus = filter_words(corpus)\n",
    "    return corpus\n",
    "\n",
    "corpus = normalization(docs_list)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gobierno', 'español', 'puigdemont', 'gobierno', 'españo', 'puesto', 'puigdemont', 'semana', 'líder', 'cataluña', 'puigdemont', 'asegurar', 'catalán', 'quedar', 'líder', 'año', 'seguir', 'año', 'puigdemont', 'semana']\n"
     ]
    }
   ],
   "source": [
    "def bow_freq (corpus):\n",
    "    \"\"\"Función que cuenta el número de veces que aparece una palabra \n",
    "        en el corpus y lo almacena en un diccionario\n",
    "    \"\"\"\n",
    "    bow = dict()\n",
    "    for doc in corpus:\n",
    "        for word in doc:\n",
    "            if word in bow:\n",
    "                bow[word] += 1\n",
    "            else:\n",
    "                bow[word] = 1\n",
    "    return bow\n",
    "\n",
    "def drop_less_frecuency_words(corpus, n):\n",
    "    bow = bow_freq(corpus)\n",
    "    for index, doc in enumerate(corpus):\n",
    "        corpus[index] = [word for word in doc if bow[word] >= n]\n",
    "    return corpus\n",
    "\n",
    "corpus = drop_less_frecuency_words(corpus, 10)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras por frecuencia de aparición con NLTK**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'gobierno': 2, 'español': 1, 'puigdemont': 4, 'españo': 1, 'puesto': 1, 'semana': 2, 'líder': 2, 'cataluña': 1, 'asegurar': 1, 'catalán': 1, 'quedar': 1, 'año': 2, 'seguir': 1})\n",
      "defaultdict(<class 'int'>, {'ciudadano': 4, 'cifuent': 4, 'año': 3, 'venir': 3, 'elección': 3, 'gobierno': 2, 'partido': 2, 'semana': 1, 'presidenta': 2, 'madrid': 3, 'hacer': 1, 'líder': 1, 'demostrar': 1, 'puesto': 2, 'pasar': 1})\n",
      "defaultdict(<class 'int'>, {'mariano': 3, 'rajoy': 5, 'presidencia': 5, 'españo': 2, 'recordar': 2, 'líder': 1, 'partido': 2, 'cifuent': 1, 'año': 2, 'país': 1, 'equipo': 1, 'importante': 1, 'jugar': 1, 'mañana': 1, 'ligo': 1, 'liga': 1, 'nombre': 1, 'demostrar': 1, 'gobierno': 1, 'público': 1, 'decir': 1, 'asegurar': 1, 'hacer': 1, 'ciudadano': 1, 'español': 1})\n",
      "defaultdict(<class 'int'>, {'cifuent': 5, 'empezar': 1, 'presidenta': 1, 'madrid': 1, 'nombre': 2, 'mañana': 1, 'asegurar': 1, 'hora': 1, 'español': 1, 'presidencia': 1, 'gobierno': 1, 'líder': 1, 'españa': 1})\n",
      "defaultdict(<class 'int'>, {'puigdemont': 2, 'pedir': 2, 'cifuent': 2, 'rajoy': 3, 'españa': 1, 'cataluña': 1, 'presidenta': 1, 'madrid': 3, 'español': 2, 'catalán': 1, 'asegurar': 1, 'nombre': 1, 'tipo': 1, 'psoe': 1, 'presidencia': 1, 'hora': 1, 'gobierno': 1, 'partido': 1})\n",
      "defaultdict(<class 'int'>, {'cifuent': 5, 'presidenta': 2, 'madrid': 1, 'gobierno': 1, 'asegurar': 1, 'psoe': 2, 'demostrar': 2, 'ciudadano': 2, 'decir': 1})\n",
      "defaultdict(<class 'int'>, {'español': 4, 'presupuesto': 8, 'ministro': 3, 'salir': 2, 'mañana': 1, 'pasar': 1, 'gobierno': 2, 'pedir': 2, 'decir': 2, 'asegurar': 1, 'equipo': 1, 'millón': 1})\n",
      "defaultdict(<class 'int'>, {'presupuesto': 5, 'ministro': 1, 'importante': 1, 'año': 3, 'partido': 2, 'marcar': 1, 'español': 1, 'gobierno': 3, 'pasar': 1, 'mañana': 1, 'casa': 1, 'millón': 1})\n",
      "defaultdict(<class 'int'>, {'mariano': 3, 'rajoy': 4, 'españa': 5, 'asegurar': 1, 'españo': 2, 'salir': 1, 'país': 1, 'catalán': 1, 'real': 1, 'volver': 3, 'presidente': 1, 'nombre': 1, 'español': 1, 'tipo': 1})\n",
      "defaultdict(<class 'int'>, {'arrimada': 4, 'presidenta': 2, 'casa': 2, 'presidente': 1, 'catalán': 1, 'cataluña': 1, 'minuto': 1, 'llevar': 1, 'hora': 1, 'pedir': 1})\n",
      "defaultdict(<class 'int'>, {'ciudadano': 3, 'elección': 4, 'diario': 1, 'país': 2, 'punto': 2, 'psoe': 1, 'puesto': 1, 'presidente': 3, 'partido': 2, 'mariano': 2, 'rajoy': 3, 'gracias': 1, 'presidencia': 1, 'arrimada': 1, 'cataluña': 1, 'líder': 1, 'central': 1, 'madrid': 1, 'español': 1, 'resultado': 1})\n",
      "defaultdict(<class 'int'>, {'rajoy': 4, 'puigdemont': 2, 'seguir': 1, 'presidente': 1, 'gobierno': 1, 'españo': 1, 'mariano': 2, 'catalán': 1, 'hacer': 1, 'madrid': 1})\n",
      "defaultdict(<class 'int'>, {'puigdemont': 5, 'gobierno': 4, 'españo': 2, 'presidente': 1, 'cataluña': 3, 'español': 3, 'catalán': 3, 'mariano': 3, 'rajoy': 5, 'minuto': 1, 'dejar': 1, 'demostrar': 1, 'hora': 1, 'elección': 1, 'central': 1, 'españa': 1, 'equipo': 1})\n",
      "defaultdict(<class 'int'>, {'rajoy': 3, 'volver': 2, 'ganar': 1, 'elección': 1, 'resultado': 3, 'mariano': 2, 'minuto': 2, 'gobierno': 2, 'españo': 1, 'presidente': 2, 'españa': 2, 'partido': 4, 'demostrar': 1, 'psoe': 1, 'seguir': 1, 'líder': 1, 'ciudadano': 1, 'pedro': 1, 'sánchez': 1})\n",
      "defaultdict(<class 'int'>, {'arrimada': 7, 'país': 7, 'ciudadano': 2, 'presidencia': 2, 'catalán': 2, 'diario': 4, 'elección': 1, 'mañana': 1, 'presidenta': 2, 'cataluña': 1, 'presidente': 1, 'gobierno': 1, 'dejar': 1, 'llegar': 1, 'equipo': 1, 'partido': 1, 'español': 1, 'díaz': 1})\n",
      "defaultdict(<class 'int'>, {'cabeza': 3, 'ciudadano': 7, 'decir': 2, 'importante': 1, 'país': 1, 'año': 1, 'barcelón': 1, 'dejar': 1, 'llegar': 2, 'resultado': 1, 'hablar': 1, 'seguir': 1, 'arrimada': 1})\n",
      "defaultdict(<class 'int'>, {'pedro': 3, 'sánchez': 6, 'rajoy': 4, 'hacer': 3, 'díaz': 3, 'líder': 2, 'partido': 1, 'presidencia': 1, 'catalán': 1, 'presidente': 2, 'gobierno': 1, 'mariano': 1, 'hablar': 1, 'psoe': 2, 'decir': 1, 'asegurar': 1, 'ver': 1})\n",
      "defaultdict(<class 'int'>, {'pedro': 6, 'sánchez': 5, 'seguir': 1, 'líder': 1, 'mañana': 2, 'llevar': 1, 'psoe': 1, 'ver': 2, 'pasar': 1, 'año': 2, 'decir': 1, 'presidente': 1, 'hora': 1, 'quedar': 2})\n",
      "defaultdict(<class 'int'>, {'díaz': 7, 'decir': 2, 'rojo': 1, 'victoria': 2, 'psoe': 2, 'ciudadano': 1, 'español': 1, 'sánchez': 2, 'sociedad': 1, 'presidenta': 2, 'jornada': 1, 'seguir': 1})\n",
      "defaultdict(<class 'int'>, {'diario': 5, 'país': 5, 'nombre': 4, 'resultado': 1, 'psoe': 1, 'victoria': 1, 'pedro': 2, 'sánchez': 2, 'asegurar': 1, 'díaz': 1, 'demostrar': 1, 'quedar': 1, 'decir': 1, 'hora': 1})\n",
      "defaultdict(<class 'int'>, {'rajoy': 3, 'asegurar': 1, 'seguir': 1, 'mariano': 2, 'recordar': 1, 'presidente': 1, 'presupuesto': 1, 'psoe': 1, 'llevar': 1, 'partido': 1, 'elección': 1, 'gallego': 1})\n",
      "defaultdict(<class 'int'>, {'iglesia': 6, 'partido': 2, 'españo': 1, 'elección': 1, 'líder': 2, 'seguir': 1, 'hora': 1})\n",
      "defaultdict(<class 'int'>, {'seguir': 1, 'elección': 2, 'mañana': 1, 'psoe': 3, 'dejar': 1, 'españa': 1, 'quedar': 1, 'partido': 1, 'iglesia': 2, 'ciudadano': 1, 'tipo': 1, 'hora': 1, 'resultado': 1})\n",
      "defaultdict(<class 'int'>, {'guindo': 5, 'pedir': 2, 'español': 3, 'banco': 2, 'central': 2, 'europeo': 5, 'empezar': 2, 'millón': 1, 'europa': 2, 'tipo': 1, 'ministro': 2, 'economía': 1, 'recordar': 1, 'año': 1, 'españo': 1, 'puesto': 1, 'españa': 2, 'crisis': 1, 'llegar': 1, 'bce': 1, 'presidente': 1, 'draghi': 1})\n",
      "defaultdict(<class 'int'>, {'europeo': 2, 'año': 2, 'público': 2, 'español': 1, 'economía': 2, 'conseguir': 1, 'jornada': 1, 'casa': 1, 'hacer': 1, 'mañana': 2, 'seguir': 1, 'bce': 1, 'crisis': 1})\n",
      "defaultdict(<class 'int'>, {'presidente': 3, 'banco': 3, 'central': 2, 'europeo': 3, 'hablar': 2, 'draghi': 4, 'bce': 1, 'recordar': 1, 'público': 1, 'economía': 1, 'europa': 1, 'crisis': 1, 'pedir': 1, 'salir': 1})\n",
      "defaultdict(<class 'int'>, {'crisis': 5, 'gráfico': 13, 'draghi': 3, 'presidente': 2, 'banco': 2, 'central': 1, 'europeo': 3, 'fmi': 1, 'rojo': 7, 'bce': 3, 'tipo': 2, 'gobierno': 2, 'economía': 1, 'llevar': 1, 'dejar': 1, 'hablar': 1, 'país': 1})\n",
      "defaultdict(<class 'int'>, {'bce': 4, 'volver': 1, 'tipo': 5, 'economía': 1, 'europeo': 2, 'banco': 1, 'central': 1, 'punto': 1, 'presidente': 2, 'draghi': 3, 'minuto': 1, 'público': 1, 'ver': 2, 'mañana': 1, 'llegar': 1, 'quedar': 1, 'europa': 1, 'dejar': 1})\n",
      "defaultdict(<class 'int'>, {'guindo': 5, 'banco': 3, 'central': 3, 'europeo': 2, 'gobierno': 2, 'españo': 1, 'ministro': 2, 'puesto': 3, 'economía': 2, 'español': 3, 'quedar': 1, 'país': 1, 'draghi': 2, 'españa': 1, 'conseguir': 1, 'importante': 1, 'rajoy': 1, 'año': 2, 'demostrar': 1, 'llegar': 1})\n",
      "defaultdict(<class 'int'>, {'gobierno': 2, 'crisis': 3, 'español': 4, 'volver': 1, 'ministro': 1, 'economía': 5, 'guindo': 3, 'mañana': 1, 'sociedad': 1, 'dejar': 1, 'año': 1, 'empezar': 1, 'real': 1})\n",
      "defaultdict(<class 'int'>, {'país': 4, 'mañana': 1, 'fmi': 3, 'iglesia': 3, 'millón': 2, 'presidente': 1, 'recordar': 1, 'españo': 1, 'hacer': 1, 'partido': 1})\n",
      "defaultdict(<class 'int'>, {'fmi': 5, 'pedir': 1, 'españa': 4, 'gobierno': 1, 'mariano': 1, 'rajoy': 1, 'español': 1, 'llevar': 1, 'mañana': 1, 'venir': 1})\n",
      "defaultdict(<class 'int'>, {'dejar': 1, 'diario': 1, 'economía': 1, 'ministro': 1, 'público': 1, 'gobierno': 2, 'seguir': 1, 'salir': 1, 'europeo': 3, 'crisis': 1, 'llegar': 1, 'asegurar': 1, 'presidente': 2, 'año': 1, 'país': 1, 'rojo': 2, 'fmi': 1})\n",
      "defaultdict(<class 'int'>, {'fmi': 4, 'año': 4, 'venir': 3, 'españo': 1, 'mañana': 1, 'español': 3, 'seguir': 2, 'resultado': 1, 'importante': 1, 'llegar': 3, 'banco': 1, 'españa': 1, 'semana': 1, 'ver': 1, 'pasar': 2, 'gobierno': 1, 'ciudadano': 1})\n",
      "defaultdict(<class 'int'>, {'barça': 1, 'minuto': 2, 'liga': 2, 'girón': 1, 'jugador': 2, 'seguir': 3, 'ganar': 4, 'partido': 7, 'europa': 1, 'año': 3, 'venir': 1, 'sevillo': 1, 'villareal': 1, 'semana': 1, 'valencia': 2, 'catalán': 1, 'llegar': 1, 'gracias': 1, 'leganés': 2, 'celta': 2, 'punto': 1, 'futbol': 4, 'palma': 1, 'real': 3, 'sociedad': 1, 'demostrar': 1, 'hora': 1, 'athletic': 1, 'deportivo': 2, 'eibar': 1, 'alavés': 1, 'atlético': 2, 'levante': 1, 'champions': 1, 'getafir': 1, 'espanyol': 1, 'ver': 1, 'empezar': 1, 'madrid': 2, 'conseguir': 1, 'árbitro': 1, 'campo': 1})\n",
      "defaultdict(<class 'int'>, {'real': 5, 'madrid': 3, 'ronaldo': 2, 'levante': 2, 'eibar': 1, 'jugador': 3, 'seguir': 1, 'año': 2, 'futbol': 2, 'deportivo': 1, 'palma': 1, 'gallego': 1, 'ganar': 2, 'partido': 2, 'pasar': 2, 'valencia': 1, 'alavés': 1, 'volver': 2, 'venir': 1, 'pedir': 1, 'cabeza': 2, 'presidente': 1, 'recordar': 2, 'sociedad': 1, 'getafir': 1, 'empezar': 2, 'espanyol': 1, 'betis': 1, 'jugar': 1, 'conseguir': 1, 'victoria': 2, 'equipo': 1, 'leganés': 1, 'sevillo': 2, 'athletic': 1, 'barça': 1, 'ver': 1, 'catalán': 1, 'temporada': 2, 'villareal': 1, 'atlético': 2, 'ligo': 1, 'importante': 1, 'celta': 1, 'hacer': 1, 'liga': 1, 'aficionado': 1, 'gol': 1})\n",
      "defaultdict(<class 'int'>, {'liga': 3, 'español': 1, 'barça': 4, 'ganar': 3, 'villareal': 2, 'girón': 1, 'llevar': 1, 'punto': 4, 'demostrar': 1, 'athletic': 1, 'sevillo': 1, 'aficionado': 2, 'equipo': 2, 'año': 1, 'deportivo': 2, 'eibar': 2, 'jugador': 3, 'marcar': 1, 'gol': 1, 'leganés': 1, 'jornada': 1, 'partido': 1, 'estadio': 1, 'salir': 3, 'real': 3, 'madrid': 2, 'conseguir': 2, 'victoria': 2, 'getafir': 1, 'levante': 1, 'espanyol': 3, 'valencia': 1, 'sánchez': 1, 'temporada': 2, 'ver': 1, 'árbitro': 1, 'resultado': 1, 'jugar': 2, 'barcelón': 1, 'atlético': 2, 'gracias': 2, 'catalán': 1, 'sociedad': 1, 'alavés': 1, 'volver': 1, 'quedar': 1, 'europa': 1, 'valencio': 1, 'empezar': 2, 'pedir': 2, 'celta': 1, 'palma': 1, 'champions': 1, 'semana': 1})\n",
      "defaultdict(<class 'int'>, {'pitar': 3, 'barça': 2, 'girón': 1, 'leganés': 1, 'catalán': 1, 'partido': 2, 'llevar': 1, 'ronaldo': 5, 'gol': 3, 'palma': 1, 'sevillo': 2, 'punto': 2, 'importante': 2, 'victoria': 1, 'real': 9, 'madrid': 10, 'champions': 5, 'eibar': 1, 'local': 1, 'millón': 1, 'año': 1, 'gracias': 1, 'alavés': 2, 'deportivo': 1, 'seguir': 2, 'hablar': 1, 'europa': 1, 'ganar': 5, 'valencia': 1, 'valencio': 1, 'resultado': 2, 'sociedad': 2, 'levante': 2, 'casa': 1, 'atlético': 2, 'athletic': 2, 'estadio': 1, 'espanyol': 1, 'equipo': 1, 'betis': 1, 'volver': 3, 'semana': 1, 'marcar': 1, 'getafir': 1, 'celta': 1, 'jugar': 1, 'jugador': 1})\n",
      "defaultdict(<class 'int'>, {'pedir': 1, 'partido': 5, 'champions': 1, 'gol': 5, 'ronaldo': 2, 'athletic': 2, 'palma': 1, 'conseguir': 2, 'quedar': 1, 'millón': 1, 'empezar': 3, 'ganar': 2, 'alavés': 1, 'puesto': 1, 'llevar': 1, 'punto': 1, 'campo': 1, 'villareal': 1, 'local': 1, 'dejar': 2, 'jugador': 2, 'salir': 1, 'atlético': 1, 'marcar': 2, 'asegurar': 1, 'pasar': 3, 'decir': 1, 'equipo': 2, 'volver': 1, 'jugar': 1, 'leganés': 2, 'eibar': 2, 'europa': 1, 'madrid': 2, 'sociedad': 1, 'aficionado': 3, 'venir': 1, 'girón': 1, 'sevillo': 1, 'minuto': 3, 'hacer': 1, 'barcelón': 1, 'catalán': 1, 'celta': 2, 'espanyol': 2, 'valencio': 1, 'levante': 1, 'deportivo': 2, 'betis': 1, 'gallego': 1})\n",
      "defaultdict(<class 'int'>, {'barça': 2, 'athletic': 1, 'eibar': 1, 'equipo': 2, 'llevar': 1, 'punto': 2, 'árbitro': 2, 'deportivo': 1, 'levante': 1, 'gallego': 1, 'salir': 1, 'ver': 1, 'dejar': 1, 'gol': 3, 'minuto': 1, 'madrid': 3, 'real': 2, 'volver': 2, 'europa': 1, 'demostrar': 1, 'pitar': 3, 'girón': 1, 'liga': 1, 'jugador': 4, 'partido': 3, 'marcar': 2, 'público': 1, 'estadio': 2, 'casa': 2, 'sociedad': 1, 'villareal': 1, 'ganar': 1, 'leganés': 1, 'espanyol': 1, 'llegar': 1, 'atlético': 1, 'palma': 1, 'victoria': 1, 'local': 1, 'aficionado': 1, 'sevillo': 1, 'jugar': 2, 'barcelón': 1, 'alavés': 1, 'país': 1, 'catalán': 1, 'año': 1, 'celta': 1, 'betis': 2, 'jornada': 1})\n",
      "defaultdict(<class 'int'>, {'real': 4, 'madrid': 3, 'jugar': 2, 'deportivo': 3, 'getafir': 1, 'athletic': 1, 'conseguir': 2, 'público': 1, 'espanyol': 2, 'jugador': 4, 'partido': 6, 'ganar': 2, 'barça': 2, 'mañana': 1, 'atlético': 2, 'volver': 1, 'casa': 1, 'estadio': 1, 'aficionado': 1, 'levante': 2, 'villareal': 1, 'tipo': 1, 'palma': 1, 'valencia': 2, 'punto': 2, 'liga': 1, 'equipo': 1, 'alavés': 2, 'leganés': 2, 'millón': 1, 'local': 1, 'marcar': 1, 'gol': 2, 'jornada': 1, 'quedar': 2, 'hacer': 1, 'puesto': 2, 'sociedad': 1, 'celta': 1, 'demostrar': 2, 'pasar': 1, 'campo': 1, 'llevar': 1, 'gallego': 1, 'betis': 1, 'gracias': 1, 'eibar': 2, 'año': 1})\n",
      "defaultdict(<class 'int'>, {'real': 5, 'madrid': 5, 'gracias': 2, 'punto': 3, 'getafir': 1, 'jugador': 5, 'local': 4, 'cabeza': 1, 'palma': 2, 'equipo': 5, 'llevar': 1, 'gol': 2, 'girón': 1, 'dejar': 1, 'pasar': 1, 'casa': 3, 'villareal': 2, 'ganar': 3, 'eibar': 1, 'atlético': 1, 'aficionado': 1, 'partido': 2, 'deportivo': 1, 'valencia': 1, 'demostrar': 2, 'gallego': 2, 'marcar': 1, 'levante': 1, 'celta': 3, 'volver': 2, 'espanyol': 2, 'athletic': 1, 'sánchez': 1, 'asegurar': 1, 'seguir': 1, 'presidente': 1, 'semana': 1, 'sociedad': 1, 'barcelón': 1, 'año': 1, 'catalán': 1, 'betis': 1, 'leganés': 1, 'jugar': 1})\n",
      "defaultdict(<class 'int'>, {'real': 4, 'madrid': 3, 'punto': 6, 'jugador': 5, 'atlético': 2, 'árbitro': 1, 'público': 1, 'demostrar': 1, 'casa': 1, 'valencio': 2, 'volver': 2, 'gracias': 2, 'campo': 2, 'palma': 1, 'eibar': 2, 'europa': 1, 'victoria': 1, 'aficionado': 1, 'pasar': 1, 'gol': 1, 'leganés': 2, 'sociedad': 1, 'temporada': 1, 'empezar': 2, 'champions': 1, 'barcelón': 2, 'levante': 1, 'catalán': 1, 'villareal': 1, 'deportivo': 1, 'gallego': 1, 'conseguir': 1, 'semana': 1, 'venir': 1, 'athletic': 1, 'alavés': 1, 'local': 1, 'celta': 1, 'quedar': 1, 'equipo': 1, 'ligo': 1, 'espanyol': 1, 'millón': 1, 'país': 1})\n",
      "defaultdict(<class 'int'>, {'barça': 1, 'temporada': 3, 'levante': 1, 'equipo': 3, 'gol': 2, 'año': 2, 'athletic': 1, 'sociedad': 1, 'partido': 6, 'volver': 1, 'jugar': 5, 'español': 1, 'resultado': 1, 'eibar': 2, 'valencia': 1, 'punto': 2, 'valencio': 1, 'jugador': 3, 'local': 1, 'conseguir': 1, 'atlético': 2, 'alavés': 1, 'victoria': 1, 'madrid': 4, 'salir': 1, 'real': 3, 'girón': 1, 'ganar': 2, 'empezar': 3, 'seguir': 1, 'celta': 1, 'estadio': 1, 'palma': 1, 'espanyol': 1, 'nombre': 1, 'deportivo': 1, 'barcelón': 1, 'hablar': 1, 'quedar': 1, 'leganés': 1, 'jornada': 1, 'importante': 1})\n",
      "defaultdict(<class 'int'>, {'gol': 5, 'jugador': 3, 'futbol': 1, 'estadio': 1, 'mañana': 1, 'aficionado': 2, 'equipo': 5, 'catalán': 1, 'conseguir': 3, 'presidente': 1, 'temporada': 1, 'sociedad': 1, 'deportivo': 1, 'champions': 1, 'marcar': 1, 'españa': 1, 'español': 1, 'barça': 1, 'dejar': 1})\n",
      "defaultdict(<class 'int'>, {'semana': 2, 'alavés': 2, 'palma': 2, 'jugador': 4, 'empezar': 1, 'partido': 7, 'hora': 1, 'gol': 2, 'asegurar': 1, 'victoria': 1, 'equipo': 6, 'dejar': 2, 'jugar': 3, 'tipo': 1, 'getafir': 1, 'eibar': 1, 'público': 2, 'estadio': 2, 'pasar': 1, 'ver': 2, 'real': 3, 'madrid': 3, 'ronaldo': 1, 'deportivo': 1, 'leganés': 1, 'gallego': 1, 'volver': 2, 'ganar': 2, 'árbitro': 2, 'pitar': 1, 'local': 4, 'seguir': 1, 'celta': 1, 'demostrar': 2, 'sociedad': 1, 'betis': 2, 'atlético': 1, 'llevar': 1, 'punto': 1, 'levante': 1, 'athletic': 1, 'llegar': 1, 'villareal': 1, 'barça': 1, 'catalán': 2, 'girón': 1, 'resultado': 1, 'español': 1})\n",
      "defaultdict(<class 'int'>, {'real': 4, 'madrid': 3, 'equipo': 3, 'punto': 2, 'levante': 1, 'partido': 7, 'gol': 4, 'barcelón': 1, 'celta': 2, 'árbitro': 1, 'victoria': 3, 'barça': 1, 'demostrar': 3, 'atlético': 1, 'sociedad': 1, 'local': 3, 'conseguir': 1, 'minuto': 4, 'deportivo': 2, 'jugador': 4, 'athletic': 1, 'importante': 1, 'volver': 1, 'liga': 1, 'leganés': 2, 'villareal': 1, 'marcar': 1, 'futbol': 1, 'getafir': 1, 'valencia': 1, 'valencio': 1, 'público': 1, 'jornada': 1, 'eibar': 2, 'espanyol': 1, 'recordar': 1, 'estadio': 1, 'ligo': 1, 'palma': 1, 'llevar': 1, 'jugar': 1, 'alavés': 1, 'girón': 1, 'quedar': 1})\n",
      "defaultdict(<class 'int'>, {'liga': 1, 'recordar': 1, 'partido': 1, 'semana': 1, 'mañana': 1, 'diario': 1})\n",
      "defaultdict(<class 'int'>, {'cabeza': 4, 'volver': 2, 'real': 7, 'madrid': 5, 'punto': 5, 'barça': 2, 'girón': 1, 'sociedad': 1, 'catalán': 1, 'hablar': 1, 'demostrar': 1, 'árbitro': 3, 'getafir': 1, 'alavés': 1, 'ligo': 1, 'partido': 4, 'gol': 4, 'aficionado': 1, 'victoria': 1, 'equipo': 6, 'ganar': 2, 'leganés': 1, 'jugador': 2, 'barcelón': 1, 'salir': 2, 'liga': 1, 'celta': 2, 'llevar': 1, 'gallego': 1, 'campo': 2, 'betis': 1, 'sevillo': 1, 'atlético': 1, 'pitar': 1, 'local': 1, 'deportivo': 1, 'minuto': 1, 'pedir': 1, 'espanyol': 1, 'valencia': 2, 'athletic': 1, 'futbol': 1, 'jugar': 1, 'marcar': 1, 'eibar': 1})\n",
      "defaultdict(<class 'int'>, {'madrid': 3, 'ligo': 2, 'victoria': 5, 'betis': 1, 'millón': 1, 'aficionado': 3, 'pasar': 1, 'empezar': 1, 'árbitro': 2, 'pitar': 2, 'partido': 4, 'leganés': 2, 'deportivo': 3, 'local': 1, 'hablar': 2, 'valencia': 1, 'conseguir': 2, 'jornada': 1, 'liga': 1, 'atlético': 2, 'jugador': 2, 'rojo': 1, 'presidente': 1, 'alavés': 2, 'espanyol': 1, 'volver': 3, 'sevillo': 1, 'barça': 1, 'ganar': 1, 'catalán': 1, 'punto': 4, 'levante': 1, 'cataluña': 1, 'equipo': 4, 'español': 2, 'celta': 1, 'athletic': 1, 'gracias': 2, 'campo': 2, 'llegar': 2, 'gallego': 1, 'casa': 1, 'sociedad': 1, 'eibar': 1, 'tipo': 1, 'hacer': 1, 'año': 1, 'jugar': 1, 'real': 1, 'palma': 2, 'demostrar': 1, 'importante': 1, 'seguir': 1})\n",
      "defaultdict(<class 'int'>, {'jornada': 4, 'ligo': 3, 'gol': 1, 'gobierno': 1, 'españo': 1, 'equipo': 7, 'catalán': 11, 'real': 5, 'madrid': 4, 'hacer': 1, 'liga': 7, 'temporada': 2, 'barcelón': 1, 'ganar': 1, 'partido': 5, 'español': 1, 'jugador': 2, 'punto': 5, 'jugar': 4, 'levante': 1, 'getafir': 1, 'valencio': 3, 'semana': 1, 'venir': 2, 'pasar': 1, 'españa': 1, 'eibar': 1, 'victoria': 1, 'líder': 1, 'europeo': 1, 'betis': 2, 'alavés': 1, 'futbol': 1, 'año': 1, 'valencia': 1, 'puesto': 2, 'barça': 1, 'celta': 1, 'atlético': 1, 'campo': 1, 'gallego': 1, 'ver': 1, 'leganés': 2, 'athletic': 1, 'villareal': 2, 'palma': 1, 'minuto': 1, 'seguir': 1, 'sociedad': 1, 'espanyol': 2, 'deportivo': 1, 'hora': 1, 'mañana': 1})\n",
      "defaultdict(<class 'int'>, {'real': 5, 'madrid': 6, 'cataluña': 2, 'presidente': 1, 'semana': 1, 'estadio': 1, 'barcelón': 1, 'catalán': 5, 'aficionado': 1, 'españo': 1, 'líder': 1, 'psoe': 1, 'pedro': 1, 'sánchez': 1, 'quedar': 1, 'españa': 1, 'mariano': 1, 'rajoy': 1})\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "def vectorize(corpus):\n",
    "    features = defaultdict(int)\n",
    "    for token in corpus:\n",
    "        features[token] += 1\n",
    "    return features\n",
    "\n",
    "vectors = map(vectorize, corpus)\n",
    "\n",
    "# Resultados\n",
    "for v in vectors:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de palabras -> palabra: id\n",
      "\n",
      "{'asegurar': 0, 'año': 1, 'cataluña': 2, 'catalán': 3, 'españo': 4, 'español': 5, 'gobierno': 6, 'líder': 7, 'puesto': 8, 'puigdemont': 9, 'quedar': 10, 'seguir': 11, 'semana': 12, 'cifuent': 13, 'ciudadano': 14, 'demostrar': 15, 'elección': 16, 'hacer': 17, 'madrid': 18, 'partido': 19, 'pasar': 20, 'presidenta': 21, 'venir': 22, 'decir': 23, 'equipo': 24, 'importante': 25, 'jugar': 26, 'liga': 27, 'ligo': 28, 'mariano': 29, 'mañana': 30, 'nombre': 31, 'país': 32, 'presidencia': 33, 'público': 34, 'rajoy': 35, 'recordar': 36, 'empezar': 37, 'españa': 38, 'hora': 39, 'pedir': 40, 'psoe': 41, 'tipo': 42, 'millón': 43, 'ministro': 44, 'presupuesto': 45, 'salir': 46, 'casa': 47, 'marcar': 48, 'presidente': 49, 'real': 50, 'volver': 51, 'arrimada': 52, 'llevar': 53, 'minuto': 54, 'central': 55, 'diario': 56, 'gracias': 57, 'punto': 58, 'resultado': 59, 'dejar': 60, 'ganar': 61, 'pedro': 62, 'sánchez': 63, 'díaz': 64, 'llegar': 65, 'barcelón': 66, 'cabeza': 67, 'hablar': 68, 'ver': 69, 'jornada': 70, 'rojo': 71, 'sociedad': 72, 'victoria': 73, 'gallego': 74, 'iglesia': 75, 'banco': 76, 'bce': 77, 'crisis': 78, 'draghi': 79, 'economía': 80, 'europa': 81, 'europeo': 82, 'guindo': 83, 'conseguir': 84, 'fmi': 85, 'gráfico': 86, 'alavés': 87, 'athletic': 88, 'atlético': 89, 'barça': 90, 'campo': 91, 'celta': 92, 'champions': 93, 'deportivo': 94, 'eibar': 95, 'espanyol': 96, 'futbol': 97, 'getafir': 98, 'girón': 99, 'jugador': 100, 'leganés': 101, 'levante': 102, 'palma': 103, 'sevillo': 104, 'valencia': 105, 'villareal': 106, 'árbitro': 107, 'aficionado': 108, 'betis': 109, 'gol': 110, 'ronaldo': 111, 'temporada': 112, 'estadio': 113, 'valencio': 114, 'local': 115, 'pitar': 116}\n",
      "\n",
      "Apariciones de las palabras en los documentos (id, 1):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (9, 1),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(corpus)\n",
    "vectors = [[(token[0], 1) for token in dictionary.doc2bow(doc)] for doc in corpus]\n",
    "\n",
    "# Resultados\n",
    "print('Diccionario de palabras -> palabra: id\\n')\n",
    "print(dictionary.token2id)\n",
    "print('\\nApariciones de las palabras en los documentos (id, 1):')\n",
    "vectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aficionado', 'alavés', 'arrimada', 'asegurar', 'athletic', 'atlético', 'año', 'banco', 'barcelón', 'barça', 'bce', 'betis', 'cabeza', 'campo', 'casa', 'cataluña', 'catalán', 'celta', 'central', 'champions', 'cifuent', 'ciudadano', 'conseguir', 'crisis', 'decir', 'dejar', 'demostrar', 'deportivo', 'diario', 'draghi', 'díaz', 'economía', 'eibar', 'elección', 'empezar', 'equipo', 'espanyol', 'españa', 'españo', 'español', 'estadio', 'europa', 'europeo', 'fmi', 'futbol', 'gallego', 'ganar', 'getafir', 'girón', 'gobierno', 'gol', 'gracias', 'gráfico', 'guindo', 'hablar', 'hacer', 'hora', 'iglesia', 'importante', 'jornada', 'jugador', 'jugar', 'leganés', 'levante', 'liga', 'ligo', 'llegar', 'llevar', 'local', 'líder', 'madrid', 'marcar', 'mariano', 'mañana', 'millón', 'ministro', 'minuto', 'nombre', 'palma', 'partido', 'pasar', 'país', 'pedir', 'pedro', 'pitar', 'presidencia', 'presidenta', 'presidente', 'presupuesto', 'psoe', 'puesto', 'puigdemont', 'punto', 'público', 'quedar', 'rajoy', 'real', 'recordar', 'resultado', 'rojo', 'ronaldo', 'salir', 'seguir', 'semana', 'sevillo', 'sociedad', 'sánchez', 'temporada', 'tipo', 'valencia', 'valencio', 'venir', 'ver', 'victoria', 'villareal', 'volver', 'árbitro']\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.23388801 0.14192601 0.         ... 0.         0.19573158 0.17397362]\n",
      " [0.         0.05077815 0.         ... 0.11157363 0.         0.        ]\n",
      " [0.11959074 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Transformamos los documentos tokenizados a documentos sin tokenizar\n",
    "for index, doc in enumerate(corpus):\n",
    "    corpus[index] = \" \".join(doc)\n",
    "    \n",
    "tfidf = TfidfVectorizer()\n",
    "corpus_tfidf = tfidf.fit_transform(corpus)\n",
    "\n",
    "# Resultados\n",
    "print(tfidf.get_feature_names())\n",
    "print(corpus_tfidf.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
