{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Clasificación de textos con NLTK y Scikit-Learn\n",
    "\n",
    "* NLTK dispone de un wrapper para hacer uso de los clasificadores de Scikit-Learn.\n",
    "\n",
    "\n",
    "* Este wrapper consiste básicamente en pasarle una ***bolsa de palabras*** (en formato NLTK) y un ***objeto de alguna clase de Scikit que implemente un clasificador*** (Naive Bayes, Regresión Logística, SVM, etc.)\n",
    "\n",
    "\n",
    "* En este notebook (al igual que en el notebbok \"09_Sckit_Clasificacion_textos.ipynb\") vamos a ver como clasificar una serie de Tweets en Ingles sobre críticas a los productos de Apple.\n",
    "\n",
    "\n",
    "* Estos tweets estan clasificados como: *positivos*, *neutros* o *negativos*\n",
    "\n",
    "\n",
    "* El este notebook vamos a realizar los sigueintes pasos (similar que en el notebook \"09_Sckit_Clasificacion_textos.ipynb\" pero usando diferentes librerías).\n",
    "    \n",
    "    1. Carga de los datos (tweets)\n",
    "    2. Normalización (en ingles) de los tweets con Spacy\n",
    "    3. Particionado de Datos con Scikit\n",
    "    4. Creacción de la Bolsa de Palabras con NLTK\n",
    "    5. Creacción de modelos con NLTK y Scikit\n",
    "        - Multinomial Naive Bayes\n",
    "        - Bernoulli Naive Bayes\n",
    "        - Regresion Logistica\n",
    "        - Support Vector Machine\n",
    "    6. Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 1. Carga de Datos\n",
    "\n",
    "\n",
    "* El primer paso que vamos a realizar es el de cargar los datos. Para ello ***leeremos el csv con pandas*** (pasandolo a un dataframe) y posteriormente lo transformaremos en una lista de tuplas (*tweets*) donde cada tupla esta formada por:\n",
    "    - **Posición 0**: Tweet\n",
    "    - **Posición 1**: Polaridad (Positivo | Neutro | Negativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Tweets Cargados: 3804\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tweets_file = './data/Apple_Tweets.csv'\n",
    "df = pd.read_csv(tweets_file, header=None)\n",
    "tweets = [tuple(x) for x in df.values]\n",
    "print('Número de Tweets Cargados: {num}'.format(num=len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 2. Normalización\n",
    "\n",
    "* Para la normalización en este ejemplo haremos uso de ***spaCy***, de la misma mamera que se ha realizado en el notebook \"09_Sckit_Clasificacion_textos.ipynb\".\n",
    "\n",
    "\n",
    "* Para ***normalizar*** los tweets realizaremos las siguientes acciones:\n",
    "    1. Pasamos las frases a minúsculas.\n",
    "    2. Eliminamos los signos de puntuación.\n",
    "    3. Eliminamos las palabras con menos de 3 caracteres.\n",
    "    4. Eliminamos las Stop-Words.\n",
    "    5. Eliminamos las palabras que empiecen por '@' o 'http'.\n",
    "    6. Pasamos la palabra a su lema\n",
    "\n",
    "\n",
    "* Todos estos pasos los vamos a realizar en una misma función.\n",
    "\n",
    "\n",
    "* ***NOTA***: *De cara a la normalización de textos se pueden realizar más acciones que las que vamos a realizar, pero con estas serán más que suficientes para realizar un ejercicio con fines didácticos*\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3804/3804 [00:27<00:00, 137.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def normalize(tweets):\n",
    "    \"\"\"normalizamos los tweets que nos vienen en una lista de tuplas, en la posición '0' de la tupla\"\"\"\n",
    "    for index, tweet in enumerate(tqdm(tweets)):\n",
    "        tokens = nlp(tweet[0].lower()) # Paso el tweet a minúsculas y a un objeto de la clase Doc de Spacy\n",
    "        tweet_nor = \" \".join([word.lemma_ for word in tokens if (not word.is_punct)\n",
    "                              and (len(word.text) > 2) and (not word.is_stop) \n",
    "                              and (not word.text.startswith('@')) and (not word.text.startswith('http'))])\n",
    "        tweets[index] = (tweet_nor, tweet[1])\n",
    "    return tweets\n",
    "\n",
    "# Normalizamos los tweets\n",
    "X = normalize(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 3. Particionado de Datos (Train y Test)\n",
    "\n",
    "* Vamos a particionar los datos en conjunto de Train y Test.\n",
    "\n",
    "\n",
    "* Para este ejemplo nos vamos a quedar con:\n",
    "    - 80% de datos de entrenamiento\n",
    "    - 20% de datos de test\n",
    "    \n",
    "    \n",
    "* Pasos:\n",
    "    1. Pasamos el Target (positivo, negativo o neutro) a una lista.\n",
    "    2. Dividimos los datos en entrenamiento y test\n",
    "\n",
    "\n",
    "* ***NOTA***: Las estructuras de datos de entrenamiento y test son tuplas de dos posiciones. La primera posición contiene el tweet normalizado y la segunda posición el target. Esta estructura la dejamos de esta manera para construir la estructura de datos que necesita la clase \"classify\" de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Tweets para el entrenamiento: 3043\n",
      "Número de Tweets para el test: 761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Obtenemos el target\n",
    "y = [y[1] for y in X] \n",
    "\n",
    "# Dividimos los datos en entrenamiento y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print('Número de Tweets para el entrenamiento: {num}'.format(num=len(X_train)))\n",
    "print('Número de Tweets para el test: {num}'.format(num=len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 4. Bolsa de Palabras (Extracción de Características)\n",
    "\n",
    "\n",
    "* NLTK necesita una determina estructura de datos de entrada para generar el modelo. Para ello necesitamos crear:\n",
    "\n",
    "    - Un Diccionario (CUIDADO no es un diccionario python) con todas las palabras del corpus.\n",
    "    \n",
    "    - Una tupla que contenga:\n",
    "        1. Posición 1 (indice '0'): un Booleano por cada palabra del diccionario, para indicar si aparece o no la palabra de la frase. \n",
    "        2. Posición 2 (indice '1'): Etiqueta (target) de la frase. \n",
    "    \n",
    "\n",
    "* Para ello creamos las siguientes 2 funciones:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def get_unique_words(tweets):\n",
    "    \"\"\"Función que devuelve una lista con todas las palabras únicas que aparecen en las frases\"\"\"\n",
    "    all_words = []\n",
    "    for (tweet, sentiment) in tweets:\n",
    "        all_words.extend(nltk.word_tokenize(tweet))\n",
    "    return list(set(all_words))\n",
    "\n",
    "def extract_features(document):\n",
    "    \"\"\"Función que crea el conjunto de entrenamiento del clasificador\n",
    "       1: Toma todos los documentos del corpus\n",
    "       2: Toma todas las palabras del corpus\n",
    "       3: Escribre (True|False) si aparece cada palabra del corpus en la frase\n",
    "    \"\"\"\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in unique_words:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El siguiente paso es:\n",
    "    1. Crear el diccionario del corpus con las palabras únicas\n",
    "    2. Pasar los datos de entrenamiento y test a la estructura que necesita NLTK:\n",
    "        * Diccionario python con un Booleano por cada palabra del diccionario de palabras del corpus, para indicar si aparece o no la palabra de la frase. \n",
    "        2. Etiqueta (target) de la frase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el diccionario de palabras\n",
    "unique_words = get_unique_words(X)\n",
    "\n",
    "# Creamos las estructaras de datos para NLTK:\n",
    "X_train_nltk = nltk.classify.apply_features(extract_features, X_train)\n",
    "X_test_nltk = nltk.classify.apply_features(extract_features, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 5. Creacción del Modelo\n",
    "\n",
    "\n",
    "* Una vez tenemos creada la bolsa de palabras, podemos usar cualquier algoritmo de aprendizaje para la clasificación.\n",
    "\n",
    "\n",
    "* Para este ejemplo vamos a usar los siguientes algoritmos:\n",
    "\n",
    "    - Naive Bayes (NLTK): https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py\n",
    "    - Multinomial Naive Bayes: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "    - Bernoulli Naive Bayes: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html\n",
    "    - Regresion Logistica: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "    - Support Vector Machine Classifier: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando Naive Bayes\n",
      "Entrenando Multinomial Naive Bayes\n",
      "Entrenando Bernoulli Naive Bayes\n",
      "Entrenando Regresion Logistica\n",
      "Entrenando Support Vector Machine\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Clasificadores\n",
    "print('Entrenando Naive Bayes')\n",
    "nb = nltk.classify.NaiveBayesClassifier.train(X_train_nltk)\n",
    "\n",
    "print('Entrenando Multinomial Naive Bayes')\n",
    "mnb = nltk.classify.SklearnClassifier(MultinomialNB()).train(X_train_nltk)\n",
    "\n",
    "print('Entrenando Bernoulli Naive Bayes')\n",
    "bnb = nltk.classify.SklearnClassifier(BernoulliNB()).train(X_train_nltk)\n",
    "\n",
    "print('Entrenando Regresion Logistica')\n",
    "lr = nltk.classify.SklearnClassifier(LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)).train(X_train_nltk)\n",
    "\n",
    "print('Entrenando Support Vector Machine')\n",
    "svm = nltk.classify.SklearnClassifier(SVC(kernel='linear')).train(X_train_nltk)\n",
    "\n",
    "clasificadores = {'Naive Bayes' : nb,\n",
    "                  'Multinomial Naive Bayes': mnb,\n",
    "                  'Bernoulli Naive Bayes': bnb,\n",
    "                  'Regresion Logistica': lr,\n",
    "                  'Support Vector Machine': svm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 6. Evaluación del Modelo\n",
    "\n",
    "\n",
    "* Para cada uno de los modelos vamos a calcular las siguientes métricas de evaluación para los datos de test:\n",
    "\n",
    "    1. **Accuracy**\n",
    "    2. **Precision**\n",
    "    3. **Recall**\n",
    "    4. **F1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUANDO MODELO: Naive Bayes\n",
      "EVALUANDO MODELO: Multinomial Naive Bayes\n",
      "EVALUANDO MODELO: Bernoulli Naive Bayes\n",
      "EVALUANDO MODELO: Regresion Logistica\n",
      "EVALUANDO MODELO: Support Vector Machine\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.593955</td>\n",
       "      <td>0.524648</td>\n",
       "      <td>0.593955</td>\n",
       "      <td>0.544561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "      <td>0.578187</td>\n",
       "      <td>0.334300</td>\n",
       "      <td>0.578187</td>\n",
       "      <td>0.423650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.584757</td>\n",
       "      <td>0.516856</td>\n",
       "      <td>0.584757</td>\n",
       "      <td>0.455602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regresion Logistica</th>\n",
       "      <td>0.583443</td>\n",
       "      <td>0.501226</td>\n",
       "      <td>0.583443</td>\n",
       "      <td>0.490483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.586071</td>\n",
       "      <td>0.506737</td>\n",
       "      <td>0.586071</td>\n",
       "      <td>0.511264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         accuracy  precision    recall        f1\n",
       "name                                                            \n",
       "Naive Bayes              0.593955   0.524648  0.593955  0.544561\n",
       "Multinomial Naive Bayes  0.578187   0.334300  0.578187  0.423650\n",
       "Bernoulli Naive Bayes    0.584757   0.516856  0.584757  0.455602\n",
       "Regresion Logistica      0.583443   0.501226  0.583443  0.490483\n",
       "Support Vector Machine   0.586071   0.506737  0.586071  0.511264"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "evaluacion = list()\n",
    "for k, v in clasificadores.items():\n",
    "    print ('EVALUANDO MODELO: {model}'.format(model=k))\n",
    "    \n",
    "    # Obtengo las predicciones\n",
    "    y_predictions = [v.classify(tweet[0]) for tweet in X_test_nltk]\n",
    "    \n",
    "    model = {}\n",
    "    model['name'] = k\n",
    "    model['accuracy'] = accuracy_score(y_true=y_test, y_pred=y_predictions)\n",
    "    model['precision'] = precision_score(y_true=y_test, y_pred=y_predictions, average='weighted')\n",
    "    model['recall'] = recall_score(y_true=y_test, y_pred=y_predictions, average='weighted')\n",
    "    model['f1'] = f1_score(y_true=y_test, y_pred=y_predictions, average='weighted')\n",
    "    evaluacion.append(model)\n",
    "\n",
    "# Pasamos los resultados a un DataFrame para visualizarlos mejor\n",
    "df = pd.DataFrame.from_dict(evaluacion)\n",
    "df.set_index(\"name\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "# Conclusiones\n",
    "\n",
    "* A nivel de Algoritmos de Aprendizaje (Algoritmos de Clasificación) hemos realizado los mismo que en el notebook \"09_Sckit_Clasificacion_textos.ipynb\" y como se puede apreciar los resultados obtenidos en este notebook son considerablemente peores.\n",
    "\n",
    "\n",
    "* Esto es debido a como construimos las ***bolsa de palabras***:\n",
    "\n",
    "    - **\"09_Sckit_Clasificacion_textos.ipynb\"**: Seleccionamos solo las 'N' (N=1000) con más frecuencia del corpus y además filtramos aquellas palabras que no aparecen en un mínimo de 'M' (M=3) tweets.\n",
    "    - **\"11_NLTK_Sckit_Clasificacion_textos.ipynb\"** (este notebook): En este caso construimos la bolsa de palabras sin ningún tipo de restricciones, metiendo todas las palabras en la Bolsa de Palabras.\n",
    "    \n",
    "    \n",
    "* Por otro lado la normalización de datos realizada (tanto en un notebook como en otro) es una ***normalización de datos muy simple y trivial***. No se ha pasado a analizar con un mínimo detalle la naturaleza de los tweets (solo se han eliminado menciones '@' y enlaces 'http') y seguro que hay muchas palabras que distorsionan la clasificación.\n",
    "\n",
    "\n",
    "* Realizando una normalización muchos más extricta se hubiesen obtenido resultados mejores, pero esto no se ha realizado ya que la finalidad didáctica de estos notebooks (\"*09_Sckit_Clasificacion_textos.ipynb*\", \"*11_NLTK_Sckit_Clasificacion_textos.ipynb*\") es ver el proceso de clasificación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
