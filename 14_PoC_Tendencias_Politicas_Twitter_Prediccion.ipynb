{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - Prueba de Concepto: Tendencias Políticas en Twitter - Explotación -\n",
    "\n",
    "\n",
    "* Este notebook es la continuación del notebook: *13_PoC_Tendencias_Politicas_Twitter_Generacion_Exportacion_Modelos.ipynb*\n",
    "\n",
    "\n",
    "* En este notebook vamos a ***explotar*** un modelo ya creado anteriormente y va a tener como objetivo ***clasificar los tweets de una determinada cuenta de twitter*** en función de su tendencia política.\n",
    "\n",
    "\n",
    "* Para realizar todo esto, lo haremos de la siguiente manera:\n",
    "\n",
    "    1. Lectura (via API) de los tweets de una determinada cuenta de twitter\n",
    "    2. Normalización de los tweets\n",
    "    3. Importación de los modelos (Clasificación y BoW)\n",
    "    3. Creacción de la Bolsa de Palabras (BoW) de los nuevos tweets\n",
    "    4. Predicción\n",
    "    \n",
    "    \n",
    "<hr>\n",
    "\n",
    "\n",
    "## Lectura (via API) de tweets\n",
    "\n",
    "* Para leer los tweets de una cuenta de twitter podemos usar el API de Twitter directamente o utilizar la librería ***tweepy*** que nos facilitamo mucho la labor a la hora de obtener datos de Twitter.\n",
    "\n",
    "\n",
    "* No tenemos como objetivo en este notebook explicar como funciona esta librería. Para saber de su funcionamiento podeis ver su página web: https://www.tweepy.org/\n",
    "\n",
    "\n",
    "* En esta punto vamos a leer 'N' tweets (si Twitter nos los facilita) de una determinada cuenta de Twitter.\n",
    "\n",
    "\n",
    "* En primer lugar tenemos que autenticarnos en Twitter con el protocolo OAuth (https://es.wikipedia.org/wiki/OAuth) y para ello necesitamos unos keys y unos tokens que nos proporcionará Twitter al registrar una APP. Este proceso de registro de una App es un poco tediodo y tampoco es el objetivo en esta PoC el explicar ese proceso. Para más información visitar la web de desarrolladores de Twitter (https://developer.twitter.com/).\n",
    "\n",
    "\n",
    "* Nos autenticamos con Twitter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# Claves de cliente y tokens de acceso\n",
    "consumer_key = 'mwQE4IaYG0TwQSJUqmWCvwnVw'\n",
    "consumer_secret = 'vLvzs0Dd0UfRIxm2t5PKG7xYipYl9VCG4PipVmyPOzitJVTfzX'\n",
    "access_token = '488598384-w2jkOq0CVA0NDYTd86V7uDtk5rp5HQqOEkmXaQz2'\n",
    "access_token_secret = 'CblkBkfayCQIGIj3IRM16kyh9ect0ai2XqPu4KM4fWrOH'\n",
    "\n",
    "# Proceso de autenticación OAuth\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vamos a petir a Twitter que nos devuelva de una determinada cuenta (*account*) un determinado número de tweets (*num_tweets*)\n",
    "\n",
    "\n",
    "* Dado que se ha registrado una aplicación de Twitter gratuita, es posible que Twitter no nos devuelva de una determinada cuenta todos los tweets que le pedimos.\n",
    "\n",
    "\n",
    "* A modo de poder probar esta prueba de concepto, dejo algunas cuentas de Twitter (de marcado caracter político) para predecir cual es su tendencia política. Esta cuentas no han sido utilizadas para generar el dataset con el que hemos generado el modelo.\n",
    "\n",
    "    - Iñigo Errejon: @ierrejon\n",
    "    - Manuela Carmena: @ManuelaCarmena\n",
    "    - Susana Díaz: @susanadiaz\n",
    "    - Josep Borrell: @JosepBorrellF\n",
    "    - Manuel Valls: @manuelvalls\n",
    "    - Alberto Núñez Feijó: @FeijooGalicia\n",
    "    - Mariano Rajoy: @marianorajoy\n",
    "    - Jordi Evole: @jordievole\n",
    "    - Eduardo Inda: @eduardoinda\n",
    "    - Jorge Verstrynge: @VerstryngeJorge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "account = '@ierrejon'\n",
    "num_tweets = 50\n",
    "tweets = list()\n",
    "for user_status in api.user_timeline(screen_name=account, count = num_tweets, include_rts=False, tweet_mode=\"extended\"):\n",
    "        tweets.append(user_status.full_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Normalización\n",
    "\n",
    "\n",
    "* Tenemos que normalizar de la misma manera que se han normlizado los tweets con los que se generaron el modelo.\n",
    "\n",
    "\n",
    "* Realizamos las misma acciones para ***normalizar*** los tweets:\n",
    "    1. Pasamos las frases a minúsculas.\n",
    "    2. Sustituimos los puntos por espacios ya que hay muchas palabras unidas por un punto\n",
    "    3. Quitamos la almuhadilla de los hashtags para considerarlos como palabras.\n",
    "    4. Eliminamos los signos de puntuación.\n",
    "    5. Eliminamos las palabras con menos de 3 caracteres.\n",
    "    6. Eliminamos las Stop-Words.\n",
    "    7. Eliminamos los enlaces(http) y las menciones (@)\n",
    "    8. Pasamos la palabra a su lema\n",
    "    \n",
    "\n",
    "* Por último vamos a eliminar los tweets que tras la normalización no contengan ninguna palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def normalize(tweets):\n",
    "    \"\"\"normalizamos la lista de tweets\"\"\"\n",
    "    for index, tweet in enumerate(tweets):\n",
    "        # Tokenizamos el tweets realizando los puntos 1,2 y 3.\n",
    "        tweet = nlp(tweet.lower().replace('.', ' ').replace('#', ' '))\n",
    "        # Puntos 4,5,6,7 y 8\n",
    "        tweets[index] = \" \".join([word.lemma_ for word in tweet if (not word.is_punct)\n",
    "                                  and (len(word.text) > 2) and (not word.is_stop) \n",
    "                                  and (not word.text.startswith('@'))\n",
    "                                  and (not word.text.startswith('http'))\n",
    "                                  and (not ':' in word.text)])\n",
    "    # Devolvemos los tweets que tras la normalización tenga palabras\n",
    "    return [tweet for tweet in tweets if len(tweet)>0]\n",
    "\n",
    "# Normalizamos los tweets\n",
    "X_norm = normalize(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Importamos los modelos\n",
    "\n",
    "* Vamos a importar los modelos creados en el notebook *13_PoC_Tendencias_Politicas_Twitter_Generacion_Exportacion_Modelos.ipynb*\n",
    "\n",
    "\n",
    "* Temos que importar:\n",
    "    1. El modelo creado con el Algoritmo de Aprendizaje ***Bernoulli Naive Bayes***\n",
    "    2. El modelo para crear la Bolsa de Palabras\n",
    "    \n",
    "    \n",
    "#### 1. Importamos el modelo para la clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = './models/bernoullinb_tweets_politica.pickle'\n",
    "classifier_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Importamos el modelo para crear la Bolsa de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './models/vectorizer_bow_tweets_politica.pickle'\n",
    "vectorizer = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Creamos la Bolsa de Palabras\n",
    "\n",
    "\n",
    "* Tenemos que usar el modelo de bolsa de palabras creado con los tweets de entrenamiento ya que contiene el diccionario (o vocabulario) con el que se ha entrenado el modelo para la clasificación.\n",
    "\n",
    "\n",
    "* Este diccionario contiene 1000 palabras y los tweets que tenemos que predecir hay que transformalos a un vector de frecuencias donde nos diga cuantas veces aparecen las palabras del tweets dentro de esas 1000 palabras con las que hemos entrenado.\n",
    "\n",
    "\n",
    "* En el caso de que un tweet contenga una palabra que no esté entre esas 1000 palabras, esta palabra no se tendrá en cuenta para predecir la tendencia política del tweet, ya que el modelo no ha sido entrenado con esa palabra.\n",
    "\n",
    "\n",
    "* Vamos a pasar ***Bolsa de Palabras de frecuencias*** los tweets leidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = vectorizer.transform(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Predicción\n",
    "\n",
    "\n",
    "* Con el modelo importado y la bolsa de palabras creada, vamos a clasificar cada unos de los tweets en su tendencia política.\n",
    "\n",
    "\n",
    "* Para ello vamos a llamar al método \"***predict()***\" y le vamos a pasar la ***Bolsa de palabras de los Tweets*** para que nos clasifique ese tweet.\n",
    "\n",
    "\n",
    "* Como lo que nos interesa es ***clasificar la cuenta de Twitter*** (o persona que esta detras de esa cuenta) en su ***tendencia política***, vamos a calcular los porcentajes de clasificación de los tweets en su tendencia política:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "podemos = 72.7%\n",
      "psoe = 15.2%\n",
      "pp = 9.1%\n",
      "ciudadanos = 3.0%\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier_model.predict(X_bow)\n",
    "prediccion = dict()\n",
    "total = 0\n",
    "for pred in predictions:\n",
    "    if pred in prediccion:\n",
    "        prediccion[pred] += 1\n",
    "    else:\n",
    "        prediccion[pred] = 1\n",
    "    total += 1\n",
    "        \n",
    "for k,v in prediccion.items():\n",
    "    print('{partido} = {porc:0.1f}%'.format(partido=k, porc=(v/float(total))*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
