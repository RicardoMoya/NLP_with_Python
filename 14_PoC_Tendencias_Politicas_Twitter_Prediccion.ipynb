{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - Prueba de Concepto: Tendencias Pol√≠ticas en Twitter - Explotaci√≥n -\n",
    "\n",
    "\n",
    "* Este notebook es la continuaci√≥n del notebook: *13_PoC_Tendencias_Politicas_Twitter_Generacion_Exportacion_Modelos.ipynb*\n",
    "\n",
    "\n",
    "* En este notebook vamos a ***explotar*** un modelo ya creado anteriormente y va a tener como objetivo ***clasificar los tweets de una determinada cuenta de twitter*** en funci√≥n de su tendencia pol√≠tica.\n",
    "\n",
    "\n",
    "* Para realizar todo esto, lo haremos de la siguiente manera:\n",
    "\n",
    "    1. Lectura (via API) de los tweets de una determinada cuenta de twitter\n",
    "    2. Normalizaci√≥n de los tweets\n",
    "    3. Importaci√≥n de los modelos (Clasificaci√≥n y BoW)\n",
    "    3. Creacci√≥n de la Bolsa de Palabras (BoW) de los nuevos tweets\n",
    "    4. Predicci√≥n\n",
    "    \n",
    "    \n",
    "<hr>\n",
    "\n",
    "\n",
    "## Lectura (via API) de tweets\n",
    "\n",
    "* Para leer los tweets de una cuenta de twitter podemos usar el API de Twitter directamente o utilizar la librer√≠a ***tweepy*** que nos facilitamo mucho la labor a la hora de obtener datos de Twitter.\n",
    "\n",
    "\n",
    "* No tenemos como objetivo en este notebook explicar como funciona esta librer√≠a. Para saber de su funcionamiento podeis ver su p√°gina web: https://www.tweepy.org/\n",
    "\n",
    "\n",
    "* En esta punto vamos a leer 'N' tweets (si Twitter nos los facilita) de una determinada cuenta de Twitter.\n",
    "\n",
    "\n",
    "* En primer lugar tenemos que autenticarnos en Twitter con el protocolo OAuth (https://es.wikipedia.org/wiki/OAuth) y para ello necesitamos unos keys y unos tokens que nos proporcionar√° Twitter al registrar una APP. Este proceso de registro de una App es un poco tediodo y tampoco es el objetivo en esta PoC el explicar ese proceso. Para m√°s informaci√≥n visitar la web de desarrolladores de Twitter (https://developer.twitter.com/).\n",
    "\n",
    "\n",
    "* Nos autenticamos con Twitter:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "# Claves de cliente y tokens de acceso\n",
    "CONSUMER_KEY = 'fcCFQpt3lhMzeCgGhznWsb8C5'\n",
    "CONSUMER_SECRET = 'lljbTkudnEvn0SWn6ZPw5Svam6TzD9q58AhBsgrsqGN9AdqrvF'\n",
    "ACCESS_TOKEN = '1021745678974902279-7VhCtZkPsqhcaHPq4NFalaShazGbTV'\n",
    "ACCESS_TOKEN_SECRET = '0xDVjwimkp1WKjjZpMpLuj0V3aRkAeKuvc4DiHkyfvYkZ'\n",
    "\n",
    "# Proceso de autenticaci√≥n OAuth\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vamos a petir a Twitter que nos devuelva de una determinada cuenta (*account*) un determinado n√∫mero de tweets (*num_tweets*)\n",
    "\n",
    "\n",
    "* Dado que se ha registrado una aplicaci√≥n de Twitter gratuita, es posible que Twitter no nos devuelva de una determinada cuenta todos los tweets que le pedimos.\n",
    "\n",
    "\n",
    "* A modo de poder probar esta prueba de concepto, dejo algunas cuentas de Twitter (de marcado caracter pol√≠tico) para predecir cual es su tendencia pol√≠tica. Esta cuentas no han sido utilizadas para generar el dataset con el que hemos generado el modelo.\n",
    "\n",
    "    - I√±igo Errejon: @ierrejon\n",
    "    - Manuela Carmena: @ManuelaCarmena\n",
    "    - Susana D√≠az: @susanadiaz\n",
    "    - Josep Borrell: @JosepBorrellF\n",
    "    - Manuel Valls: @manuelvalls\n",
    "    - Alberto N√∫√±ez Feij√≥: @FeijooGalicia\n",
    "    - Mariano Rajoy: @marianorajoy\n",
    "    - Jordi Evole: @jordievole\n",
    "    - Eduardo Inda: @eduardoinda\n",
    "    - Jorge Verstrynge: @VerstryngeJorge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 - Se ha muerto Almudena Grandes. Narradora de nuestras derrotas y de la dignidad. Del orgullo de ser de los nuestros. Que la tierra te sea leve, compa√±era ‚úäüèº https://t.co/CBO17ALxr7\n",
      "\n",
      "1 - Este fin de semana es el plenario de @MasMadrid__. Distrito a distrito y barrio a barrio, la primera fuerza de la ciudad, con los pies en el territorio y los ojos en el futuro.\n",
      "\n",
      "Ser√° un orgullo tomar la palabra en el acto de clausura. All√≠ nos vemos! ‚úåüèº https://t.co/LXyGWucJ15\n",
      "\n",
      "2 - @izenikgabea Jejeje pues est√°n en ello y se subir√° pronto. Besarkada bat!\n",
      "\n",
      "3 - Espa√±a tiene un problema de desigualdad. No se corrige con mordazas sino con justicia social. Hay que derogar la ley mordaza. https://t.co/VqEo3scUfL\n",
      "\n",
      "4 - Hasta que todo el mundo est√© vacunado podr√°n seguir apareciendo nuevas variantes. Para acabar con el Covid liberemos las patentes. https://t.co/FQStOnSOtF\n",
      "\n",
      "5 - @jonbaldw Si.\n",
      "\n",
      "6 - Salimos ahora de casi tres horas de un lujo de charla, con la libreta llena de notas y la cabeza rebosando ideas sugerentes. Gracias Amador y a todos los que hab√©is venido y hab√©is participado de la conversaci√≥n üòä https://t.co/MuGgwHTrFA\n",
      "\n",
      "7 - @Andreu_fb @HOI_Game @Arheo_ @podcat_paradox @Traxium00 @Javi103Percebe @joanhornos @JaviBooma @rsegural @OceloTStrategy @AdriGlezMontes @CapitnWillMoran Jejejejeje com saps, cabr√≥ üòâ\n",
      "\n",
      "8 - @Javi103Percebe @Andreu_fb @HOI_Game @Arheo_ @podcat_paradox @Traxium00 @joanhornos @JaviBooma @rsegural @OceloTStrategy @AdriGlezMontes @CapitnWillMoran Solo para ordenador, no?\n",
      "\n",
      "9 - @Andreu_fb @HOI_Game @Arheo_ @podcat_paradox @Traxium00 @Javi103Percebe @joanhornos @JaviBooma @rsegural @OceloTStrategy @AdriGlezMontes @CapitnWillMoran Que √©s aquesta meravella?\n",
      "\n",
      "10 - Tomando notas para la conversaci√≥n de esta tarde. Cada uno de los dos libros es, a su manera, un balance de √©poca y una propuesta para salir del ciclo defensivo. Vamos a poner a los dos a discutir a ver qu√© sale! All√≠ nos vemos ! https://t.co/JXzt0wYCDb\n",
      "\n",
      "11 - Pues aqu√≠ tenemos otro ejemplo de algoritmos que sesgan contra minor√≠as o contra los m√°s vulnerables. Por eso hay que auditarlos y corregir.\n",
      "\n",
      "https://t.co/JYyL7p1x5R\n",
      "\n",
      "12 - Tienes 21 a√±os, est√°s en el paro y te bombardea publicidad sobre criptomonedas.\n",
      "\n",
      "No es casualidad, son algoritmos. Regularlos tambi√©n es proteger a los m√°s vulnerables: https://t.co/vYtAtMMbYu\n",
      "\n",
      "13 - Lo que nos ense√±a C√°diz https://t.co/h41O5vRmeZ\n",
      "\n",
      "14 - Cuando te quiten derechos, C√°diz. Cuando no cumplan los convenios, C√°diz. Cuando te digan \"no queda otra\", C√°diz. Cuando te digan que luchar no vale de nada, C√°diz.\n",
      "\n",
      "15 - Es gracioso porque hablan en otras lenguas. https://t.co/xm1fCwBoDe\n",
      "\n",
      "16 - Hasta que ninguna mujer sufra violencia por ser mujer. Hoy y cada d√≠a: a defender y ampliar la Espa√±a feminista #25N\n",
      "\n",
      "17 - @htejero_ Exactamente eso. Ni los dolores. Se articulan o, al menos, si no pueden, se respetan.\n",
      "\n",
      "18 - E S T A D O   E M P R E N D E D O R üí™üèº https://t.co/ufxEsMxG67\n",
      "\n",
      "19 - Felicidades a @ABaerbock y a @Die_Gruenen. Acuerdo de 80% de renovables en 2030, voto a los 16... la Ola Verde llega al gobierno alem√°n y sigue extendi√©ndose por Europa ‚úåüèºüåª\n",
      "\n",
      "20 - ‚ÄºÔ∏è Cambio de hora: la conversaci√≥n con Amador Fern√°ndez-Savater finalmente ser√° a las 20h. https://t.co/TPJXwxLKPr https://t.co/VZdDo1qDMQ\n",
      "\n",
      "21 - Esta tanqueta sale m√°s borrosa en las fotos pero tambi√©n sirve para aplastar derechos. https://t.co/rFgKvsgqqK\n",
      "\n",
      "22 - üî¥ Cambio de hora: finalmente ser√° a las 20h. https://t.co/K4cUdWR8eg\n",
      "\n",
      "23 - @kolontai1959 üòä‚úäüèº\n",
      "\n",
      "24 - https://t.co/isqi6GeLF4\n",
      "\n",
      "25 - ¬øPero no se hund√≠a la econom√≠a ? https://t.co/N3TXYz84eT\n",
      "\n",
      "26 - Ser joven hoy en Espa√±a te convierte en grupo de riesgo: https://t.co/Mdyfjapd1s\n",
      "\n",
      "27 - C√°diz necesita trabajo, no tanquetas. Hoy al Gobierno: https://t.co/IN9k5XyRS5\n",
      "\n",
      "28 - Este viernes estar√© con Amador Fern√°ndez-Savater poniendo en com√∫n nuestros ultimos libros. Una conversaci√≥n distinta, desde tradiciones diferentes y que promete  üòâ https://t.co/6i872vSyI7\n",
      "\n",
      "29 - ¬øY si en lugar de tanquetas el Gobierno llevase a C√°diz un plan de reindustrializaci√≥n?\n",
      "\n",
      "30 - üòç‚úäüèº https://t.co/Pq0BizG1e2\n",
      "\n",
      "31 - Me pide un representante de la Coordinadora de trabajadores del metal de la Bah√≠a de C√°diz que transmita: que les cuesta mucho que los medios les pregunten y cuenten sus razones.Que el motivo principal de la huelga no son ni IPC ni pluses, sino exigir que se cumplan los convenios\n",
      "\n",
      "32 - Allende y Pinochet. El que dio el Golpe y el que lo sufrio. El que muri√≥ y el que lo asesin√≥. La equidistancia https://t.co/9s0RxqRNYp\n",
      "\n",
      "33 - @fentvia Moltes gr√†cies!!\n"
     ]
    }
   ],
   "source": [
    "account = '@ierrejon'\n",
    "num_tweets = 50\n",
    "tweets = list()\n",
    "for user_status in api.user_timeline(screen_name=account, count = num_tweets, include_rts=False, tweet_mode=\"extended\"):\n",
    "        tweets.append(user_status.full_text)\n",
    "\n",
    "for index, tweet in enumerate(tweets):\n",
    "    print('\\n{} - {}'.format(index, tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Normalizaci√≥n\n",
    "\n",
    "\n",
    "* Tenemos que normalizar de la misma manera que se han normlizado los tweets con los que se generaron el modelo.\n",
    "\n",
    "\n",
    "* Realizamos las misma acciones para ***normalizar*** los tweets:\n",
    "    1. Pasamos las frases a min√∫sculas.\n",
    "    2. Sustituimos los puntos por espacios ya que hay muchas palabras unidas por un punto\n",
    "    3. Quitamos la almuhadilla de los hashtags para considerarlos como palabras.\n",
    "    4. Eliminamos los signos de puntuaci√≥n.\n",
    "    5. Eliminamos las palabras con menos de 3 caracteres.\n",
    "    6. Eliminamos las Stop-Words.\n",
    "    7. Eliminamos los enlaces(http) y las menciones (@)\n",
    "    8. Pasamos la palabra a su lema\n",
    "    \n",
    "\n",
    "* Por √∫ltimo vamos a eliminar los tweets que tras la normalizaci√≥n no contengan ninguna palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:00<00:00, 167.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N¬∫ de Tweets validos de @ierrejon: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def normalize(tweets, min_words=3):\n",
    "    \"\"\"\n",
    "    Funci√≥n que dada una lista de tweets ([tweet, clase]), normaliza los tweets y devuelve una lista\n",
    "    con los tweets normalizados, descartando aquellos tweets que tras la normalizaci√≥n tengan menos de\n",
    "    \"min_words\" palabras en el tweet.\n",
    "    \n",
    "    :param tweets:       Lista de Tweets ([tweet, clase]) con el tweet y la clase a la que pertenece\n",
    "    :param min_words:    N√∫mero minimo de palabras que tiene que tener un tweet tras la normalizaci√≥n\n",
    "    :return:             Lista de Tweets ([tweet, clase]) normalizados\n",
    "    \"\"\"\n",
    "    tweets_list = []\n",
    "    for tweet in tqdm(tweets):\n",
    "        # Tokenizamos el tweets realizando los puntos 1,2 y 3.\n",
    "        tw = nlp(tweet.lower().replace('.', ' ').replace('#', ' ').strip())\n",
    "        \n",
    "        # Normalizamos Puntos 4,5,6,7 y 8\n",
    "        tw = ([word.lemma_ for word in tw if (not word.is_punct)\n",
    "               and (len(word.text) > 2) and (not word.is_stop)\n",
    "               and (not word.text.startswith('@'))\n",
    "               and (not word.text.startswith('http'))\n",
    "               and (not ':' in word.text)])\n",
    "        \n",
    "        # Eliminamos los tweets que tras la normalizaci√≥n tengan menos de \"min_words\" palabras\n",
    "        if len(tw) >= min_words:\n",
    "            tweets_list.append(\" \".join(tw))\n",
    "    return tweets_list\n",
    "\n",
    "# Normalizamos las frases\n",
    "X_norm = normalize(tweets, min_words=5)\n",
    "\n",
    "print('N¬∫ de Tweets validos de {}: {}'.format(account, len(X_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Importamos los modelos\n",
    "\n",
    "* Vamos a importar los modelos creados en el notebook *13_PoC_Tendencias_Politicas_Twitter_Generacion_Exportacion_Modelos.ipynb*\n",
    "\n",
    "\n",
    "* Temos que importar:\n",
    "    1. El modelo creado con el Algoritmo de Aprendizaje ***Bernoulli Naive Bayes***\n",
    "    2. El modelo para crear la Bolsa de Palabras\n",
    "    \n",
    "    \n",
    "#### 1. Importamos el modelo para la clasificaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = './models/best_model_tweets_politica.pickle'\n",
    "classifier_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Importamos el modelo para crear la Bolsa de Palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './models/vectorizer_bow_tweets_politica.pickle'\n",
    "vectorizer = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Creamos la Bolsa de Palabras\n",
    "\n",
    "\n",
    "* Tenemos que usar el modelo de bolsa de palabras creado con los tweets de entrenamiento ya que contiene el diccionario (o vocabulario) con el que se ha entrenado el modelo para la clasificaci√≥n.\n",
    "\n",
    "\n",
    "* Este diccionario contiene 1000 palabras y los tweets que tenemos que predecir hay que transformalos a un vector de frecuencias donde nos diga cuantas veces aparecen las palabras del tweets dentro de esas 1000 palabras con las que hemos entrenado.\n",
    "\n",
    "\n",
    "* En el caso de que un tweet contenga una palabra que no est√© entre esas 1000 palabras, esta palabra no se tendr√° en cuenta para predecir la tendencia pol√≠tica del tweet, ya que el modelo no ha sido entrenado con esa palabra.\n",
    "\n",
    "\n",
    "* Vamos a pasar ***Bolsa de Palabras de frecuencias*** los tweets leidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow = vectorizer.transform(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Predicci√≥n\n",
    "\n",
    "\n",
    "* Con el modelo importado y la bolsa de palabras creada, vamos a clasificar cada unos de los tweets en su tendencia pol√≠tica.\n",
    "\n",
    "\n",
    "* Para ello vamos a llamar al m√©todo \"***predict()***\" y le vamos a pasar la ***Bolsa de palabras de los Tweets*** para que nos clasifique ese tweet.\n",
    "\n",
    "\n",
    "* Como lo que nos interesa es ***clasificar la cuenta de Twitter*** (o persona que esta detras de esa cuenta) en su ***tendencia pol√≠tica***, vamos a calcular los porcentajes de clasificaci√≥n de los tweets en su tendencia pol√≠tica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "podemos = 50.0%\n",
      "vox = 30.0%\n",
      "psoe = 5.0%\n",
      "pp = 10.0%\n",
      "ciudadanos = 5.0%\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier_model.predict(X_bow)\n",
    "prediccion = dict()\n",
    "total = 0\n",
    "for pred in predictions:\n",
    "    if pred in prediccion:\n",
    "        prediccion[pred] += 1\n",
    "    else:\n",
    "        prediccion[pred] = 1\n",
    "    total += 1\n",
    "        \n",
    "for k,v in prediccion.items():\n",
    "    print('{partido} = {porc:0.1f}%'.format(partido=k, porc=(v/float(total))*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
