{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Conceptos para el Procesamiento del Lenguaje Natural (NLP)\n",
    "\n",
    "\n",
    "* En este Notebook vamos a enumerar y definir algunos de los conceptos más importantes que se dan en el Procesamiento del Lenguaje Natural y a ver algunos ejemplos en código.\n",
    "\n",
    "\n",
    "* Mostremos a continuación la definición de estos conceptos:\n",
    "\n",
    "\n",
    "### 1.- Corpus:\n",
    "\n",
    "* Un ***Corpus*** (en Latín \"cuerpo\") en el NLP se refiere a una colección de textos como pueda ser un conjunto de artículos periodísticos, libros, críticas, tweets, etc.\n",
    "\n",
    "### 2.- Bag of Words (BoW):\n",
    "\n",
    "* ***BoW*** (Bolsa de palabras) es un modelo que se utiliza para simplificar el contenido de un documento (o conjunto de documentos) omitiendo la gramática y el orden de las palabras, centrándose solo en el número de ocurrencias de palabras dentro del texto.\n",
    "\n",
    "### 3.- Normalización:\n",
    "\n",
    "* La ***normalización*** es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones:\n",
    "\n",
    "    - Convertir todo el texto en mayúscula o minúsculas\n",
    "    - Eliminar, puntos, comas, comillas, etc.\n",
    "    - Convertir los números a su equivalente a palabras\n",
    "    - Quitar palabras que no aportan significado al texto (Stop-words)\n",
    "    - Etc.\n",
    "\n",
    "### 4.- Tokenización:\n",
    "\n",
    "* Es una tarea que divide las cadenas de texto del documento en piezas más pequeñas o tokens. En la fase de tokenización los documentos se dividen en oraciones y estas se \"tokenizan\" en palabras. Aunque la tokenización es el proceso de dividir grandes cadenas de texto en cadenas más pequeñas, se suele diferenciar la:\n",
    "<span></span><br><br>\n",
    "    - ***Segmentación***: Tarea de dividir grandes cadenas de texto en piezas más pequeñas como oraciones o párrafos.\n",
    "<span></span><br><br>\n",
    "    - ***Tokenización***: Tarea de dividir grandes cadenas de texto solo y exclusivamente en palabras.\n",
    "\n",
    "\n",
    "### 5.- Stemming:\n",
    "\n",
    "* ***Stemming*** es el proceso de eliminar los afijos (sufijos, prefijos, infijos, circunflejos) de una palabra para obtener un tallo de palabra.\n",
    "<span></span><br><br>\n",
    "     + *Ejemplo*: Conduciendo -> conducir\n",
    "\n",
    "\n",
    "### 6.- Lematización:\n",
    "\n",
    "\n",
    "* La ***lematización*** es el proceso lingüístico que sustituye una palabra con forma flexionada (plurales, femeninos, verbos conjugados, etc.) por su lema; es decir, por una palabra válida en el idioma. \n",
    "\n",
    "\n",
    "* Si lo queremos definir de otra manera es sustituir una palabra con forma flexionada por la palabra que encontraríamos en el diccionario. \n",
    "<span></span><br><br>\n",
    "    + *Ejemplo*: Coches -> Coche; Guapas -> Guapo\n",
    "\n",
    "\n",
    "### 7.- Stop Words:\n",
    "\n",
    "\n",
    "* Son palabras que no aportan nada al significado de las frases como las preposiciones, determinantes, etc.\n",
    "\n",
    "\n",
    "### 8.- Parts-of-speech (POS) Tagging:\n",
    "\n",
    "\n",
    "* Consiste en asignar una etiqueta de categoría a las partes tokenizadas de una oración. El etiquetado POS más popular sería identificar palabras como sustantivos, verbos, adjetivos, etc.\n",
    "\n",
    "\n",
    "* En la lengua castellana nos podemos encontrar 9 categorías de palabras:\n",
    "\n",
    "    - Artículo o determinante \n",
    "    - Sustantivo o nombre \n",
    "    - Pronombre \n",
    "    - Verbo \n",
    "    - Adjetivo \n",
    "    - Adverbio \n",
    "    - Preposición \n",
    "    - Conjunción \n",
    "    - Interjección\n",
    "\n",
    "\n",
    "### 9.- n-grammas:\n",
    "\n",
    "\n",
    "* Los ***n-gramas*** son otro modelo de representación para simplificar los contenidos de selección de texto. \n",
    "\n",
    "\n",
    "* A diferencia de la representación sin orden de una bolsa de palabras (bag of words), el modelado de n-gramas está interesado en preservar secuencias contiguas de N elementos de la selección de texto.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# Ejemplos con NLTK\n",
    "\n",
    "*NOTA: Los conceptos de \"Corpus\" (1), \"Bag of Words\" (2) y \"Normalización\" (3) son unos conceptos más amplios que explicar que el resto y por tanto se explicaran en otros notebooks de manera específica.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Tokenización\n",
    "\n",
    "* Divide las cadenas de texto del documento en piezas más pequeñas o tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Un', 'radar', 'multa', 'a', 'Mariano', 'Rajoy', 'por', 'caminar', 'demasiado', 'rapido']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
    "words = nltk.word_tokenize(doc)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.- Stemming\n",
    "\n",
    "* Proceso de eliminar los afijos\n",
    "\n",
    "\n",
    "* Para realizar el Stemming con NLTK tenemos que seleccionar el \"Stemmer\" adecuado dependiendo del idioma.\n",
    "\n",
    "\n",
    "* En NLTK existen dos \"Stemmers\" que son los siguientes:\n",
    "    * PorterStemmer\n",
    "    * SnowballStemmer\n",
    "\n",
    "\n",
    "* Para más información sobre estos ver el siguiente enlace: http://www.nltk.org/howto/stem.html\n",
    "<span></span><br><br>\n",
    "     + *Ejemplo en Inglés* con el *PorterStemmer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "minimum\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  \n",
    "stm = PorterStemmer()\n",
    "print (stm.stem('running'))\n",
    "print (stm.stem('minimum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los Stemmers de NLTK para idiomas distintos al Ingles son relativamente malos ya que NLTK esta pensado para la lengua inglesa.\n",
    "    + *Ejemplo en Español* con el *SnowballStemmer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n",
      "minim\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stm = SnowballStemmer('spanish') # Hay que indicarle explicitamente el idioma\n",
    "print (stm.stem('corriendo'))\n",
    "print (stm.stem('mínimo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.- Lematización\n",
    "\n",
    "\n",
    "* Proceso lingüístico que sustituye una palabra con forma flexionada (plurales, femeninos, verbos conjugados, etc.) por su lema; es decir, por una palabra válida en el idioma.\n",
    "\n",
    "\n",
    "* La Lematización que hace NLTK solo es buena para la lengua inglesa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n",
      "perros\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "print (lemm.lemmatize('dogs'))\n",
    "print (lemm.lemmatize('perros'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.- Stop words\n",
    "\n",
    "\n",
    "* Son las palabras que no aportan nada al significado de la frase.\n",
    "\n",
    "\n",
    "* NLTK tiene para una serie de idiomas un listado de Stop Words.\n",
    "\n",
    "\n",
    "* Para el Español dispone de un listado de stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'una', 'habíais', 'sería', 'estaríamos', 'habríais', 'eso', 'fuesen', 'cual', 'habido', 'quien', 'durante', 'tus', 'son', 'sean', 'esos', 'tendrás', 'hayan', 'otra', 'ti', 'hubiste', 'sí', 'habíamos', 'qué', 'en', 'tenían', 'hubo', 'hubiesen', 'fuisteis', 'están', 'he', 'algunos', 'otros', 'habrán', 'tendré', 'tuvierais', 'su', 'tanto', 'tu', 'tuvieses', 'tendréis', 'pero', 'estos', 'estéis', 'hubimos', 'hayas', 'sentida', 'sintiendo', 'tuvimos', 'estada', 'por', 'estaban', 'desde', 'fueses', 'mis', 'estarías', 'hubiese', 'estarían', 'tuviera', 'nosotras', 'habrá', 'sentidos', 'habías', 'estas', 'seáis', 'hubieras', 'para', 'seremos', 'seas', 'serías', 'unos', 'tendrán', 'fuimos', 'hubierais', 'fueran', 'estábamos', 'quienes', 'nosotros', 'míos', 'esa', 'suyos', 'tenemos', 'estás', 'tú', 'les', 'estén', 'habida', 'sois', 'vuestros', 'suyas', 'con', 'habían', 'al', 'hubieron', 'estuvierais', 'mías', 'estarán', 'mío', 'estuvieras', 'habiendo', 'estoy', 'nada', 'nuestro', 'fuese', 'de', 'hasta', 'como', 'tuvieras', 'estáis', 'fuera', 'serás', 'hayáis', 'estar', 'estuvieseis', 'eran', 'habré', 'hubieseis', 'donde', 'es', 'fuerais', 'tuviéramos', 'estuviéramos', 'tenga', 'fuéramos', 'tenéis', 'soy', 'habría', 'sea', 'hubieran', 'teníais', 'esto', 'otro', 'ella', 'esta', 'teníamos', 'tenido', 'habríamos', 'estabais', 'tuyos', 'no', 'a', 'siente', 'habrían', 'habremos', 'estés', 'tenida', 'fueseis', 'tuviste', 'estuviésemos', 'tienes', 'lo', 'seréis', 'estuviesen', 'tengáis', 'tenidas', 'estuvieses', 'las', 'habidos', 'ante', 'yo', 'me', 'él', 'vosotros', 'y', 'ya', 'la', 'estaríais', 'tengas', 'cuando', 'tuvieron', 'tienen', 'estuve', 'serían', 'nuestras', 'entre', 'tuvisteis', 'tuviese', 'ni', 'estemos', 'sus', 'hay', 'sin', 'tendríais', 'sentido', 'tuvo', 'estaré', 'le', 'fuésemos', 'tengamos', 'seré', 'tendrías', 'tengan', 'haya', 'habrías', 'vuestras', 'había', 'has', 'estuvo', 'tuviésemos', 'era', 'algo', 'fui', 'fuiste', 'estamos', 'e', 'estuvisteis', 'todos', 'mía', 'estaremos', 'han', 'seamos', 'un', 'tened', 'estadas', 'vuestra', 'todo', 'serán', 'erais', 'muy', 'os', 'esté', 'tendremos', 'estuviste', 'te', 'estando', 'ha', 'será', 'sobre', 'vuestro', 'hube', 'estad', 'hubisteis', 'tuvieran', 'ellas', 'se', 'nuestra', 'estaría', 'mucho', 'seríamos', 'estuvieran', 'sentid', 'seríais', 'nos', 'ellos', 'tenía', 'esas', 'estuviese', 'tenidos', 'que', 'estuvieron', 'mí', 'o', 'tuvieseis', 'antes', 'estará', 'estuviera', 'tiene', 'uno', 'estados', 'fueron', 'tuya', 'contra', 'éramos', 'ese', 'hemos', 'tuviesen', 'habrás', 'teniendo', 'mi', 'fue', 'nuestros', 'porque', 'hayamos', 'también', 'tendríamos', 'otras', 'tuve', 'hubieses', 'tengo', 'los', 'poco', 'hubiésemos', 'tendrían', 'suyo', 'suya', 'está', 'estuvimos', 'fueras', 'habéis', 'tendrá', 'hubiera', 'tuyo', 'el', 'muchos', 'eres', 'más', 'tendría', 'habidas', 'estado', 'tuyas', 'estaréis', 'eras', 'estabas', 'sentidas', 'algunas', 'estarás', 'somos', 'estaba', 'tenías', 'habréis', 'vosotras', 'del', 'este', 'hubiéramos'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('spanish')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Este listado de palabras se utiliza para eliminarlas de los textos.\n",
    "\n",
    "\n",
    "* Veamos a continuación como obtener las stop words de una frase tras su tokenización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "por\n"
     ]
    }
   ],
   "source": [
    "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
    "words = nltk.word_tokenize(doc)\n",
    "for word in words:\n",
    "        if word in stopwords.words('spanish'):\n",
    "            print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.- Part of Speech (PoS)\n",
    "\n",
    "\n",
    "* Consiste en asignar una etiqueta de categoría a las partes tokenizadas de una oración: sustantivos, verbos, adjetivos, etc.\n",
    "\n",
    "\n",
    "* El PoS de NLTK solo esta disponible para el ingles y tiene las siguientes categorias:\n",
    "\n",
    "|Tag|Meaning|\n",
    "|---|---|\n",
    "|ADJ|adjective|\n",
    "|ADP|adposition|\n",
    "|ADV|adverb|\n",
    "|CONJ|conjunction|\n",
    "|DET|determiner|\n",
    "|NOUN|noun|\n",
    "|NUM|numeral|\n",
    "|PRT|particle|\n",
    "|PRON|pronoun|\n",
    "|VERB|verb|\n",
    "|.|punctuation|\n",
    "|X|other|\n",
    "\n",
    "\n",
    "* Nota: La tabla anterior no significa que solo asigne esas categorias, si no que tiene esas categorias y luego las va desgranando; por ejemplo, los verbos o adjetivos pueden ser de diferentes tipos y les pondrá una etiqueta en función de ese tipo.\n",
    "\n",
    "\n",
    "* Veamos a continuación un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Is', 'VBZ'), ('marathon', 'JJ'), ('running', 'VBG'), ('bad', 'JJ'), ('for', 'IN'), ('you', 'PRP'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "doc = nltk.word_tokenize('Is marathon running bad for you?')\n",
    "print (nltk.pos_tag(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS en Español:\n",
    "\n",
    "\n",
    "* Para poder \"tagear\" correctamente las palabras en Español, tenemos que descargarnos un diccionario específico para esta lengua.\n",
    "\n",
    "\n",
    "* El grupo de Procesamiento de Lenguaje Natural de la Universidad de Stanford ha desarrollado un diccionario en castellano que nos pertime etiquetar las palabras.\n",
    "\n",
    "\n",
    "* En el siguiente enlace se puede ver su descripción: https://nlp.stanford.edu/software/spanish-faq.shtml\n",
    "\n",
    "\n",
    "* Para ello debemos de descargarnos el software especifico proporcionado por la universidad de Standford a través del siguiente enlace: https://nlp.stanford.edu/software/tagger.shtml\n",
    "\n",
    "<img src=\"./imgs/004_Standford_tagger.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "* Una vez descargada la librería tenemos que:\n",
    "    1. Descomprimir el fichero\n",
    "    2. Obtener el jar: stanford-postagger-3.9.2.jar\n",
    "    3. Obtener el tagger spanish.tagger que se encuentra dentro de la carpeta models\n",
    "\n",
    "\n",
    "* Estos ficheros necesarios ya estan copiados dentro del proyecto en la carpeta 'libs'\n",
    "\n",
    "\n",
    "* Veamos como ejecutarlo (*Nota: si se utiliza windows hay que poner las rutas absolutas de estos ficheros (variables 'jar' y 'tagger_file')*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Un', 'di0000'), ('radar', 'nc0s000'), ('multa', 'nc0s000'), ('a', 'sp000'), ('Mariano', 'np00000'), ('Rajoy', 'np00000'), ('por', 'sp000'), ('caminar', 'vmn0000'), ('demasiado', 'rg'), ('rapido', 'aq0000')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.internals import find_jars_within_path\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "\n",
    "jar = \"./libs/Standford_tagger/stanford-postagger-3.9.2.jar\"\n",
    "tagger_file = \"./libs/Standford_tagger/spanish.tagger\"\n",
    "\n",
    "tagger = StanfordPOSTagger(tagger_file, jar)\n",
    "\n",
    "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
    "words = nltk.word_tokenize(doc)\n",
    "tags = tagger.tag(words)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este caso el \"taggeo\" es diferente al de NLTK.\n",
    "\n",
    "\n",
    "* Si nos fijamos en la documentación:\n",
    "    - ('Un', 'di0000') -> Article (indefinite)\n",
    "    - ('radar', 'nc0s000') -> Common noun (singular)\n",
    "    - ('multa', 'nc0s000') -> Common noun (singular)\n",
    "    - ('a', 'sp000') -> Preposition\n",
    "    - ('Mariano', 'np00000') -> Proper noun\n",
    "    - ('Rajoy', 'np00000') -> Proper noun\n",
    "    - ('por', 'sp000') -> Preposition\n",
    "    - ('caminar', 'vmn0000') -> Verb (main, infinitive)\n",
    "    - ('demasiado', 'rg') -> Adverb (general)\n",
    "    - ('rapido', 'aq0000')  -> Adjective (descriptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.- n-grams\n",
    "\n",
    "* Modelo de representación que selecciona secuencias contiguas de N elementos de la selección de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Un', 'radar', 'multa')\n",
      "('radar', 'multa', 'a')\n",
      "('multa', 'a', 'Mariano')\n",
      "('a', 'Mariano', 'Rajoy')\n",
      "('Mariano', 'Rajoy', 'por')\n",
      "('Rajoy', 'por', 'caminar')\n",
      "('por', 'caminar', 'demasiado')\n",
      "('caminar', 'demasiado', 'rapido')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "doc = \"Un radar multa a Mariano Rajoy por caminar demasiado rapido\"\n",
    "words = nltk.word_tokenize(doc)\n",
    "num_elementos = 3\n",
    "n_grams = ngrams(words, num_elementos)\n",
    "for grams in n_grams:\n",
    "    print (grams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
