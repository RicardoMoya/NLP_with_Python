{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n",
    "\n",
    "* En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n",
    "\n",
    "\n",
    "* Estos artículos se encuentran en un fichero csv dentro de la carpeta \"data\" del proyecto (./data/corpus_mundo_today.csv).\n",
    "\n",
    "\n",
    "* Este CSV esta formado por 3 campos que son:\n",
    "    - Tema\n",
    "    - Título\n",
    "    - Texto\n",
    "    \n",
    "    \n",
    "* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n",
    "\n",
    "\n",
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
    "<span></span><br><br>\n",
    "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    " <span></span><br><br>       \n",
    "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "* Aprovechando la normalización realizada anteriormente se pide:\n",
    "\n",
    "    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "<span></span><br><br>   \n",
    "    7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras por frecuencia de aparición con NLTK**\n",
    "<span></span><br><br>\n",
    "    8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n",
    "<span></span><br><br>   \n",
    "    9. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_file = './data/corpus_mundo_today.csv'\n",
    "docs_list = list()\n",
    "file_txt = open(docs_file).read()\n",
    "for line in file_txt.split('\\n'):\n",
    "    line = line.split('||')\n",
    "    docs_list.append(line[1] + ' ' + line[2])\n",
    "docs_list = docs_list[1:] # Elimino la cabecera del fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(docs_list):\n",
    "    # TODO\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(docs):\n",
    "    # TODO\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(docs):\n",
    "    # TODO\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(docs):\n",
    "    # TODO\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gobierno español sumará a Junqueras las condenas que no vaya a cumplir Puigdemont Después del revés recibido por el Gobierno de España tras la puesta en libertad de Carles Puigdemont por parte de la justicia alemana, el juez Pablo Llarena ha decidido esta semana, a instancias del Ejecutivo, que sumará a Oriol Junqueras las condenas que no vaya a cumplir el líder del PDeCAT. El exvicepresidente de Cataluña, que permanece en la prisión madrileña de Estremera desde el pasado dos de noviembre, asumiría por tanto todos los delitos atribuidos a Carles Puigdemont y, de esta manera, el Tribunal Supremo se asegura de que los actos del expresidente catalán durante la última legislatura no quedan impunes, ya que “Junqueras pagará por todos y cada uno de ellos”. Con esta maniobra ideada para burlar la justicia alemana, el líder de Esquerra Republicana se enfrenta a 50 años más de prisión. “Seguiremos adelante aunque a Junqueras le caigan cien años más, nadie nos va a parar”, ha dicho hoy Carles Puigdemont desde Alemania. “Haré lo que tenga que hacer y si Junqueras se tiene que sacrificar por ello, lo asumiré con resignación y determinación”, ha prometido. “Seguim!”, tuiteaba poco después de trascender la decisión de Llarena. Según fuentes anónimas del poder judicial, se está barajando también la posibilidad de añadir a la pena de Oriol Junqueras las condenas que puedan imponerse en el futuro a Iñaki Urdangarin, Rodrigo Rato o Esperanza Aguirre, entre otros, así como la de un delito de robo con fuerza ocurrido hace una semana en Huesca y del que la policía ha sido incapaz de encontrar al culpable.\n"
     ]
    }
   ],
   "source": [
    "def normalization(docs_list):\n",
    "    corpus = tokenization(docs_list)\n",
    "    corpus = remove_words(corpus)\n",
    "    corpus = lematization(corpus)\n",
    "    corpus = filter_words(corpus)\n",
    "    return corpus\n",
    "\n",
    "corpus = normalization(docs_list)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Gobierno español sumará a Junqueras las condenas que no vaya a cumplir Puigdemont Después del revés recibido por el Gobierno de España tras la puesta en libertad de Carles Puigdemont por parte de la justicia alemana, el juez Pablo Llarena ha decidido esta semana, a instancias del Ejecutivo, que sumará a Oriol Junqueras las condenas que no vaya a cumplir el líder del PDeCAT. El exvicepresidente de Cataluña, que permanece en la prisión madrileña de Estremera desde el pasado dos de noviembre, asumiría por tanto todos los delitos atribuidos a Carles Puigdemont y, de esta manera, el Tribunal Supremo se asegura de que los actos del expresidente catalán durante la última legislatura no quedan impunes, ya que “Junqueras pagará por todos y cada uno de ellos”. Con esta maniobra ideada para burlar la justicia alemana, el líder de Esquerra Republicana se enfrenta a 50 años más de prisión. “Seguiremos adelante aunque a Junqueras le caigan cien años más, nadie nos va a parar”, ha dicho hoy Carles Puigdemont desde Alemania. “Haré lo que tenga que hacer y si Junqueras se tiene que sacrificar por ello, lo asumiré con resignación y determinación”, ha prometido. “Seguim!”, tuiteaba poco después de trascender la decisión de Llarena. Según fuentes anónimas del poder judicial, se está barajando también la posibilidad de añadir a la pena de Oriol Junqueras las condenas que puedan imponerse en el futuro a Iñaki Urdangarin, Rodrigo Rato o Esperanza Aguirre, entre otros, así como la de un delito de robo con fuerza ocurrido hace una semana en Huesca y del que la policía ha sido incapaz de encontrar al culpable.\n"
     ]
    }
   ],
   "source": [
    "def drop_less_frecuency_words(corpus, n):\n",
    "    # TODO\n",
    "    return corpus\n",
    "\n",
    "corpus = drop_less_frecuency_words(corpus, 10)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras por frecuencia de aparición con NLTK**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
