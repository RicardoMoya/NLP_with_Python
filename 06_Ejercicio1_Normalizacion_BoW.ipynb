{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Ejercicio: Normalización de textos y Bolsa de Palabras\n",
    "\n",
    "* En el siguiente ejercicio vamos a trabajar con una serie de artículo obtenido de la web \"https://www.elmundotoday.com/\".\n",
    "\n",
    "\n",
    "* Estos artículos se encuentran en un fichero csv dentro de la carpeta \"data\" del proyecto (./data/corpus_mundo_today.csv).\n",
    "\n",
    "\n",
    "* Este CSV esta formado por 3 campos que son:\n",
    "    - Tema\n",
    "    - Título\n",
    "    - Texto\n",
    "    \n",
    "    \n",
    "* El ejercicio consiste en Normalizar este ***Corpus*** tomando el *título* y *texto* como contenido de cada documento y crear 3 ***Bolsa de Palabras*** de la tres formas vistas en el notebbok **\"05_Bag_of_Words_BoW\"**.\n",
    "\n",
    "\n",
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Dada una lista en la que cada elemento de la misma tiene el contenido (título + texto) de cada documento del corpus se pide:\n",
    "<span></span><br><br>\n",
    "    1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "<span></span><br><br>\n",
    "    4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    " <span></span><br><br>       \n",
    "    5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "        * **input**: lista de documentos (lista de Strings).\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "* Aprovechando la normalización realizada anteriormente se pide:\n",
    "\n",
    "    6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "        * **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "        * **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "        * **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "<span></span><br><br>\n",
    "    7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**\n",
    "<span></span><br><br>   \n",
    "    8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Ejercicios de Nomalización:\n",
    "\n",
    "* Leemos el corpus y pasamos los documentos (Título + Texto) a una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_file = './data/corpus_mundo_today.csv'\n",
    "docs_list = list()\n",
    "file_txt = open(docs_file, encoding=\"utf8\").read()\n",
    "for line in file_txt.split('\\n'):\n",
    "    line = line.split('||')\n",
    "    docs_list.append(line[1] + ' ' + line[2])\n",
    "docs_list = docs_list[1:] # Elimino la cabecera del fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Crear una función que devuelva los documentos *Tokenizados* (una lista de listas) y con los tokens (palabras) en minúsculas.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(docs_list):\n",
    "    # TODO\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Crear una función que elimine los tokens que sean signos de puntuación y *Stop-Words*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(docs):\n",
    "    # TODO\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Crear una función que transforme cada token a su lema (*Lematización*)**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematization(docs):\n",
    "    # TODO\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Crear una función que elimine todos los tokens que no sean *Nombres* (NOUN, PROPN), *Verbos*, *Advervios* o *Adjetivos*.**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(docs):\n",
    "    # TODO\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. **Función que dada una lista de documentos, devuelva los documentos normalizados. Este ejercicio ya esta hecho y simplemente tiene que funcionar llamando a las 4 funciones anteriores.**\n",
    "\n",
    "* **input**: lista de documentos (lista de Strings).\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(docs_list):\n",
    "    corpus = tokenization(docs_list)\n",
    "    corpus = remove_words(corpus)\n",
    "    corpus = lematization(corpus)\n",
    "    corpus = filter_words(corpus)\n",
    "    return corpus\n",
    "\n",
    "corpus = normalization(docs_list)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## 2.- Ejercicios de BoW:\n",
    "\n",
    "#### 6. **Crear una función que dada una lista de documentos (*corpus*) tokenizados, elimine del corpus aquellos tokens que aparecen menos de 'N' veces (N=10) en el corpus**\n",
    "\n",
    "* **input**: lista de listas, en la que cada lista contiene los tokens del documento normalizados.\n",
    "* **input**: 'N' -> Parámetro que nos indica el número mínimo de apariciones de la palabra en el corpus.\n",
    "* **output**: lista de listas, en la que cada lista contiene los tokens del documento normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_less_frecuency_words(corpus, n):\n",
    "    # TODO\n",
    "    return corpus\n",
    "\n",
    "corpus = drop_less_frecuency_words(corpus, 10)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras en ONE-HOT-ENCODE con Gensim**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. **Dado el corpus, normalizado y con tokens que aparecen 10 veces o más en el corpus, se pide crear una bolsa de palabras aplicando el TF-IDF con Scikit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
