{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - Prueba de Concepto: Tendencias Pol√≠ticas en Twitter\n",
    "\n",
    "\n",
    "* Vamos a realizar una prueba de concepto que consiste en:\n",
    "    1. Entrenar un modelo\n",
    "    2. Exportar el modelo\n",
    "    3. Explotar el modelo\n",
    "    \n",
    "\n",
    "* En esta PoC vamos a tener como ***dataset*** un conjunto de ***tweets etiquetados con el nombre de un partido pol√≠tico***.\n",
    "\n",
    "\n",
    "* Con estos ***tweets***; una vez ***Normalizados*** y creada la ***bolsa de palabras***, vamos a ***crear y evaluar*** una serie de ***modelos creados con Algoritmos de Aprendizaje de Clasificaci√≥n***.\n",
    "\n",
    "\n",
    "* Una vez evaluados ***nos quedaremos con el Algoritmos de Aprendizaje que mejor modelo genere*** y ***crearemos un modelo con el Algoritmos de Aprendizaje seleccionado entrenandolo con todo el dataset***. Posteriormente ***exportaremos ese modelo*** para su explotaci√≥n.\n",
    "\n",
    "\n",
    "* Por √∫ltimo '*simulando un producto*' ***leeremos los tweets de una determinda cuenta de twitter*** y ***pasandole los tweets al modelo*** (previamente importado), nos ***clasificar√° esos tweets seg√∫n su tendencia pol√≠tica***.\n",
    "\n",
    "\n",
    "* Para realizar todo esto y diferenciar lo que es la ***generaci√≥n de modelos*** y ***explotaci√≥n de modelos*** vamos a realizar todo este proceso en dos notebooks.\n",
    "\n",
    "\n",
    "# 1. Generaci√≥n de modelos\n",
    "\n",
    "* Notebook: *13_PoC_Tendencias_Politicas_Twitter_Generacion_Exportacion_Modelos.ipynb*\n",
    "\n",
    "\n",
    "* En este punto realizaremos las siguientes acciones:\n",
    "\n",
    "    1. Carga de datos (tweets)\n",
    "    2. Normalizaci√≥n de los tweets\n",
    "    3. Creacci√≥n de la Bolsa de Palabras (BoW)\n",
    "    4. Particionado de los datos con el m√©todo del 'Cross Validation'\n",
    "    5. Creacci√≥n de modelos y evaluaci√≥n\n",
    "    6. Elecci√≥n del mejor modelo y exportaci√≥n en pickle\n",
    "        * Exportar el modelo creado por un algoritmo de aprendizaje\n",
    "        * Exportar la bolsa de palabras\n",
    "\n",
    "\n",
    "# 2. Explotaci√≥n de modelos\n",
    "\n",
    "* Notebook: *14_PoC_Tendencias_Politicas_Twitter_Prediccion.ipynb*\n",
    "\n",
    "\n",
    "* En este punto realizaremos las siguientes acciones:\n",
    "\n",
    "    1. Lectura (via API) de los tweets de una determinada cuenta de twitter\n",
    "    2. Normalizaci√≥n de los tweets\n",
    "    3. Importaci√≥n de los modelos (Clasificaci√≥n y BoW)\n",
    "    3. Creacci√≥n de la Bolsa de Palabras (BoW) de los nuevos tweets\n",
    "    4. Predicci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Carga de Datos\n",
    "\n",
    "\n",
    "* El primer paso que vamos a realizar es el de cargar los datos. \n",
    "\n",
    "\n",
    "* Este fichero lo podemos leer como un '*csv*' con pandas pasandole como separador '***::::***'.\n",
    "\n",
    "\n",
    "* Este fichero esta estructurado de la siguiente manera\n",
    "    - **Cuenta**: Cuenta de twitter\n",
    "    - **Partido**: Partido pol√≠tico al que pertenece (ciudadanos, podemos, pp, psoe)\n",
    "    - **Timestamp**: Instante en el que se public√≥ el tweet\n",
    "    - **Tweet**: Tweet.\n",
    "    \n",
    "    \n",
    "* Leemos los datos y mostramos una muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de Tweets Cargados: 3843\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuenta</th>\n",
       "      <th>partido</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>sanchezcastejon</td>\n",
       "      <td>psoe</td>\n",
       "      <td>1557858287</td>\n",
       "      <td>En #Santiago, con el candidato a la alcald√≠a, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>ahorapodemos</td>\n",
       "      <td>podemos</td>\n",
       "      <td>1558629554</td>\n",
       "      <td>üé• \"Nos quieren esconder d√≥nde se deciden las c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>PSOE</td>\n",
       "      <td>psoe</td>\n",
       "      <td>1556785931</td>\n",
       "      <td>‚úäüèΩüåπ @abalosmeco Cumplimos 140 a√±os porque somo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>sanchezcastejon</td>\n",
       "      <td>psoe</td>\n",
       "      <td>1555563308</td>\n",
       "      <td>Acabo de terminar mis 10 km diarios de #runnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>PSOE</td>\n",
       "      <td>psoe</td>\n",
       "      <td>1558177124</td>\n",
       "      <td>¬°S E G U I M O S! No paramos en este s√°bado de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2183</th>\n",
       "      <td>PSOE</td>\n",
       "      <td>psoe</td>\n",
       "      <td>1558086146</td>\n",
       "      <td>üî¥ @abalosmeco Seguiremos trabajando por nuestr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2106</th>\n",
       "      <td>TeoGarciaEgea</td>\n",
       "      <td>pp</td>\n",
       "      <td>1557950615</td>\n",
       "      <td>¬øHar√°s que pase otra vez el 26 de mayo? #VotaP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>InesArrimadas</td>\n",
       "      <td>ciudadanos</td>\n",
       "      <td>1558617960</td>\n",
       "      <td>La Sra. Batet empieza su andadura al frente de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>PSOE</td>\n",
       "      <td>psoe</td>\n",
       "      <td>1557940875</td>\n",
       "      <td>üì∫@TimmermansEU en @24h_tve: Para Europa, l@s s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>carmencalvo_</td>\n",
       "      <td>psoe</td>\n",
       "      <td>1556214089</td>\n",
       "      <td>En encuentro @psoetorrijos @pscmpsoe: Ciudadan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cuenta     partido   timestamp  \\\n",
       "1664  sanchezcastejon        psoe  1557858287   \n",
       "2837     ahorapodemos     podemos  1558629554   \n",
       "131              PSOE        psoe  1556785931   \n",
       "613   sanchezcastejon        psoe  1555563308   \n",
       "2366             PSOE        psoe  1558177124   \n",
       "2183             PSOE        psoe  1558086146   \n",
       "2106    TeoGarciaEgea          pp  1557950615   \n",
       "3061    InesArrimadas  ciudadanos  1558617960   \n",
       "1966             PSOE        psoe  1557940875   \n",
       "1083     carmencalvo_        psoe  1556214089   \n",
       "\n",
       "                                                  tweet  \n",
       "1664  En #Santiago, con el candidato a la alcald√≠a, ...  \n",
       "2837  üé• \"Nos quieren esconder d√≥nde se deciden las c...  \n",
       "131   ‚úäüèΩüåπ @abalosmeco Cumplimos 140 a√±os porque somo...  \n",
       "613   Acabo de terminar mis 10 km diarios de #runnin...  \n",
       "2366  ¬°S E G U I M O S! No paramos en este s√°bado de...  \n",
       "2183  üî¥ @abalosmeco Seguiremos trabajando por nuestr...  \n",
       "2106  ¬øHar√°s que pase otra vez el 26 de mayo? #VotaP...  \n",
       "3061  La Sra. Batet empieza su andadura al frente de...  \n",
       "1966  üì∫@TimmermansEU en @24h_tve: Para Europa, l@s s...  \n",
       "1083  En encuentro @psoetorrijos @pscmpsoe: Ciudadan...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets_file = './data/tweets_politica.csv'\n",
    "df = pd.read_csv(tweets_file, sep='::::', engine='python')\n",
    "print('N√∫mero de Tweets Cargados: {num}'.format(num=df.shape[0]))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Para esta PoC vamos a utilizar solo el '***Tweet***' y el '***Partido Pol√≠tico***' asociado al tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de Tweets Cargados: 3843\n"
     ]
    }
   ],
   "source": [
    "tweets = [tuple(x) for x in df[['tweet', 'partido']].values]\n",
    "print('N√∫mero de Tweets Cargados: {num}'.format(num=len(tweets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Normalizaci√≥n\n",
    "\n",
    "* Utilizamos ***spaCy*** para la tokenizaci√≥n y normalizaci√≥n.\n",
    "\n",
    "\n",
    "* Tras realizar un an√°lisis del contenido de los tweets pasamos a realizar las siguientes acciones para ***normalizar*** los tweets:\n",
    "    1. Pasamos las frases a min√∫sculas.\n",
    "    2. Sustituimos los puntos por espacios ya que hay muchas palabras unidas por un punto\n",
    "    3. Quitamos la almuhadilla de los hashtags para considerarlos como palabras.\n",
    "    4. Eliminamos los signos de puntuaci√≥n.\n",
    "    5. Eliminamos las palabras con menos de 3 caracteres.\n",
    "    6. Eliminamos las Stop-Words.\n",
    "    7. Eliminamos los enlaces(http) y las menciones (@)\n",
    "    8. Pasamos la palabra a su lema\n",
    "\n",
    "\n",
    "* Todos estos pasos los vamos a realizar en una misma funci√≥n.\n",
    "\n",
    "\n",
    "* ***NOTA***: Se pueden realizar m√°s acciones de normalizaci√≥n que las realizadas, como tratamiento de emoticonos, tratamiento especial de referencia a cuentas, hashtags, etc. Al tratarse de un PoC did√°ctica se ha realizado una normalizaci√≥n '*sencilla*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Divido los datos en dos listas \n",
    "#     X: los tweets\n",
    "#     y: target (polaridad)\n",
    "\n",
    "X = [doc[0] for doc in tweets]\n",
    "y = np.array([doc[1] for doc in tweets])\n",
    "\n",
    "def normalize(sentenses):\n",
    "    \"\"\"normalizamos la lista de frases y devolvemos la misma lista de frases normalizada\"\"\"\n",
    "    for index, sentense in enumerate(sentenses):\n",
    "        # Tokenizamos el tweets realizando los puntos 1,2 y 3.\n",
    "        sentense = nlp(sentense.lower().replace('.', ' ').replace('#', ' '))\n",
    "        # Puntos 4,5,6,7 y 8\n",
    "        sentenses[index] = \" \".join([word.lemma_ for word in sentense if (not word.is_punct)\n",
    "                                     and (len(word.text) > 2) and (not word.is_stop) \n",
    "                                     and (not word.text.startswith('@')) \n",
    "                                     and (not word.text.startswith('http'))\n",
    "                                     and (not ':' in word.text)])\n",
    "    return sentenses\n",
    "\n",
    "# Normalizamos las frases\n",
    "X_norm = normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Bolsa de palabras\n",
    "\n",
    "* Pasamos a construir una bolsa de palabras de frecuencias.\n",
    "\n",
    "\n",
    "* Vamos a utilizar (para construir la bolsa de palabras) la clase \"*CountVectorizer*\" de scikit, quedandonos con:\n",
    "    - Las 1000 palabras m√°s frecuentes.\n",
    "    - Que las palabras aparezcan por los menos en 5 tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=1000, min_df=5)\n",
    "vectorizer.fit_transform(X_norm)\n",
    "\n",
    "# Pasamos los tweets normalizados a Bolsa de palabras\n",
    "X_bow = vectorizer.transform(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Particionado de Datos (Cross Validation)\n",
    "\n",
    "* En este ejercicio el objetivo es ver cual es el mejor modelo que podemos construir para clasificar tweets seg√∫n la tendencia pol√≠tica, por tanto utilizaremos el m√©todo de evaluaci√≥n del ***Cross Validation*** para que todos los tweets sean (en alg√∫n momento) de entrenamiento y test.\n",
    "\n",
    "\n",
    "* Para ello dividiremos el conjunto de tweets en ***10 conjuntos*** para que en cada iteraccion uno de esos grupos sea el conjunto de test y el resto sea el conjunto de entrenamiento.\n",
    "\n",
    "\n",
    "* Para realizar este particionamiento de datos utilizaremos la clase ***KFold*** de Scikit-Learn: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## Creacci√≥n de Modelos y Evaluaci√≥n (Accuracy)\n",
    "\n",
    "\n",
    "* Vamos a crear y evaluar una serie de modelos para ver cual es que obtiene mejores resultados.\n",
    "\n",
    "\n",
    "* Los modelos que vamos a crear y evaluar son los siguientes:\n",
    "    - Multinomial Naive Bayes\n",
    "    - Bernoulli Naive Bayes\n",
    "    - Regresion Logistica\n",
    "    - Support Vector Machine\n",
    "    - Random Forest\n",
    "    \n",
    "\n",
    "* Para simplificar el problema solo vamos a evaluar los modelos (***Cross Validation***) con el accuracy y nos quedaremos con el modelo que mejor accuracy tenga.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREANDO MODELO: Multinomial Naive Bayes\n",
      "\tAVG Accuracy: 0.7718\n",
      "CREANDO MODELO: Bernoulli Naive Bayes\n",
      "\tAVG Accuracy: 0.7996\n",
      "CREANDO MODELO: Regresion Logistica\n",
      "\tAVG Accuracy: 0.7973\n",
      "CREANDO MODELO: Support Vector Machine\n",
      "\tAVG Accuracy: 0.7804\n",
      "CREANDO MODELO: Random Forest\n",
      "\tAVG Accuracy: 0.8020\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import statistics\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=1000)\n",
    "svm = LinearSVC()\n",
    "rf = RandomForestClassifier(max_depth=50, n_estimators=50, max_features=5)\n",
    "\n",
    "clasificadores = {'Multinomial Naive Bayes': mnb,\n",
    "                  'Bernoulli Naive Bayes': bnb,\n",
    "                  'Regresion Logistica': lr,\n",
    "                  'Support Vector Machine': svm,\n",
    "                  'Random Forest': rf}\n",
    "\n",
    "# Ajustamos los modelos y calculamos el accuracy para los datos de entrenamiento\n",
    "accuracy = list()\n",
    "for k, v in clasificadores.items():\n",
    "    print ('CREANDO MODELO: {clas}'.format(clas=k))\n",
    "    model = {}\n",
    "    model['name'] = k\n",
    "    model['acc_list'] = list()\n",
    "    for train_index, test_index in kf.split(X_bow):\n",
    "        v.fit(X_bow[train_index], y[train_index])\n",
    "        acc = v.score(X_bow[test_index], y[test_index])\n",
    "        model['acc_list'].append(acc)\n",
    "    model['acc'] = statistics.mean(model['acc_list'])\n",
    "    accuracy.append(model)\n",
    "    print ('\\tAVG Accuracy: {avg:0.4f}'.format(avg=statistics.mean(model['acc_list'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>acc_list</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "      <td>0.771803</td>\n",
       "      <td>[0.7844155844155845, 0.7610389610389611, 0.732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <td>0.799617</td>\n",
       "      <td>[0.8311688311688312, 0.8467532467532467, 0.792...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regresion Logistica</th>\n",
       "      <td>0.797282</td>\n",
       "      <td>[0.8441558441558441, 0.8025974025974026, 0.789...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Vector Machine</th>\n",
       "      <td>0.780373</td>\n",
       "      <td>[0.7896103896103897, 0.8155844155844156, 0.763...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.801977</td>\n",
       "      <td>[0.7896103896103897, 0.8311688311688312, 0.787...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              acc  \\\n",
       "name                                \n",
       "Multinomial Naive Bayes  0.771803   \n",
       "Bernoulli Naive Bayes    0.799617   \n",
       "Regresion Logistica      0.797282   \n",
       "Support Vector Machine   0.780373   \n",
       "Random Forest            0.801977   \n",
       "\n",
       "                                                                  acc_list  \n",
       "name                                                                        \n",
       "Multinomial Naive Bayes  [0.7844155844155845, 0.7610389610389611, 0.732...  \n",
       "Bernoulli Naive Bayes    [0.8311688311688312, 0.8467532467532467, 0.792...  \n",
       "Regresion Logistica      [0.8441558441558441, 0.8025974025974026, 0.789...  \n",
       "Support Vector Machine   [0.7896103896103897, 0.8155844155844156, 0.763...  \n",
       "Random Forest            [0.7896103896103897, 0.8311688311688312, 0.787...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pasamos los resultados a un DataFrame para visualizarlos mejor\n",
    "results = pd.DataFrame.from_dict(accuracy)\n",
    "results.set_index(\"name\", inplace=True)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "# Elecci√≥n del mejor modelo y exportaci√≥n en ***Pickle***\n",
    "\n",
    "\n",
    "* Parace que el mejor modelo (a nivel de accuracy) es el creado por el algoritmo de aprendizaje de ***Bernoulli Naive Bayes***.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Creacci√≥n del Modelo con todos los tweets\n",
    "\n",
    "\n",
    "* Una vez que hemos seleccionado el Algoritmo de Aprendizaje (***Bernoulli Naive Bayes***) que mejor modelos genera, vamos a crear un modelo utilizando todos los datos (tweets) disponibles en el dataset.\n",
    "\n",
    "\n",
    "* Pasamos a crear el modelo con todos los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "model_bnb = BernoulliNB()\n",
    "model_bnb.fit(X_bow, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exportaci√≥n del Modelo Pickle\n",
    "\n",
    "\n",
    "* Ahora vamos a exportar el modelo creado por el Algoritmo de Aprendizaje ***Bernoulli Naive Bayes*** en formato pickle.\n",
    "\n",
    "\n",
    "* El modelo creado se ha generado con una determinada ***bolsa de palabras*** que contenia 1000 Palabras en su diccionario. Por tanto tambien tenemos que exportar el modelo generado por el ***CountVectorizer*** para que los nuevos tweets a predecir se ajusten a esa BoW.\n",
    "\n",
    "\n",
    "* Por tanto tenemos que realizar:\n",
    "    1. Exportar el ***Modelo*** generado por el Algoritmo de Aprendizaje ***Bernoulli Naive Bayes***.\n",
    "    2. Exportar la ***Bolsa de Palabras*** generado por la clase ***CountVectorizer***.\n",
    "    \n",
    "    \n",
    "* Exportamos el modelo generado por el Algoritmo de Aprendizaje ***Bernoulli Naive Bayes***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = './models/bernoullinb_tweets_politica.pickle'\n",
    "save_model = open(filename,\"wb\")\n",
    "pickle.dump(model_bnb, save_model) # con la funci√≥n 'dump' guardamos el modelo\n",
    "save_model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exportar la ***Bolsa de Palabras*** generado por la clase ***CountVectorizer***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './models/vectorizer_bow_tweets_politica.pickle'\n",
    "save_bow = open(filename,\"wb\")\n",
    "pickle.dump(vectorizer, save_bow)\n",
    "save_bow.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "# Bonus Track - T√©cnicas de Evaluaci√≥n -\n",
    "\n",
    "* Para evaluar las hip√≥tesis obtenidas tras la aplicaci√≥n de alguna de las t√©cnicas de ML, es necesario disponer de un conjunto de datos (etiquetados o no) para generar la mejor de las hip√≥tesis posibles y minimizar el error emp√≠rico\n",
    "\n",
    "\n",
    "* Dado un conjunto de datos, podemos enumerar los siguientes m√©todos de evaluaci√≥n en funci√≥n de c√≥mo se dividen los datos de entrenamiento y de test:\n",
    "\n",
    "    - ***Resustituci√≥n***:¬†Todos los datos disponibles se utilizan como datos de test y de entrenamiento.\n",
    "\n",
    "    - ***Partici√≥n (Hold Out)***:¬†Divide los datos en dos subconjuntos: uno de entrenamiento y uno de test \n",
    "\n",
    "    - ***Validaci√≥n cruzada (Cross Validation)***:¬†Divide los datos aleatoriamente en ‚ÄòN‚Äô bloques. Cada bloque se utiliza como test para un sistema entrenado por el resto de bloques \n",
    "\n",
    "    - ***Exclusi√≥n individual (Leaving one out)***:¬†Este m√©todo utiliza cada dato individual como dato √∫nico de test de un sistema entrenado con todos los datos excepto el de test "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
