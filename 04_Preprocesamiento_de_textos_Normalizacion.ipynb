{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Preprocesamiento de textos (Normalización)\n",
    "\n",
    "* Antes de procesar los texto con cualquier algoritmo de aprendizaje automático (supervisado o no supervisado) es necesario realizar un preporcesamiento con el objetivo de limpiar, normalizar y estructurar el texto.\n",
    "\n",
    "\n",
    "* Para ello se propone el siguiente framework:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/007_framework_preprocesamiento_texto.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "* Los pasos propuestos en este framework pueden abordarse en el orden que se quiera e incluso alguno de estas etapas no sería necesario realizarse en función de como tengamos los textos.\n",
    "\n",
    "\n",
    "* Definamos a continuación lo que hay que realizar en cada uno de estos pasos:\n",
    "\n",
    "\n",
    "1.- ***Eliminación de ruido***: \n",
    "\n",
    "   * Este paso tiene como objetivo eliminar todos aquellos símbolos o caracteres que no aportan nada en el significado de las frases (ojo no confundir con las stop-words), como por ejemplo etiquetas HTML (para el caso del scraping), parseos de XML, JSON, etc.\n",
    "    \n",
    "2.- ***Tokenización***: \n",
    "   * Este paso tiene como objetivo dividir las cadenas de texto del documento en piezas más pequeñas o tokens.\n",
    "   * Aunque la tokenización es el proceso de dividir grandes cadenas de texto en cadenas más pequeñas, se suele diferenciar la:\n",
    "       * ***Segmentation***: Tarea de dividir grandes cadenas de texto en piezas más pequeñas como oraciones o párrafos.\n",
    "       * ***Tokenization***: Tarea de dividir grandes cadenas de texto solo y exclusivamente en palabras.\n",
    "    \n",
    "3.- ***Normalización***:\n",
    "\n",
    "   * La normalización es una tarea que tiene como objetivo poner todo el texto en igualdad de condiciones:\n",
    "        * Convertir todo el texto en mayúscula o minúsculas\n",
    "        * Eliminar, puntos, comas, comillas, etc.\n",
    "        * Convertir los números a su equivalente a palabras\n",
    "        * Quitar las Stop-words\n",
    "        * etc.\n",
    "        \n",
    "<hr>\n",
    "\n",
    "## Ejemplo de Preprocesamiento de Texto.\n",
    "\n",
    "\n",
    "* Aunque no hay una norma o guía de como realizar una normalización de texto ya que esta depende del problema a resolver y de la naturaleza del texto, vamos a mostrar a continuación algunas operaciones más o menos comúnes para la tokenización y normalización de los textos.\n",
    "\n",
    "\n",
    "* Si bien este ejemplo esta hecho utilizando la librería de ***spaCy*** (ya que lo vamos a aplicar sobre un texto en Español) puede realizarse tambien con la librería de ***NLTK*** e incluso determinadas funcionalidades de tratamiento de strings lo podemos hacer con otras librerías.\n",
    "\n",
    "\n",
    "* En el siguiente ejemplo vamos a tokenizar y normalizar un texto:\n",
    "    1. Transformar un texto en tokens\n",
    "    2. Eliminar los tokens que son signos (puntuación, exclamación, etc.)\n",
    "    3. Eliminar las palabras que tienen menos de 'N' caracteres\n",
    "    4. Eliminar las palabras que son Stop Words\n",
    "    5. Pasar el texto a minúsculas\n",
    "    6. Lematización\n",
    "    \n",
    "    \n",
    "* **Nota**: *la normalización de texto que se va a codificar a continuación puede codificarse de forma más optimizada sin la necesidad de recorrer tantas veces la lista de tokens. Ya que este es un ejemplo con fines didácticos, este se centra en los conceptos y no en la optimización*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    \"\"\"\n",
    "    Función que dado un texto devuelve una lista con las palabras del texto no vacias\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [word.text.strip() for word in doc if len(word.text.strip()) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, elimina los signos de puntuación\n",
    "    \"\"\"\n",
    "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
    "    return [word.text for word in doc if not word.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_words(words, num_chars):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras y un número mínimo de caracteres que tienen que tener\n",
    "    las palabras, elimina todas las palabras que tengan menos caracteres que los indicados\n",
    "    \"\"\"\n",
    "    return [word for word in words if len(word) > num_chars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, elimina las Stop Words\n",
    "    \"\"\"\n",
    "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
    "    return [word.text for word in doc if not word.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, las transforma a minúsculas\n",
    "    \"\"\"\n",
    "    return [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(words):\n",
    "    \"\"\"\n",
    "    Función que dada una lista de palabras, devuelve esa lista con el lema de cada una de esas palabras\n",
    "    \"\"\"\n",
    "    doc = spacy.tokens.doc.Doc(nlp.vocab, words=words)\n",
    "    return [word.lemma_ for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Dado un texto, devuelve el texto tokenizado y normalizado\n",
    "    \"\"\"\n",
    "    words = get_tokens(text=text)\n",
    "    words = remove_punctuation(words=words)\n",
    "    words = remove_short_words(words=words, num_chars=3)\n",
    "    words = remove_stop_words(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = lemmatizer(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasamos a tokenizar y normalizar el siguiente texto usando la función de normalización realizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fernando', 'alonso', 'volver', 'sacar', 'petróleo', 'carrera', 'salir', 'acabar', 'premiar', 'coronar', 'adelantar', 'pistar', 'sebastian', 'vettel', 'líder', 'mundial', 'aunque', 'querer', 'sacar', 'pechar', 'coche', 'tocar', 'problema', 'dirección', 'claro', 'desventaja', 'perder', 'recto', 'imposible', 'adelantarle', 'conseguir', 'pillarle', 'abrir', 'pensar', 'curvo', 'poder', 'intentar', 'salir', 'contento', 'séptimo', 'sumar', 'punto', 'carrera', 'señalar']\n"
     ]
    }
   ],
   "source": [
    "raw = \"\"\"Fernando Alonso ha vuelto a sacar petróleo de la carrera, saliendo 13º y acabando 7º un \n",
    "         gran premio que ha coronado adelantando en pista a Sebastian Vettel, líder del Mundial.\n",
    "         Aunque no ha querido sacar pecho por ello: \"Su coche estaba tocado, tenía problemas de dirección, \n",
    "         estaban en clara desventaja e iba perdiendo cada vez más, vi que en la recta iba a ser imposible \n",
    "         adelantarle incluso con el DRS no conseguía pillarle así que como se abría mucho pensé que en la \n",
    "         primera curva que pudiera lo intentaba por dentro y a la primera salió bien y creo que hay que \n",
    "         estar contentos, séptimos otra vez, sumando puntos en las tres carreras\", ha señalado.\"\"\"\n",
    "print(normalize(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En este ejemplo podemos ver como reducimos las palabras (tokens) del texto original, quedandonos con lo importante y normalizado\n",
    "#### Pasamos de 128 tokens del texto original a 44 tokens tras la normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens del texto original: 128\n",
      "Número de tokens distintos del texto original: 91\n",
      "Número de tokens tras la normalización: 44\n",
      "Número de tokens distintos tras la normalización: 41\n"
     ]
    }
   ],
   "source": [
    "print('Número de tokens del texto original: ' + str(len(get_tokens(raw))))\n",
    "print('Número de tokens distintos del texto original: ' + str(len(set(get_tokens(raw)))))\n",
    "print('Número de tokens tras la normalización: ' + str(len(normalize(raw))))\n",
    "print('Número de tokens distintos tras la normalización: ' + str(len(set(normalize(raw)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "# Bonus Track - Tratamiento de Strings -\n",
    "\n",
    "## Unicode\n",
    "\n",
    "* Uno de los quebraderos de cabeza que se tiene a la hora de trabajar con python (sobre todo con python 2.X) es el tema de la codificación de los textos. Una manera sencilla de entender que es lo que queremos hacer con los textos seria la siguiente:\n",
    "\n",
    "    1. ¿Cual es la codificación de mi fichero original?\n",
    "    2. **Decode**: Paso el string de mi fichero a Unicode (cambio de codificación)\n",
    "    3. Realizo las operaciones que sean necesarias sobre los strings codificados en **Unicode**\n",
    "    4. **Encode**: Escribo de **Unicode** a otra **codificación** el string con el que he trabajado\n",
    "\n",
    "<img src=\"./imgs/008_Unicode_Decode_Encode.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "## Operaciones con Strings\n",
    "\n",
    "\n",
    "* Muchas veces tenemos que realizar operaciones de transforación sobre palabras o textos. A continuación se muestran algunas de las funciones más útiles para trabajar con strings:\n",
    "\n",
    "|Nombre Función|Funcionalidad|\n",
    "|---|---|\n",
    "|s.find(t)|index of first instance of string t inside s (-1 if not found)|\n",
    "|s.rfind(t)|index of last instance of string t inside s (-1 if not found)|\n",
    "|s.index(t)|like s.find(t) except it raises ValueError if not found|\n",
    "|s.rindex(t)|like s.rfind(t) except it raises ValueError if not found|\n",
    "|s.join(text)|combine the words of the text into a string using s as the glue|\n",
    "|s.split(t)|split s into a list wherever a t is found (whitespace by default)|\n",
    "|s.splitlines()|split s into a list of strings, one per line|\n",
    "|s.lower()|a lowercased version of the string s|\n",
    "|s.upper()|an uppercased version of the string s|\n",
    "|s.title()|a titlecased version of the string s|\n",
    "|s.strip()|a copy of s without leading or trailing whitespace|\n",
    "|s.replace(t, u)|replace instances of t with u inside s|\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
