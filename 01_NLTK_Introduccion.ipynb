{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%"
    }
   },
   "source": [
    "# 01 - Introducción a la librería NLTK\n",
    "\n",
    "\n",
    "* NLTK (https://www.nltk.org/) es una librería para Python, usada para el análisis y la manipulación del lenguaje natural.\n",
    "\n",
    "\n",
    "* Se instala bien con el gestor de paquetes \"pip\" o con \"conda\" en caso de utilizarlo. Para instalarlo con pip o conda se realiza de la siguiente manera respectivamente:\n",
    "\n",
    "```\n",
    ">> pip install nltk\n",
    ">> conda install nltk\n",
    "```\n",
    "\n",
    "<hr>\n",
    "\n",
    "## 1.- Instalación y descargar de las bases de datos\n",
    "\n",
    "* NLTK utiliza una serie de bases de datos léxicas para la manipulación del lenguaje natural.\n",
    "\n",
    "\n",
    "* También dispone de una una serie de corpus (colección de textos) que nos podemos descargar para \"jugar\" con ellos.\n",
    "\n",
    "\n",
    "* Para descargarnos las bases de datos y los corpus realizaremos lo siguiente:\n",
    "    1. Importar la librería nltk\n",
    "    2. llamar a al método \"download\"<sup>(*)</sup>\n",
    "    3. Aparecerá una ventana emergente para seleccionar todo lo que NLTK nos permite descargar\n",
    "    \n",
    "    \n",
    "###### (*): Si utilizas un MAC es posible que al ejecutar el método \"download()\" haga un logout de la sesión. Para evitarlo y para que se descargue todo el contenido, es necesario pasarle al método \"download\" como parámetros aquello que nos queramos descargar, en nuestro caso todo:\n",
    "\n",
    "```\n",
    ">> nltk.download('all')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Seleccionar todo aquello que queramos descargar.\n",
    "    1. Para empezar seleccionamos todo (all)\n",
    "    2. Pulsamos el boton de descargar\n",
    "    \n",
    "<img src=\"./imgs/001_nltk_download_db.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "5. Una vez que ya tenemos todo descargado no aparecerá con fondo verde todo lo descargardo:\n",
    "\n",
    "<img src=\"./imgs/002_nltk_download_all.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "6. Llegados a este punto ya tenemos descargado todos los corpus y bases de datos lexicas en el directorio que se indicar en la ventana emergente.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- Corpus\n",
    "\n",
    "\n",
    "* Un ***Corpus*** (en Latín \"*Cuerpo*\") dentro del contexto del NLP se refiere a una colección de textos como puede ser un conjunto de artítulo periodísticos, libros, críticas, tweets, etc.\n",
    "\n",
    "\n",
    "* NLTK dispone de una serie de ***Corpus*** con los que poder trabajar y realizar pruebas.\n",
    "\n",
    "\n",
    "* Algunos de los ***corpus*** que pueden ser de interés didáctico son los siguientes:\n",
    "\n",
    "|Corpus|Content|\n",
    "|---|---|\n",
    "|Brown Corpus|15 genres, 1.15M words, tagged, categorized|\n",
    "|CESS Treebanks|1M words, tagged and parsed (Catalan, Spanish)|\n",
    "|Gutenberg (selections)|18 texts, 2M words|\n",
    "|Inaugural Address Corpus|U.S. Presidential Inaugural Addresses (1789–present)|\n",
    "|Movie Reviews|2k movie reviews with sentiment polarity classification|\n",
    "|Reuters Corpus|1.3M words, 10k news documents, categorized|\n",
    "|Stopwords Corpus|2,400 stopwords for 11 languages|\n",
    "|WordNet 3.0 (English)|145k synonym sets|\n",
    "\n",
    "\n",
    "* Para más información relativa a los corpus ir al siguiente enlace: http://www.nltk.org/howto/\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.- Manejo de Corpus (funcionalidades)\n",
    "\n",
    "Dentro de NLTK podemos encontrarnos diferentes tipos de Corpus que podrían clasificarse en:\n",
    "\n",
    "* Textos planos: como el corpus de *Gutenberg*\n",
    "* Textos categorizados: como el corpus de *Bronwn* (15 generos)\n",
    "* Textos multicategóricos: como el corpus de *Reuters* (1 documentos, varias categorias)\n",
    "* Textos temporales: como el corpus *Inaugural Address Corpus*, discursos presidenciales a lo largo de la historia\n",
    "\n",
    "Para el manejo de estos corpus NLTK nos ofrece las siguientes funciones:\n",
    "\n",
    "|Example|Description|\n",
    "|---|---|\n",
    "|fileids()|the files of the corpus|\n",
    "|fileids([categories])|the files of the corpus corresponding to these categories|\n",
    "|categories()|the categories of the corpus|\n",
    "|categories([fileids])|the categories of the corpus corresponding to these files|\n",
    "|raw()|the raw content of the corpus|\n",
    "|raw(fileids=[f1,f2,f3])|the raw content of the specified files|\n",
    "|raw(categories=[c1,c2])|the raw content of the specified categories|\n",
    "|words()|the words of the whole corpus|\n",
    "|words(fileids=[f1,f2,f3])|the words of the specified fileids|\n",
    "|words(categories=[c1,c2])|the words of the specified categories|\n",
    "|sents()|the sentences of the whole corpus|\n",
    "|sents(fileids=[f1,f2,f3])|the sentences of the specified fileids|\n",
    "|sents(categories=[c1,c2])|the sentences of the specified categories|\n",
    "|abspath(fileid)|the location of the given file on disk|\n",
    "|encoding(fileid)|the encoding of the file (if known)|\n",
    "|open(fileid)|open a stream for reading the given corpus file|\n",
    "|root|if the path to the root of locally installed corpus|\n",
    "|readme()|the contents of the README file of the corpus|\n",
    "\n",
    "### 2.1.1.- Ejemplo con el corpus de Gutenberg\n",
    "\n",
    "**1. ¿Que ficheros componen el corpus?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. ¿Cual es el contenido del fichero \"blake-poems.txt\"?**\n",
    "\n",
    "(Por legibilidad mostramos solo los 300 primeros caracteres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Poems by William Blake 1789]\\n\\n \\nSONGS OF INNOCENCE AND OF EXPERIENCE\\nand THE BOOK of THEL\\n\\n\\n SONGS OF INNOCENCE\\n \\n \\n INTRODUCTION\\n \\n Piping down the valleys wild,\\n   Piping songs of pleasant glee,\\n On a cloud I saw a child,\\n   And he laughing said to me:\\n \\n \"Pipe a song about a Lamb!\"\\n   So I piped'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contenido = gutenberg.raw(\"blake-poems.txt\")\n",
    "contenido[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. De cada uno de los ficheros mostramos:**\n",
    "    - Número de caracteres\n",
    "    - Número de palabras\n",
    "    - Número de frases\n",
    "    - Número de palabras distintas que aparecen en el texto (primero las pasamos a minúsculas)\n",
    "    - Número de medio de caracteres por palabra\n",
    "    - Número medio de palabras por frase\n",
    "    - Diversidad léxica (número de palabras / palabras distintas del texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887071     192427     7752       7344       4          24         26         austen-emma.txt\n",
      "466292     98171      3747       5835       4          26         16         austen-persuasion.txt\n",
      "673022     141576     4999       6403       4          28         22         austen-sense.txt\n",
      "4332554    1010654    30103      12767      4          33         79         bible-kjv.txt\n",
      "38153      8354       438        1535       4          19         5          blake-poems.txt\n",
      "249439     55563      2863       3940       4          19         14         bryant-stories.txt\n",
      "84663      18963      1054       1559       4          17         12         burgess-busterbrown.txt\n",
      "144395     34110      1703       2636       4          20         12         carroll-alice.txt\n",
      "457450     96996      4779       8335       4          20         11         chesterton-ball.txt\n",
      "406629     86063      3806       7794       4          22         11         chesterton-brown.txt\n",
      "320525     69213      3742       6349       4          18         10         chesterton-thursday.txt\n",
      "935158     210663     10230      8447       4          20         24         edgeworth-parents.txt\n",
      "1242990    260819     10059      17231      4          25         15         melville-moby_dick.txt\n",
      "468220     96825      1851       9021       4          52         10         milton-paradise.txt\n",
      "112310     25833      2163       3032       4          11         8          shakespeare-caesar.txt\n",
      "162881     37360      3106       4716       4          12         7          shakespeare-hamlet.txt\n",
      "100351     23140      1907       3464       4          12         6          shakespeare-macbeth.txt\n",
      "711215     154883     4250       12452      4          36         12         whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "for file in gutenberg.fileids():\n",
    "    num_chars = len(gutenberg.raw(file))\n",
    "    num_words = len(gutenberg.words(file))\n",
    "    num_sents = len(gutenberg.sents(file))\n",
    "    num_words_distinct = len(set([w.lower() for w in gutenberg.words(file)]))\n",
    "    avg_chars_words = int(num_chars/num_words)\n",
    "    avg_words_sents = int(num_words/num_sents)\n",
    "    lexical_diversity = int(num_words/num_words_distinct)\n",
    "    print(\"{num_chars:<10} {num_words:<10} {num_sents:<10} {num_words_distinct:<10} {avg_chars_words:<10} {avg_words_sents:<10} {lexical_diversity:<10} {file:<10}\"\n",
    "          .format(num_chars=num_chars, num_words=num_words, num_sents=num_sents,\n",
    "                  num_words_distinct=num_words_distinct, avg_chars_words=avg_chars_words,\n",
    "                  avg_words_sents = avg_words_sents, lexical_diversity=lexical_diversity, file=file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### 2.1.2.- Ejemplo con el corpus de Brown\n",
    "\n",
    "Este es un corpus que contiene una serie de textos categorizados (o tageados) con un tipos de genero \n",
    "\n",
    "**1. ¿Cuales son las categorias del corpus de Brown?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***2. ¿Qué ficheros componen la categoría de noticias (news)?*** (por legibilidad solo mostramos 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ca01', 'ca02', 'ca03', 'ca04', 'ca05']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.fileids(['news'])[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***3.¿Que categorias corresponden al fichero \"ca01\"?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories(fileids=['ca01'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***4.¿Que palabras corresponden a la categoria humor?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'was', 'among', 'these', 'that', 'Hinkle', ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='humor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3.- WordNet\n",
    "\n",
    "* ***WordNet*** es un diccionario semántico y jerárquico en Ingles compuesto por una 155k palabras y 117K sinónimos.\n",
    "\n",
    "\n",
    "* De forma jerarquica, esta estructurado de tal manera que hay una serie de palabras llamadas \"***unique beginers***\" o \"*root synsets*\" que son palabras que definen \"conceptos\" muy generales y a partir de esos conceptos generales engloban una serie de palabras pertenecientes a ese concepto. Veamos el siguiente ejemplo:\n",
    "\n",
    "<img src=\"./imgs/003_wordnet-hierarchy.png\" style=\"width: 500px;\"/>\n",
    "<p style=\"text-align: center;\">imagen obtenida del libro: \"<i>Natural Language Procesing with Python</i>\"</p>\n",
    "\n",
    "\n",
    "* En este ejemplo vemos como un \"*camión*\" (truck) esta definido como un \"*vehiculo motorizado*\" (motor vehicle) y este a su vez esta definido por otra palabra de nivel conceptual superior, hasta llegar a un muy alto nivel de palabra que lo define como un \"*artefacto*\" (artefact).\n",
    "\n",
    "\n",
    "* Este seria (\"a grandes rasgos\") como está organizado este diccionario, de tal manera que permite obtener de una palabra cosas como:\n",
    "    * Sinónimos\n",
    "    * Antónimos\n",
    "    * Hipérnimos\n",
    "    * Hipónimos\n",
    "    * Merónimos\n",
    "    * Holónimos\n",
    "    * Etc.\n",
    "    \n",
    "\n",
    "Veamos a continuación un ejemplo con la palabra \"motorcar\" y como nos daría una lista de sinónimos (synset) de esa palabra con la función \"synsets()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En este caso nos devuelve una lista de sinónimos (synset), que serian los \"nodos\" del diccionario jerarquico a partir del cual se relaciona esa palabra. Para este ejemplo solo nos ha dado un sinónimo.\n",
    "\n",
    "* A partir del \"nodo\" 'car.n.01' vamos a:\n",
    "    * Obtener su definición\n",
    "    * Obtener los lemas de sus sinónimos (de la palabra car no de la palabra motorcar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definición: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Sinónimos: ['car', 'auto', 'automobile', 'machine', 'motorcar']\n"
     ]
    }
   ],
   "source": [
    "definicion = wn.synset('car.n.01').definition()\n",
    "sinonimos = wn.synset('car.n.01').lemma_names()\n",
    "\n",
    "print('Definición: ' + definicion)\n",
    "print('Sinónimos: ' + str(sinonimos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relación semántica entre palabras\n",
    "\n",
    "* Otro tema interesante que tenemos con ***WordNet*** es que nos permite ver la relación semáncia o la similaridad que hay entre palabras veamos por ejemplo la similaridad entre las siguientes palabras:\n",
    "\n",
    "    * car\n",
    "    * truck\n",
    "    * dog\n",
    "\n",
    "* Primero obtenemos alguno de los \"nodos\" de la palabra (el primero)\n",
    "* Comparamos la similaridad \"nodo\" con \"nodo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similaridad entre Coche y Camión: 0.3333333333333333\n",
      "Similaridad entre Coche y Perro: 0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "car = wn.synsets('car')[0]\n",
    "truck = wn.synsets('truck')[0]\n",
    "dog = wn.synsets('dog')[0]\n",
    "\n",
    "print('Similaridad entre Coche y Camión: ' + str(car.path_similarity(truck)))\n",
    "print('Similaridad entre Coche y Perro: ' + str(car.path_similarity(dog)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Aunque no hay una similaridad directa entre estas 3 palabras si que se puede apreciar que hay mayor similaridad entre dos automóviles que entre un animal y un automóvil."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
